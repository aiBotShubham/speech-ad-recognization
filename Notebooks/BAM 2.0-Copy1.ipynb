{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,valid_data,valid_label = pickle.load(f)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "# data_path = 'adress_512.pkl'\n",
    "data_path = 'adress_sequence.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)\n",
    "\n",
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(108).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(CTX),train_y.to(CTX)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09537d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(47).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4701d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d492176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.8f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3):\n",
    "        super().__init__()\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(1)\n",
    "        gate_channels=[channel]\n",
    "        gate_channels+=[channel//reduction]*num_layers\n",
    "        gate_channels+=[channel]\n",
    "\n",
    "\n",
    "        self.ca=nn.Sequential()\n",
    "        self.ca.add_module('flatten',Flatten())\n",
    "        for i in range(len(gate_channels)-2):\n",
    "            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))\n",
    "            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))\n",
    "            self.ca.add_module('relu%d'%i,nn.ReLU())\n",
    "        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))\n",
    "        \n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.avgpool(x)\n",
    "        res=self.ca(res)\n",
    "        res=res.unsqueeze(-1).unsqueeze(-1).expand_as(x)\n",
    "        return res\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.sa=nn.Sequential()\n",
    "        self.sa.add_module('conv_reduce1',nn.Conv2d(kernel_size=1,in_channels=channel,out_channels=channel//reduction))\n",
    "        self.sa.add_module('bn_reduce1',nn.BatchNorm2d(channel//reduction))\n",
    "        self.sa.add_module('relu_reduce1',nn.ReLU())\n",
    "        for i in range(num_layers):\n",
    "            self.sa.add_module('conv_%d'%i,nn.Conv2d(kernel_size=3,in_channels=channel//reduction,out_channels=channel//reduction,padding=1,dilation=dia_val))\n",
    "            self.sa.add_module('bn_%d'%i,nn.BatchNorm2d(channel//reduction))\n",
    "            self.sa.add_module('relu_%d'%i,nn.ReLU())\n",
    "        self.sa.add_module('last_conv',nn.Conv2d(channel//reduction,1,kernel_size=1))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.sa(x)\n",
    "        res=res.expand_as(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BAMBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=16,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.ca=ChannelAttention(channel=channel,reduction=reduction)\n",
    "        self.sa=SpatialAttention(channel=channel,reduction=reduction,dia_val=dia_val)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        sa_out=self.sa(x)\n",
    "        ca_out=self.ca(x)\n",
    "        weight=self.sigmoid(sa_out+ca_out)\n",
    "        out=(1+weight)*x\n",
    "        return out\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input=torch.randn(50,512,7,7)\n",
    "#     bam = BAMBlock(channel=512,reduction=16,dia_val=2)\n",
    "#     output=bam(input)\n",
    "#     print(output.shape)\n",
    "    \n",
    "\n",
    "#                   Conv2d(120, 256, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "# #               MaxPool2d(kernel_size=(2,2)),\n",
    "# #               Conv2d(256, 384, kernel_size=(2,2), padding='same'),\n",
    "# #               MaxPool2d(kernel_size=1, stride=0),\n",
    "# #               Conv2d(128, 1, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "#               torch.nn.AvgPool2d(kernel_size=(2,2), stride=1),\n",
    "#               BatchNorm2d(256),\n",
    "#               ReLU(inplace=True),\n",
    "\n",
    "class Mini_Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "        #The LW_CNN module utilizes three convolutions (C), two max-pooling\n",
    "        # (MP), one average-pooling (AP), and one batch normalization\n",
    "        # (BN) layer.\n",
    "        self.LW_CNN1 = Sequential(\n",
    "              Conv2d(40, 128, kernel_size=(3,3), stride=(2,2), padding=0),\n",
    "              MaxPool2d(kernel_size=(2,2), stride=None),\n",
    "        )\n",
    "        self.attention1 = BAMBlock(channel=128,reduction=4,dia_val=2)\n",
    "        self.LW_CNN2 = Sequential(\n",
    "              Conv2d(128, 256, kernel_size=(3,3), stride=(2,2), padding=0),\n",
    "              MaxPool2d(kernel_size=(2,2), stride=None),\n",
    "        )\n",
    "\n",
    "#         self.linear_layers = Sequential(\n",
    "#             Linear(8 , 4),\n",
    "# #             Linear(256, 64),\n",
    "#             Linear(4, 2)\n",
    "# #             Linear(16, 2)\n",
    "#         )\n",
    "        self.attention2 = BAMBlock(channel=256,reduction=8,dia_val=1)\n",
    "        self.LW_CNN3 = Sequential(\n",
    "              Conv2d(256, 128, kernel_size=(2,2), stride=(1,1), padding=0),\n",
    "              torch.nn.AvgPool2d(kernel_size=(2,2), stride=1),\n",
    "              BatchNorm2d(128)\n",
    "        )\n",
    "        self.attention3 = BAMBlock(channel=128,reduction=8,dia_val=1)\n",
    "        self.LW_CNN4 = Sequential(\n",
    "              Conv2d(128, 64, kernel_size=(2,2), stride=(1,1), padding=0),\n",
    "              Conv2d(32, 16, kernel_size=(1,1), stride=(2,2), padding=0),\n",
    "              Conv2d(16, 8, kernel_size=(1,1), stride=(1,1), padding=0),\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\n",
    "#         print(x.size)\n",
    "        x = self.LW_CNN1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.LW_CNN2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.LW_CNN3(x)\n",
    "        x = self.attention3(x)\n",
    "        x = self.LW_CNN4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "class Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "        #The LW_CNN module utilizes three convolutions (C), two max-pooling\n",
    "        # (MP), one average-pooling (AP), and one batch normalization\n",
    "        # (BN) layer.\n",
    "        self.mini_att = Mini_Att_Net()\n",
    "        self.sequences = 40\n",
    "        \n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\n",
    "#         print(x.size)\n",
    "        layers = []\n",
    "        for seq in range(40):\n",
    "            layer = x[:,seq,:,:,:]\n",
    "            layer = self.mini_att(layer)\n",
    "            layers.append(layer)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f81fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (LW_CNN1): Sequential(\n",
      "    (0): Conv2d(300, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (attention1): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=64, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN2): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (1): Linear(in_features=4, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention2): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (bn0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=32, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN3): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (attention3): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=64, out_features=8, bias=True)\n",
      "        (bn0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=8, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN4): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477fd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 20, 20]         307,456\n",
      "         MaxPool2d-2          [-1, 256, 10, 10]               0\n",
      "            Conv2d-3           [-1, 64, 10, 10]          16,448\n",
      "       BatchNorm2d-4           [-1, 64, 10, 10]             128\n",
      "              ReLU-5           [-1, 64, 10, 10]               0\n",
      "            Conv2d-6           [-1, 64, 10, 10]          36,928\n",
      "       BatchNorm2d-7           [-1, 64, 10, 10]             128\n",
      "              ReLU-8           [-1, 64, 10, 10]               0\n",
      "            Conv2d-9           [-1, 64, 10, 10]          36,928\n",
      "      BatchNorm2d-10           [-1, 64, 10, 10]             128\n",
      "             ReLU-11           [-1, 64, 10, 10]               0\n",
      "           Conv2d-12           [-1, 64, 10, 10]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 10, 10]             128\n",
      "             ReLU-14           [-1, 64, 10, 10]               0\n",
      "           Conv2d-15            [-1, 1, 10, 10]              65\n",
      " SpatialAttention-16          [-1, 256, 10, 10]               0\n",
      "AdaptiveAvgPool2d-17            [-1, 256, 1, 1]               0\n",
      "          Flatten-18                  [-1, 256]               0\n",
      "           Linear-19                   [-1, 64]          16,448\n",
      "      BatchNorm1d-20                   [-1, 64]             128\n",
      "             ReLU-21                   [-1, 64]               0\n",
      "           Linear-22                   [-1, 64]           4,160\n",
      "      BatchNorm1d-23                   [-1, 64]             128\n",
      "             ReLU-24                   [-1, 64]               0\n",
      "           Linear-25                   [-1, 64]           4,160\n",
      "      BatchNorm1d-26                   [-1, 64]             128\n",
      "             ReLU-27                   [-1, 64]               0\n",
      "           Linear-28                  [-1, 256]          16,640\n",
      " ChannelAttention-29          [-1, 256, 10, 10]               0\n",
      "          Sigmoid-30          [-1, 256, 10, 10]               0\n",
      "         BAMBlock-31          [-1, 256, 10, 10]               0\n",
      "           Conv2d-32            [-1, 128, 5, 5]         131,200\n",
      "        MaxPool2d-33            [-1, 128, 2, 2]               0\n",
      "           Conv2d-34             [-1, 32, 2, 2]           4,128\n",
      "      BatchNorm2d-35             [-1, 32, 2, 2]              64\n",
      "             ReLU-36             [-1, 32, 2, 2]               0\n",
      "           Conv2d-37             [-1, 32, 2, 2]           9,248\n",
      "      BatchNorm2d-38             [-1, 32, 2, 2]              64\n",
      "             ReLU-39             [-1, 32, 2, 2]               0\n",
      "           Conv2d-40             [-1, 32, 2, 2]           9,248\n",
      "      BatchNorm2d-41             [-1, 32, 2, 2]              64\n",
      "             ReLU-42             [-1, 32, 2, 2]               0\n",
      "           Conv2d-43             [-1, 32, 2, 2]           9,248\n",
      "      BatchNorm2d-44             [-1, 32, 2, 2]              64\n",
      "             ReLU-45             [-1, 32, 2, 2]               0\n",
      "           Conv2d-46              [-1, 1, 2, 2]              33\n",
      " SpatialAttention-47            [-1, 128, 2, 2]               0\n",
      "AdaptiveAvgPool2d-48            [-1, 128, 1, 1]               0\n",
      "          Flatten-49                  [-1, 128]               0\n",
      "           Linear-50                   [-1, 32]           4,128\n",
      "      BatchNorm1d-51                   [-1, 32]              64\n",
      "             ReLU-52                   [-1, 32]               0\n",
      "           Linear-53                   [-1, 32]           1,056\n",
      "      BatchNorm1d-54                   [-1, 32]              64\n",
      "             ReLU-55                   [-1, 32]               0\n",
      "           Linear-56                   [-1, 32]           1,056\n",
      "      BatchNorm1d-57                   [-1, 32]              64\n",
      "             ReLU-58                   [-1, 32]               0\n",
      "           Linear-59                  [-1, 128]           4,224\n",
      " ChannelAttention-60            [-1, 128, 2, 2]               0\n",
      "          Sigmoid-61            [-1, 128, 2, 2]               0\n",
      "         BAMBlock-62            [-1, 128, 2, 2]               0\n",
      "           Conv2d-63             [-1, 64, 2, 2]           8,256\n",
      "        AvgPool2d-64             [-1, 64, 1, 1]               0\n",
      "      BatchNorm2d-65             [-1, 64, 1, 1]             128\n",
      "           Conv2d-66              [-1, 8, 1, 1]             520\n",
      "      BatchNorm2d-67              [-1, 8, 1, 1]              16\n",
      "             ReLU-68              [-1, 8, 1, 1]               0\n",
      "           Conv2d-69              [-1, 8, 1, 1]             584\n",
      "      BatchNorm2d-70              [-1, 8, 1, 1]              16\n",
      "             ReLU-71              [-1, 8, 1, 1]               0\n",
      "           Conv2d-72              [-1, 8, 1, 1]             584\n",
      "      BatchNorm2d-73              [-1, 8, 1, 1]              16\n",
      "             ReLU-74              [-1, 8, 1, 1]               0\n",
      "           Conv2d-75              [-1, 8, 1, 1]             584\n",
      "      BatchNorm2d-76              [-1, 8, 1, 1]              16\n",
      "             ReLU-77              [-1, 8, 1, 1]               0\n",
      "           Conv2d-78              [-1, 1, 1, 1]               9\n",
      " SpatialAttention-79             [-1, 64, 1, 1]               0\n",
      "AdaptiveAvgPool2d-80             [-1, 64, 1, 1]               0\n",
      "          Flatten-81                   [-1, 64]               0\n",
      "           Linear-82                    [-1, 8]             520\n",
      "      BatchNorm1d-83                    [-1, 8]              16\n",
      "             ReLU-84                    [-1, 8]               0\n",
      "           Linear-85                    [-1, 8]              72\n",
      "      BatchNorm1d-86                    [-1, 8]              16\n",
      "             ReLU-87                    [-1, 8]               0\n",
      "           Linear-88                    [-1, 8]              72\n",
      "      BatchNorm1d-89                    [-1, 8]              16\n",
      "             ReLU-90                    [-1, 8]               0\n",
      "           Linear-91                   [-1, 64]             576\n",
      " ChannelAttention-92             [-1, 64, 1, 1]               0\n",
      "          Sigmoid-93             [-1, 64, 1, 1]               0\n",
      "         BAMBlock-94             [-1, 64, 1, 1]               0\n",
      "           Conv2d-95             [-1, 32, 1, 1]           2,080\n",
      "           Conv2d-96             [-1, 16, 1, 1]             528\n",
      "           Conv2d-97              [-1, 8, 1, 1]             136\n",
      "           Linear-98                    [-1, 4]              36\n",
      "           Linear-99                    [-1, 2]              10\n",
      "================================================================\n",
      "Total params: 665,881\n",
      "Trainable params: 665,881\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 2.42\n",
      "Params size (MB): 2.54\n",
      "Estimated Total Size (MB): 6.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (300, 40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=torch.randn(50,256,7,7)\n",
    "# bam = BAMBlock(channel=256,reduction=16,dia_val=2)\n",
    "# output=bam(input)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a72cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6957, val_loss: 1.0795, val_acc: 0.61702126\n",
      "Epoch [1], train_loss: 0.6633, val_loss: 0.9083, val_acc: 0.48936170\n",
      "Epoch [2], train_loss: 0.6570, val_loss: 0.7935, val_acc: 0.48936170\n",
      "Epoch [3], train_loss: 0.6445, val_loss: 0.6248, val_acc: 0.70212764\n",
      "Epoch [4], train_loss: 0.6172, val_loss: 0.6239, val_acc: 0.70212764\n",
      "Epoch [5], train_loss: 0.5743, val_loss: 0.6765, val_acc: 0.55319148\n",
      "Epoch [6], train_loss: 0.4970, val_loss: 0.6534, val_acc: 0.65957445\n",
      "Epoch [7], train_loss: 0.4180, val_loss: 0.7595, val_acc: 0.59574467\n",
      "Epoch [8], train_loss: 0.3349, val_loss: 0.7386, val_acc: 0.57446808\n",
      "Epoch [9], train_loss: 0.2557, val_loss: 1.2017, val_acc: 0.51063830\n",
      "Epoch [10], train_loss: 0.1999, val_loss: 0.9402, val_acc: 0.57446808\n",
      "Epoch [11], train_loss: 0.1290, val_loss: 1.3057, val_acc: 0.53191489\n",
      "Epoch [12], train_loss: 0.1163, val_loss: 1.1758, val_acc: 0.55319148\n",
      "Epoch [13], train_loss: 0.0797, val_loss: 1.9835, val_acc: 0.51063830\n",
      "Epoch [14], train_loss: 0.0431, val_loss: 2.8492, val_acc: 0.48936170\n",
      "Epoch [15], train_loss: 0.0379, val_loss: 1.4063, val_acc: 0.57446808\n",
      "Epoch [16], train_loss: 0.0934, val_loss: 3.5643, val_acc: 0.46808511\n",
      "Epoch [17], train_loss: 0.0194, val_loss: 1.5762, val_acc: 0.68085104\n",
      "Epoch [18], train_loss: 0.0197, val_loss: 1.4089, val_acc: 0.63829786\n",
      "Epoch [19], train_loss: 0.0050, val_loss: 2.5073, val_acc: 0.55319148\n",
      "Epoch [20], train_loss: 0.0093, val_loss: 1.6656, val_acc: 0.61702126\n",
      "Epoch [21], train_loss: 0.0025, val_loss: 1.2566, val_acc: 0.61702126\n",
      "Epoch [22], train_loss: 0.0023, val_loss: 1.2296, val_acc: 0.63829786\n",
      "Epoch [23], train_loss: 0.0012, val_loss: 1.3076, val_acc: 0.63829786\n",
      "Epoch [24], train_loss: 0.0007, val_loss: 1.4646, val_acc: 0.57446808\n",
      "Epoch [25], train_loss: 0.0008, val_loss: 1.6263, val_acc: 0.55319148\n",
      "Epoch [26], train_loss: 0.0014, val_loss: 1.5173, val_acc: 0.61702126\n",
      "Epoch [27], train_loss: 0.0004, val_loss: 1.5311, val_acc: 0.59574467\n",
      "Epoch [28], train_loss: 0.0006, val_loss: 1.6945, val_acc: 0.57446808\n",
      "Epoch [29], train_loss: 0.0023, val_loss: 2.0063, val_acc: 0.55319148\n",
      "Epoch [30], train_loss: 0.0012, val_loss: 2.7420, val_acc: 0.59574467\n",
      "Epoch [31], train_loss: 0.0009, val_loss: 3.1971, val_acc: 0.55319148\n",
      "Epoch [32], train_loss: 0.0024, val_loss: 2.2070, val_acc: 0.59574467\n",
      "Epoch [33], train_loss: 0.0045, val_loss: 2.4662, val_acc: 0.63829786\n",
      "Epoch [34], train_loss: 0.0076, val_loss: 1.7706, val_acc: 0.59574467\n",
      "Epoch [35], train_loss: 0.0071, val_loss: 1.6487, val_acc: 0.59574467\n",
      "Epoch [36], train_loss: 0.0029, val_loss: 2.2076, val_acc: 0.55319148\n",
      "Epoch [37], train_loss: 0.0038, val_loss: 2.8780, val_acc: 0.57446808\n",
      "Epoch [38], train_loss: 0.0057, val_loss: 1.7721, val_acc: 0.57446808\n",
      "Epoch [39], train_loss: 0.0136, val_loss: 1.8264, val_acc: 0.61702126\n",
      "Epoch [40], train_loss: 0.0081, val_loss: 2.0074, val_acc: 0.63829786\n",
      "Epoch [41], train_loss: 0.0144, val_loss: 1.8904, val_acc: 0.63829786\n",
      "Epoch [42], train_loss: 0.0102, val_loss: 2.1442, val_acc: 0.61702126\n",
      "Epoch [43], train_loss: 0.0423, val_loss: 1.7049, val_acc: 0.61702126\n",
      "Epoch [44], train_loss: 0.0296, val_loss: 1.6219, val_acc: 0.59574467\n",
      "Epoch [45], train_loss: 0.1144, val_loss: 3.3218, val_acc: 0.57446808\n",
      "Epoch [46], train_loss: 0.2770, val_loss: 4.0225, val_acc: 0.48936170\n",
      "Epoch [47], train_loss: 0.0614, val_loss: 2.3529, val_acc: 0.53191489\n",
      "Epoch [48], train_loss: 0.0726, val_loss: 1.5164, val_acc: 0.59574467\n",
      "Epoch [49], train_loss: 0.0181, val_loss: 1.4980, val_acc: 0.59574467\n",
      "Epoch [50], train_loss: 0.0155, val_loss: 1.4680, val_acc: 0.63829786\n",
      "Epoch [51], train_loss: 0.0173, val_loss: 1.4005, val_acc: 0.68085104\n",
      "Epoch [52], train_loss: 0.0188, val_loss: 1.4152, val_acc: 0.63829786\n",
      "Epoch [53], train_loss: 0.0069, val_loss: 1.4854, val_acc: 0.68085104\n",
      "Epoch [54], train_loss: 0.0061, val_loss: 1.5163, val_acc: 0.68085104\n",
      "Epoch [55], train_loss: 0.0061, val_loss: 1.6179, val_acc: 0.68085104\n",
      "Epoch [56], train_loss: 0.0019, val_loss: 1.6510, val_acc: 0.70212764\n",
      "Epoch [57], train_loss: 0.0051, val_loss: 1.8360, val_acc: 0.65957445\n",
      "Epoch [58], train_loss: 0.0027, val_loss: 2.1358, val_acc: 0.65957445\n",
      "Epoch [59], train_loss: 0.0062, val_loss: 1.6888, val_acc: 0.68085104\n",
      "Epoch [60], train_loss: 0.0006, val_loss: 1.8995, val_acc: 0.70212764\n",
      "Epoch [61], train_loss: 0.0005, val_loss: 2.1583, val_acc: 0.70212764\n",
      "Epoch [62], train_loss: 0.0027, val_loss: 1.8358, val_acc: 0.70212764\n",
      "Epoch [63], train_loss: 0.0002, val_loss: 2.2898, val_acc: 0.65957445\n",
      "Epoch [64], train_loss: 0.0006, val_loss: 2.7135, val_acc: 0.61702126\n",
      "Epoch [65], train_loss: 0.0043, val_loss: 2.1849, val_acc: 0.63829786\n",
      "Epoch [66], train_loss: 0.0002, val_loss: 2.1140, val_acc: 0.68085104\n",
      "Epoch [67], train_loss: 0.0026, val_loss: 2.0813, val_acc: 0.65957445\n",
      "Epoch [68], train_loss: 0.0006, val_loss: 2.1922, val_acc: 0.65957445\n",
      "Epoch [69], train_loss: 0.0005, val_loss: 2.7446, val_acc: 0.61702126\n",
      "Epoch [70], train_loss: 0.0005, val_loss: 3.2179, val_acc: 0.59574467\n",
      "Epoch [71], train_loss: 0.0120, val_loss: 3.3717, val_acc: 0.70212764\n",
      "Epoch [72], train_loss: 0.0382, val_loss: 8.0728, val_acc: 0.51063830\n",
      "Epoch [73], train_loss: 0.0189, val_loss: 6.9124, val_acc: 0.51063830\n",
      "Epoch [74], train_loss: 0.0017, val_loss: 4.9949, val_acc: 0.53191489\n",
      "Epoch [75], train_loss: 0.0017, val_loss: 3.4217, val_acc: 0.53191489\n",
      "Epoch [76], train_loss: 0.0005, val_loss: 2.4993, val_acc: 0.61702126\n",
      "Epoch [77], train_loss: 0.0014, val_loss: 2.1518, val_acc: 0.61702126\n",
      "Epoch [78], train_loss: 0.0005, val_loss: 2.0901, val_acc: 0.65957445\n",
      "Epoch [79], train_loss: 0.0005, val_loss: 2.1203, val_acc: 0.68085104\n",
      "Epoch [80], train_loss: 0.0007, val_loss: 2.1622, val_acc: 0.65957445\n",
      "Epoch [81], train_loss: 0.0015, val_loss: 2.1908, val_acc: 0.63829786\n",
      "Epoch [82], train_loss: 0.0005, val_loss: 2.2105, val_acc: 0.63829786\n",
      "Epoch [83], train_loss: 0.0007, val_loss: 2.2161, val_acc: 0.65957445\n",
      "Epoch [84], train_loss: 0.0021, val_loss: 2.2187, val_acc: 0.65957445\n",
      "Epoch [85], train_loss: 0.0003, val_loss: 2.2552, val_acc: 0.61702126\n",
      "Epoch [86], train_loss: 0.0003, val_loss: 2.2900, val_acc: 0.61702126\n",
      "Epoch [87], train_loss: 0.0002, val_loss: 2.3297, val_acc: 0.59574467\n",
      "Epoch [88], train_loss: 0.0002, val_loss: 2.3646, val_acc: 0.59574467\n",
      "Epoch [89], train_loss: 0.0007, val_loss: 2.3838, val_acc: 0.59574467\n",
      "Epoch [90], train_loss: 0.0002, val_loss: 2.4033, val_acc: 0.59574467\n",
      "Epoch [91], train_loss: 0.0003, val_loss: 2.4246, val_acc: 0.61702126\n",
      "Epoch [92], train_loss: 0.0005, val_loss: 2.4295, val_acc: 0.61702126\n",
      "Epoch [93], train_loss: 0.0001, val_loss: 2.4278, val_acc: 0.59574467\n",
      "Epoch [94], train_loss: 0.0001, val_loss: 2.4445, val_acc: 0.59574467\n",
      "Epoch [95], train_loss: 0.0001, val_loss: 2.4495, val_acc: 0.59574467\n",
      "Epoch [96], train_loss: 0.0002, val_loss: 2.4660, val_acc: 0.59574467\n",
      "Epoch [97], train_loss: 0.0001, val_loss: 2.4739, val_acc: 0.59574467\n",
      "Epoch [98], train_loss: 0.0002, val_loss: 2.4869, val_acc: 0.59574467\n",
      "Epoch [99], train_loss: 0.0001, val_loss: 2.4915, val_acc: 0.61702126\n",
      "Epoch [100], train_loss: 0.0006, val_loss: 2.5041, val_acc: 0.61702126\n",
      "Epoch [101], train_loss: 0.0004, val_loss: 2.5144, val_acc: 0.61702126\n",
      "Epoch [102], train_loss: 0.0001, val_loss: 2.5232, val_acc: 0.63829786\n",
      "Epoch [103], train_loss: 0.0000, val_loss: 2.5300, val_acc: 0.61702126\n",
      "Epoch [104], train_loss: 0.0001, val_loss: 2.5344, val_acc: 0.61702126\n",
      "Epoch [105], train_loss: 0.0000, val_loss: 2.5305, val_acc: 0.63829786\n",
      "Epoch [106], train_loss: 0.0000, val_loss: 2.5313, val_acc: 0.63829786\n",
      "Epoch [107], train_loss: 0.0003, val_loss: 2.5299, val_acc: 0.63829786\n",
      "Epoch [108], train_loss: 0.0001, val_loss: 2.5407, val_acc: 0.63829786\n",
      "Epoch [109], train_loss: 0.0001, val_loss: 2.5592, val_acc: 0.61702126\n",
      "Epoch [110], train_loss: 0.0001, val_loss: 2.5700, val_acc: 0.61702126\n",
      "Epoch [111], train_loss: 0.0000, val_loss: 2.5757, val_acc: 0.61702126\n",
      "Epoch [112], train_loss: 0.0002, val_loss: 2.5861, val_acc: 0.61702126\n",
      "Epoch [113], train_loss: 0.0000, val_loss: 2.6159, val_acc: 0.61702126\n",
      "Epoch [114], train_loss: 0.0000, val_loss: 2.6473, val_acc: 0.61702126\n",
      "Epoch [115], train_loss: 0.0001, val_loss: 2.6721, val_acc: 0.59574467\n",
      "Epoch [116], train_loss: 0.0000, val_loss: 2.6835, val_acc: 0.59574467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [117], train_loss: 0.0001, val_loss: 2.6866, val_acc: 0.59574467\n",
      "Epoch [118], train_loss: 0.0001, val_loss: 2.6954, val_acc: 0.59574467\n",
      "Epoch [119], train_loss: 0.0000, val_loss: 2.7074, val_acc: 0.59574467\n",
      "Epoch [120], train_loss: 0.0001, val_loss: 2.7138, val_acc: 0.59574467\n",
      "Epoch [121], train_loss: 0.0000, val_loss: 2.7263, val_acc: 0.59574467\n",
      "Epoch [122], train_loss: 0.0000, val_loss: 2.7273, val_acc: 0.59574467\n",
      "Epoch [123], train_loss: 0.0001, val_loss: 2.7405, val_acc: 0.59574467\n",
      "Epoch [124], train_loss: 0.0000, val_loss: 2.7425, val_acc: 0.61702126\n",
      "Epoch [125], train_loss: 0.0002, val_loss: 2.7437, val_acc: 0.59574467\n",
      "Epoch [126], train_loss: 0.0000, val_loss: 2.7570, val_acc: 0.59574467\n",
      "Epoch [127], train_loss: 0.0000, val_loss: 2.7479, val_acc: 0.59574467\n",
      "Epoch [128], train_loss: 0.0000, val_loss: 2.7553, val_acc: 0.59574467\n",
      "Epoch [129], train_loss: 0.0007, val_loss: 2.7572, val_acc: 0.59574467\n",
      "Epoch [130], train_loss: 0.0000, val_loss: 2.7514, val_acc: 0.59574467\n",
      "Epoch [131], train_loss: 0.0000, val_loss: 2.7487, val_acc: 0.59574467\n",
      "Epoch [132], train_loss: 0.0000, val_loss: 2.7461, val_acc: 0.59574467\n",
      "Epoch [133], train_loss: 0.0000, val_loss: 2.7497, val_acc: 0.59574467\n",
      "Epoch [134], train_loss: 0.0001, val_loss: 2.7550, val_acc: 0.59574467\n",
      "Epoch [135], train_loss: 0.0000, val_loss: 2.7617, val_acc: 0.59574467\n",
      "Epoch [136], train_loss: 0.0000, val_loss: 2.7641, val_acc: 0.59574467\n",
      "Epoch [137], train_loss: 0.0001, val_loss: 2.7693, val_acc: 0.59574467\n",
      "Epoch [138], train_loss: 0.0000, val_loss: 2.7761, val_acc: 0.59574467\n",
      "Epoch [139], train_loss: 0.0000, val_loss: 2.7775, val_acc: 0.59574467\n",
      "Epoch [140], train_loss: 0.0000, val_loss: 2.7928, val_acc: 0.59574467\n",
      "Epoch [141], train_loss: 0.0000, val_loss: 2.7857, val_acc: 0.59574467\n",
      "Epoch [142], train_loss: 0.0000, val_loss: 2.7875, val_acc: 0.59574467\n",
      "Epoch [143], train_loss: 0.0000, val_loss: 2.7865, val_acc: 0.59574467\n",
      "Epoch [144], train_loss: 0.0000, val_loss: 2.7888, val_acc: 0.59574467\n",
      "Epoch [145], train_loss: 0.0000, val_loss: 2.7851, val_acc: 0.59574467\n",
      "Epoch [146], train_loss: 0.0000, val_loss: 2.7906, val_acc: 0.59574467\n",
      "Epoch [147], train_loss: 0.0003, val_loss: 2.8121, val_acc: 0.59574467\n",
      "Epoch [148], train_loss: 0.0000, val_loss: 2.8128, val_acc: 0.59574467\n",
      "Epoch [149], train_loss: 0.0000, val_loss: 2.8203, val_acc: 0.59574467\n",
      "Epoch [150], train_loss: 0.0000, val_loss: 2.8184, val_acc: 0.59574467\n",
      "Epoch [151], train_loss: 0.0000, val_loss: 2.8109, val_acc: 0.59574467\n",
      "Epoch [152], train_loss: 0.0000, val_loss: 2.8157, val_acc: 0.59574467\n",
      "Epoch [153], train_loss: 0.0000, val_loss: 2.8155, val_acc: 0.59574467\n",
      "Epoch [154], train_loss: 0.0000, val_loss: 2.8157, val_acc: 0.59574467\n",
      "Epoch [155], train_loss: 0.0000, val_loss: 2.8165, val_acc: 0.59574467\n",
      "Epoch [156], train_loss: 0.0000, val_loss: 2.8261, val_acc: 0.59574467\n",
      "Epoch [157], train_loss: 0.0000, val_loss: 2.8324, val_acc: 0.59574467\n",
      "Epoch [158], train_loss: 0.0000, val_loss: 2.8257, val_acc: 0.59574467\n",
      "Epoch [159], train_loss: 0.0000, val_loss: 2.8254, val_acc: 0.59574467\n",
      "Epoch [160], train_loss: 0.0000, val_loss: 2.8246, val_acc: 0.59574467\n",
      "Epoch [161], train_loss: 0.0001, val_loss: 2.8420, val_acc: 0.59574467\n",
      "Epoch [162], train_loss: 0.0000, val_loss: 2.8364, val_acc: 0.59574467\n",
      "Epoch [163], train_loss: 0.0001, val_loss: 2.8556, val_acc: 0.59574467\n",
      "Epoch [164], train_loss: 0.0000, val_loss: 2.8509, val_acc: 0.59574467\n",
      "Epoch [165], train_loss: 0.0000, val_loss: 2.8561, val_acc: 0.59574467\n",
      "Epoch [166], train_loss: 0.0000, val_loss: 2.8560, val_acc: 0.59574467\n",
      "Epoch [167], train_loss: 0.0000, val_loss: 2.8634, val_acc: 0.61702126\n",
      "Epoch [168], train_loss: 0.0000, val_loss: 2.8571, val_acc: 0.61702126\n",
      "Epoch [169], train_loss: 0.0000, val_loss: 2.8588, val_acc: 0.61702126\n",
      "Epoch [170], train_loss: 0.0000, val_loss: 2.8603, val_acc: 0.61702126\n",
      "Epoch [171], train_loss: 0.0000, val_loss: 2.8530, val_acc: 0.61702126\n",
      "Epoch [172], train_loss: 0.0000, val_loss: 2.8557, val_acc: 0.61702126\n",
      "Epoch [173], train_loss: 0.0000, val_loss: 2.8577, val_acc: 0.59574467\n",
      "Epoch [174], train_loss: 0.0000, val_loss: 2.8692, val_acc: 0.59574467\n",
      "Epoch [175], train_loss: 0.0000, val_loss: 2.8681, val_acc: 0.59574467\n",
      "Epoch [176], train_loss: 0.0000, val_loss: 2.8729, val_acc: 0.59574467\n",
      "Epoch [177], train_loss: 0.0000, val_loss: 2.8715, val_acc: 0.59574467\n",
      "Epoch [178], train_loss: 0.0000, val_loss: 2.8655, val_acc: 0.59574467\n",
      "Epoch [179], train_loss: 0.0000, val_loss: 2.8731, val_acc: 0.61702126\n",
      "Epoch [180], train_loss: 0.0000, val_loss: 2.8845, val_acc: 0.63829786\n",
      "Epoch [181], train_loss: 0.0002, val_loss: 2.9021, val_acc: 0.59574467\n",
      "Epoch [182], train_loss: 0.0000, val_loss: 2.9008, val_acc: 0.59574467\n",
      "Epoch [183], train_loss: 0.0000, val_loss: 2.9048, val_acc: 0.59574467\n",
      "Epoch [184], train_loss: 0.0000, val_loss: 2.9110, val_acc: 0.59574467\n",
      "Epoch [185], train_loss: 0.0000, val_loss: 2.9133, val_acc: 0.59574467\n",
      "Epoch [186], train_loss: 0.0000, val_loss: 2.9160, val_acc: 0.59574467\n",
      "Epoch [187], train_loss: 0.0000, val_loss: 2.9214, val_acc: 0.59574467\n",
      "Epoch [188], train_loss: 0.0000, val_loss: 2.9316, val_acc: 0.59574467\n",
      "Epoch [189], train_loss: 0.0000, val_loss: 2.9352, val_acc: 0.59574467\n",
      "Epoch [190], train_loss: 0.0000, val_loss: 2.9494, val_acc: 0.59574467\n",
      "Epoch [191], train_loss: 0.0000, val_loss: 2.9509, val_acc: 0.59574467\n",
      "Epoch [192], train_loss: 0.0000, val_loss: 2.9520, val_acc: 0.59574467\n",
      "Epoch [193], train_loss: 0.0000, val_loss: 2.9474, val_acc: 0.59574467\n",
      "Epoch [194], train_loss: 0.0000, val_loss: 2.9476, val_acc: 0.59574467\n",
      "Epoch [195], train_loss: 0.0000, val_loss: 2.9482, val_acc: 0.59574467\n",
      "Epoch [196], train_loss: 0.0000, val_loss: 2.9553, val_acc: 0.59574467\n",
      "Epoch [197], train_loss: 0.0001, val_loss: 2.9586, val_acc: 0.59574467\n",
      "Epoch [198], train_loss: 0.0000, val_loss: 2.9650, val_acc: 0.59574467\n",
      "Epoch [199], train_loss: 0.0000, val_loss: 2.9514, val_acc: 0.59574467\n",
      "Epoch [200], train_loss: 0.0001, val_loss: 2.9638, val_acc: 0.59574467\n",
      "Epoch [201], train_loss: 0.0000, val_loss: 2.9702, val_acc: 0.59574467\n",
      "Epoch [202], train_loss: 0.0000, val_loss: 2.9678, val_acc: 0.61702126\n",
      "Epoch [203], train_loss: 0.0000, val_loss: 2.9733, val_acc: 0.59574467\n",
      "Epoch [204], train_loss: 0.0000, val_loss: 2.9763, val_acc: 0.59574467\n",
      "Epoch [205], train_loss: 0.0000, val_loss: 2.9681, val_acc: 0.61702126\n",
      "Epoch [206], train_loss: 0.0000, val_loss: 2.9561, val_acc: 0.59574467\n",
      "Epoch [207], train_loss: 0.0000, val_loss: 2.9629, val_acc: 0.59574467\n",
      "Epoch [208], train_loss: 0.0000, val_loss: 2.9583, val_acc: 0.61702126\n",
      "Epoch [209], train_loss: 0.0000, val_loss: 2.9567, val_acc: 0.61702126\n",
      "Epoch [210], train_loss: 0.0000, val_loss: 2.9624, val_acc: 0.61702126\n",
      "Epoch [211], train_loss: 0.0000, val_loss: 2.9699, val_acc: 0.59574467\n",
      "Epoch [212], train_loss: 0.0000, val_loss: 2.9767, val_acc: 0.61702126\n",
      "Epoch [213], train_loss: 0.0000, val_loss: 2.9731, val_acc: 0.59574467\n",
      "Epoch [214], train_loss: 0.0000, val_loss: 2.9716, val_acc: 0.59574467\n",
      "Epoch [215], train_loss: 0.0000, val_loss: 2.9750, val_acc: 0.59574467\n",
      "Epoch [216], train_loss: 0.0000, val_loss: 2.9828, val_acc: 0.59574467\n",
      "Epoch [217], train_loss: 0.0000, val_loss: 2.9695, val_acc: 0.59574467\n",
      "Epoch [218], train_loss: 0.0000, val_loss: 2.9744, val_acc: 0.59574467\n",
      "Epoch [219], train_loss: 0.0000, val_loss: 2.9704, val_acc: 0.59574467\n",
      "Epoch [220], train_loss: 0.0000, val_loss: 2.9787, val_acc: 0.59574467\n",
      "Epoch [221], train_loss: 0.0000, val_loss: 2.9882, val_acc: 0.59574467\n",
      "Epoch [222], train_loss: 0.0000, val_loss: 2.9844, val_acc: 0.59574467\n",
      "Epoch [223], train_loss: 0.0000, val_loss: 2.9845, val_acc: 0.59574467\n",
      "Epoch [224], train_loss: 0.0000, val_loss: 2.9844, val_acc: 0.59574467\n",
      "Epoch [225], train_loss: 0.0003, val_loss: 2.9720, val_acc: 0.61702126\n",
      "Epoch [226], train_loss: 0.0000, val_loss: 2.9382, val_acc: 0.63829786\n",
      "Epoch [227], train_loss: 0.0000, val_loss: 2.9172, val_acc: 0.63829786\n",
      "Epoch [228], train_loss: 0.0000, val_loss: 2.9067, val_acc: 0.63829786\n",
      "Epoch [229], train_loss: 0.0000, val_loss: 2.8978, val_acc: 0.63829786\n",
      "Epoch [230], train_loss: 0.0000, val_loss: 2.8917, val_acc: 0.63829786\n",
      "Epoch [231], train_loss: 0.0000, val_loss: 2.8988, val_acc: 0.63829786\n",
      "Epoch [232], train_loss: 0.0000, val_loss: 2.9050, val_acc: 0.63829786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [233], train_loss: 0.0000, val_loss: 2.9096, val_acc: 0.63829786\n",
      "Epoch [234], train_loss: 0.0000, val_loss: 2.9088, val_acc: 0.63829786\n",
      "Epoch [235], train_loss: 0.0000, val_loss: 2.9192, val_acc: 0.63829786\n",
      "Epoch [236], train_loss: 0.0000, val_loss: 2.9339, val_acc: 0.63829786\n",
      "Epoch [237], train_loss: 0.0000, val_loss: 2.9459, val_acc: 0.63829786\n",
      "Epoch [238], train_loss: 0.0001, val_loss: 2.9530, val_acc: 0.63829786\n",
      "Epoch [239], train_loss: 0.0000, val_loss: 2.9780, val_acc: 0.61702126\n",
      "Epoch [240], train_loss: 0.0000, val_loss: 2.9920, val_acc: 0.61702126\n",
      "Epoch [241], train_loss: 0.0000, val_loss: 3.0045, val_acc: 0.59574467\n",
      "Epoch [242], train_loss: 0.0000, val_loss: 3.0076, val_acc: 0.59574467\n",
      "Epoch [243], train_loss: 0.0000, val_loss: 3.0141, val_acc: 0.59574467\n",
      "Epoch [244], train_loss: 0.0000, val_loss: 3.0120, val_acc: 0.59574467\n",
      "Epoch [245], train_loss: 0.0000, val_loss: 3.0237, val_acc: 0.59574467\n",
      "Epoch [246], train_loss: 0.0000, val_loss: 3.0284, val_acc: 0.59574467\n",
      "Epoch [247], train_loss: 0.0000, val_loss: 3.0299, val_acc: 0.59574467\n",
      "Epoch [248], train_loss: 0.0000, val_loss: 3.0335, val_acc: 0.59574467\n",
      "Epoch [249], train_loss: 0.0000, val_loss: 3.0299, val_acc: 0.59574467\n",
      "Epoch [250], train_loss: 0.0000, val_loss: 3.0255, val_acc: 0.59574467\n",
      "Epoch [251], train_loss: 0.0001, val_loss: 3.0207, val_acc: 0.61702126\n",
      "Epoch [252], train_loss: 0.0000, val_loss: 3.0244, val_acc: 0.61702126\n",
      "Epoch [253], train_loss: 0.0000, val_loss: 3.0257, val_acc: 0.59574467\n",
      "Epoch [254], train_loss: 0.0000, val_loss: 3.0241, val_acc: 0.59574467\n",
      "Epoch [255], train_loss: 0.0000, val_loss: 3.0243, val_acc: 0.59574467\n",
      "Epoch [256], train_loss: 0.0000, val_loss: 3.0232, val_acc: 0.59574467\n",
      "Epoch [257], train_loss: 0.0000, val_loss: 3.0297, val_acc: 0.59574467\n",
      "Epoch [258], train_loss: 0.0000, val_loss: 3.0216, val_acc: 0.59574467\n",
      "Epoch [259], train_loss: 0.0000, val_loss: 3.0309, val_acc: 0.59574467\n",
      "Epoch [260], train_loss: 0.0000, val_loss: 3.0412, val_acc: 0.59574467\n",
      "Epoch [261], train_loss: 0.0000, val_loss: 3.0362, val_acc: 0.59574467\n",
      "Epoch [262], train_loss: 0.0000, val_loss: 3.0387, val_acc: 0.59574467\n",
      "Epoch [263], train_loss: 0.0000, val_loss: 3.0469, val_acc: 0.59574467\n",
      "Epoch [264], train_loss: 0.0000, val_loss: 3.0450, val_acc: 0.59574467\n",
      "Epoch [265], train_loss: 0.0000, val_loss: 3.0448, val_acc: 0.59574467\n",
      "Epoch [266], train_loss: 0.0000, val_loss: 3.0412, val_acc: 0.59574467\n",
      "Epoch [267], train_loss: 0.0000, val_loss: 3.0557, val_acc: 0.59574467\n",
      "Epoch [268], train_loss: 0.0001, val_loss: 3.0626, val_acc: 0.59574467\n",
      "Epoch [269], train_loss: 0.0000, val_loss: 3.0530, val_acc: 0.59574467\n",
      "Epoch [270], train_loss: 0.0000, val_loss: 3.0465, val_acc: 0.59574467\n",
      "Epoch [271], train_loss: 0.0000, val_loss: 3.0565, val_acc: 0.59574467\n",
      "Epoch [272], train_loss: 0.0000, val_loss: 3.0560, val_acc: 0.59574467\n",
      "Epoch [273], train_loss: 0.0000, val_loss: 3.0601, val_acc: 0.59574467\n",
      "Epoch [274], train_loss: 0.0000, val_loss: 3.0565, val_acc: 0.59574467\n",
      "Epoch [275], train_loss: 0.0000, val_loss: 3.0561, val_acc: 0.59574467\n",
      "Epoch [276], train_loss: 0.0000, val_loss: 3.0641, val_acc: 0.59574467\n",
      "Epoch [277], train_loss: 0.0000, val_loss: 3.0603, val_acc: 0.59574467\n",
      "Epoch [278], train_loss: 0.0000, val_loss: 3.0776, val_acc: 0.59574467\n",
      "Epoch [279], train_loss: 0.0000, val_loss: 3.0783, val_acc: 0.59574467\n",
      "Epoch [280], train_loss: 0.0000, val_loss: 3.0763, val_acc: 0.59574467\n",
      "Epoch [281], train_loss: 0.0000, val_loss: 3.0738, val_acc: 0.59574467\n",
      "Epoch [282], train_loss: 0.0000, val_loss: 3.0753, val_acc: 0.59574467\n",
      "Epoch [283], train_loss: 0.0000, val_loss: 3.0771, val_acc: 0.59574467\n",
      "Epoch [284], train_loss: 0.0000, val_loss: 3.0877, val_acc: 0.59574467\n",
      "Epoch [285], train_loss: 0.0000, val_loss: 3.0908, val_acc: 0.59574467\n",
      "Epoch [286], train_loss: 0.0000, val_loss: 3.0956, val_acc: 0.59574467\n",
      "Epoch [287], train_loss: 0.0000, val_loss: 3.0936, val_acc: 0.59574467\n",
      "Epoch [288], train_loss: 0.0000, val_loss: 3.0922, val_acc: 0.59574467\n",
      "Epoch [289], train_loss: 0.0000, val_loss: 3.0940, val_acc: 0.59574467\n",
      "Epoch [290], train_loss: 0.0001, val_loss: 3.1032, val_acc: 0.59574467\n",
      "Epoch [291], train_loss: 0.0000, val_loss: 3.0920, val_acc: 0.59574467\n",
      "Epoch [292], train_loss: 0.0000, val_loss: 3.0883, val_acc: 0.59574467\n",
      "Epoch [293], train_loss: 0.0000, val_loss: 3.0888, val_acc: 0.59574467\n",
      "Epoch [294], train_loss: 0.0000, val_loss: 3.0875, val_acc: 0.59574467\n",
      "Epoch [295], train_loss: 0.0000, val_loss: 3.0978, val_acc: 0.59574467\n",
      "Epoch [296], train_loss: 0.0000, val_loss: 3.0918, val_acc: 0.59574467\n",
      "Epoch [297], train_loss: 0.0000, val_loss: 3.0906, val_acc: 0.59574467\n",
      "Epoch [298], train_loss: 0.0000, val_loss: 3.0894, val_acc: 0.59574467\n",
      "Epoch [299], train_loss: 0.0000, val_loss: 3.0850, val_acc: 0.59574467\n",
      "Epoch [300], train_loss: 0.0000, val_loss: 3.0848, val_acc: 0.59574467\n",
      "Epoch [301], train_loss: 0.0000, val_loss: 3.0966, val_acc: 0.59574467\n",
      "Epoch [302], train_loss: 0.0000, val_loss: 3.0989, val_acc: 0.59574467\n",
      "Epoch [303], train_loss: 0.0000, val_loss: 3.0936, val_acc: 0.59574467\n",
      "Epoch [304], train_loss: 0.0000, val_loss: 3.0993, val_acc: 0.59574467\n",
      "Epoch [305], train_loss: 0.0000, val_loss: 3.1099, val_acc: 0.59574467\n",
      "Epoch [306], train_loss: 0.0000, val_loss: 3.1211, val_acc: 0.59574467\n",
      "Epoch [307], train_loss: 0.0000, val_loss: 3.1342, val_acc: 0.59574467\n",
      "Epoch [308], train_loss: 0.0000, val_loss: 3.1413, val_acc: 0.59574467\n",
      "Epoch [309], train_loss: 0.0000, val_loss: 3.1440, val_acc: 0.59574467\n",
      "Epoch [310], train_loss: 0.0000, val_loss: 3.1532, val_acc: 0.59574467\n",
      "Epoch [311], train_loss: 0.0000, val_loss: 3.1578, val_acc: 0.59574467\n",
      "Epoch [312], train_loss: 0.0000, val_loss: 3.1597, val_acc: 0.59574467\n",
      "Epoch [313], train_loss: 0.0000, val_loss: 3.1750, val_acc: 0.59574467\n",
      "Epoch [314], train_loss: 0.0000, val_loss: 3.1792, val_acc: 0.59574467\n",
      "Epoch [315], train_loss: 0.0000, val_loss: 3.1763, val_acc: 0.59574467\n",
      "Epoch [316], train_loss: 0.0000, val_loss: 3.1815, val_acc: 0.59574467\n",
      "Epoch [317], train_loss: 0.0000, val_loss: 3.1668, val_acc: 0.59574467\n",
      "Epoch [318], train_loss: 0.0000, val_loss: 3.1533, val_acc: 0.59574467\n",
      "Epoch [319], train_loss: 0.0000, val_loss: 3.1535, val_acc: 0.59574467\n",
      "Epoch [320], train_loss: 0.0000, val_loss: 3.1528, val_acc: 0.59574467\n",
      "Epoch [321], train_loss: 0.0000, val_loss: 3.1548, val_acc: 0.59574467\n",
      "Epoch [322], train_loss: 0.0000, val_loss: 3.1566, val_acc: 0.59574467\n",
      "Epoch [323], train_loss: 0.0000, val_loss: 3.1615, val_acc: 0.59574467\n",
      "Epoch [324], train_loss: 0.0000, val_loss: 3.1654, val_acc: 0.59574467\n",
      "Epoch [325], train_loss: 0.0000, val_loss: 3.1611, val_acc: 0.59574467\n",
      "Epoch [326], train_loss: 0.0000, val_loss: 3.1609, val_acc: 0.59574467\n",
      "Epoch [327], train_loss: 0.0000, val_loss: 3.1527, val_acc: 0.59574467\n",
      "Epoch [328], train_loss: 0.0000, val_loss: 3.1566, val_acc: 0.59574467\n",
      "Epoch [329], train_loss: 0.0000, val_loss: 3.1584, val_acc: 0.59574467\n",
      "Epoch [330], train_loss: 0.0000, val_loss: 3.1578, val_acc: 0.59574467\n",
      "Epoch [331], train_loss: 0.0000, val_loss: 3.1548, val_acc: 0.59574467\n",
      "Epoch [332], train_loss: 0.0000, val_loss: 3.1579, val_acc: 0.59574467\n",
      "Epoch [333], train_loss: 0.0000, val_loss: 3.1606, val_acc: 0.59574467\n",
      "Epoch [334], train_loss: 0.0000, val_loss: 3.1651, val_acc: 0.59574467\n",
      "Epoch [335], train_loss: 0.0000, val_loss: 3.1716, val_acc: 0.59574467\n",
      "Epoch [336], train_loss: 0.0000, val_loss: 3.1758, val_acc: 0.59574467\n",
      "Epoch [337], train_loss: 0.0000, val_loss: 3.1726, val_acc: 0.59574467\n",
      "Epoch [338], train_loss: 0.0000, val_loss: 3.1662, val_acc: 0.59574467\n",
      "Epoch [339], train_loss: 0.0000, val_loss: 3.1714, val_acc: 0.59574467\n",
      "Epoch [340], train_loss: 0.0000, val_loss: 3.1702, val_acc: 0.59574467\n",
      "Epoch [341], train_loss: 0.0000, val_loss: 3.1743, val_acc: 0.59574467\n",
      "Epoch [342], train_loss: 0.0000, val_loss: 3.1745, val_acc: 0.59574467\n",
      "Epoch [343], train_loss: 0.0000, val_loss: 3.1668, val_acc: 0.59574467\n",
      "Epoch [344], train_loss: 0.0001, val_loss: 3.1744, val_acc: 0.59574467\n",
      "Epoch [345], train_loss: 0.0000, val_loss: 3.1726, val_acc: 0.59574467\n",
      "Epoch [346], train_loss: 0.0000, val_loss: 3.1673, val_acc: 0.59574467\n",
      "Epoch [347], train_loss: 0.0000, val_loss: 3.1733, val_acc: 0.59574467\n",
      "Epoch [348], train_loss: 0.0000, val_loss: 3.1716, val_acc: 0.59574467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [349], train_loss: 0.0000, val_loss: 3.1762, val_acc: 0.59574467\n",
      "Epoch [350], train_loss: 0.0000, val_loss: 3.1743, val_acc: 0.59574467\n",
      "Epoch [351], train_loss: 0.0000, val_loss: 3.1782, val_acc: 0.59574467\n",
      "Epoch [352], train_loss: 0.0000, val_loss: 3.1814, val_acc: 0.59574467\n",
      "Epoch [353], train_loss: 0.0000, val_loss: 3.1933, val_acc: 0.59574467\n",
      "Epoch [354], train_loss: 0.0000, val_loss: 3.1874, val_acc: 0.61702126\n",
      "Epoch [355], train_loss: 0.0000, val_loss: 3.1855, val_acc: 0.59574467\n",
      "Epoch [356], train_loss: 0.0000, val_loss: 3.1809, val_acc: 0.59574467\n",
      "Epoch [357], train_loss: 0.0000, val_loss: 3.1732, val_acc: 0.59574467\n",
      "Epoch [358], train_loss: 0.0000, val_loss: 3.1841, val_acc: 0.59574467\n",
      "Epoch [359], train_loss: 0.0000, val_loss: 3.1806, val_acc: 0.59574467\n",
      "Epoch [360], train_loss: 0.0000, val_loss: 3.1791, val_acc: 0.59574467\n",
      "Epoch [361], train_loss: 0.0000, val_loss: 3.1906, val_acc: 0.59574467\n",
      "Epoch [362], train_loss: 0.0000, val_loss: 3.1870, val_acc: 0.59574467\n",
      "Epoch [363], train_loss: 0.0000, val_loss: 3.1986, val_acc: 0.59574467\n",
      "Epoch [364], train_loss: 0.0000, val_loss: 3.2003, val_acc: 0.59574467\n",
      "Epoch [365], train_loss: 0.0000, val_loss: 3.2049, val_acc: 0.59574467\n",
      "Epoch [366], train_loss: 0.0000, val_loss: 3.2029, val_acc: 0.59574467\n",
      "Epoch [367], train_loss: 0.0001, val_loss: 3.2334, val_acc: 0.59574467\n",
      "Epoch [368], train_loss: 0.0000, val_loss: 3.2342, val_acc: 0.59574467\n",
      "Epoch [369], train_loss: 0.0000, val_loss: 3.2486, val_acc: 0.59574467\n",
      "Epoch [370], train_loss: 0.0000, val_loss: 3.2493, val_acc: 0.59574467\n",
      "Epoch [371], train_loss: 0.0000, val_loss: 3.2473, val_acc: 0.59574467\n",
      "Epoch [372], train_loss: 0.0000, val_loss: 3.2359, val_acc: 0.59574467\n",
      "Epoch [373], train_loss: 0.0000, val_loss: 3.2263, val_acc: 0.59574467\n",
      "Epoch [374], train_loss: 0.0000, val_loss: 3.2304, val_acc: 0.59574467\n",
      "Epoch [375], train_loss: 0.0000, val_loss: 3.2377, val_acc: 0.59574467\n",
      "Epoch [376], train_loss: 0.0000, val_loss: 3.2451, val_acc: 0.59574467\n",
      "Epoch [377], train_loss: 0.0000, val_loss: 3.2494, val_acc: 0.59574467\n",
      "Epoch [378], train_loss: 0.0000, val_loss: 3.2466, val_acc: 0.59574467\n",
      "Epoch [379], train_loss: 0.0000, val_loss: 3.2437, val_acc: 0.59574467\n",
      "Epoch [380], train_loss: 0.0000, val_loss: 3.2390, val_acc: 0.59574467\n",
      "Epoch [381], train_loss: 0.0000, val_loss: 3.2443, val_acc: 0.59574467\n",
      "Epoch [382], train_loss: 0.0000, val_loss: 3.2432, val_acc: 0.59574467\n",
      "Epoch [383], train_loss: 0.0000, val_loss: 3.2500, val_acc: 0.59574467\n",
      "Epoch [384], train_loss: 0.0000, val_loss: 3.2532, val_acc: 0.59574467\n",
      "Epoch [385], train_loss: 0.0000, val_loss: 3.2505, val_acc: 0.59574467\n",
      "Epoch [386], train_loss: 0.0000, val_loss: 3.2522, val_acc: 0.59574467\n",
      "Epoch [387], train_loss: 0.0000, val_loss: 3.2743, val_acc: 0.59574467\n",
      "Epoch [388], train_loss: 0.0000, val_loss: 3.2757, val_acc: 0.59574467\n",
      "Epoch [389], train_loss: 0.0000, val_loss: 3.2743, val_acc: 0.59574467\n",
      "Epoch [390], train_loss: 0.0000, val_loss: 3.2688, val_acc: 0.59574467\n",
      "Epoch [391], train_loss: 0.0000, val_loss: 3.2681, val_acc: 0.59574467\n",
      "Epoch [392], train_loss: 0.0000, val_loss: 3.2687, val_acc: 0.59574467\n",
      "Epoch [393], train_loss: 0.0000, val_loss: 3.2652, val_acc: 0.59574467\n",
      "Epoch [394], train_loss: 0.0000, val_loss: 3.2669, val_acc: 0.59574467\n",
      "Epoch [395], train_loss: 0.0000, val_loss: 3.2660, val_acc: 0.59574467\n",
      "Epoch [396], train_loss: 0.0000, val_loss: 3.2648, val_acc: 0.59574467\n",
      "Epoch [397], train_loss: 0.0000, val_loss: 3.2681, val_acc: 0.59574467\n",
      "Epoch [398], train_loss: 0.0000, val_loss: 3.2716, val_acc: 0.61702126\n",
      "Epoch [399], train_loss: 0.0000, val_loss: 3.2651, val_acc: 0.59574467\n",
      "Epoch [400], train_loss: 0.0000, val_loss: 3.2674, val_acc: 0.59574467\n",
      "Epoch [401], train_loss: 0.0000, val_loss: 3.2590, val_acc: 0.59574467\n",
      "Epoch [402], train_loss: 0.0000, val_loss: 3.2615, val_acc: 0.59574467\n",
      "Epoch [403], train_loss: 0.0000, val_loss: 3.2615, val_acc: 0.59574467\n",
      "Epoch [404], train_loss: 0.0000, val_loss: 3.2841, val_acc: 0.59574467\n",
      "Epoch [405], train_loss: 0.0000, val_loss: 3.2879, val_acc: 0.59574467\n",
      "Epoch [406], train_loss: 0.0000, val_loss: 3.2686, val_acc: 0.59574467\n",
      "Epoch [407], train_loss: 0.0000, val_loss: 3.2730, val_acc: 0.59574467\n",
      "Epoch [408], train_loss: 0.0000, val_loss: 3.2689, val_acc: 0.59574467\n",
      "Epoch [409], train_loss: 0.0000, val_loss: 3.2648, val_acc: 0.59574467\n",
      "Epoch [410], train_loss: 0.0000, val_loss: 3.2672, val_acc: 0.61702126\n",
      "Epoch [411], train_loss: 0.0000, val_loss: 3.2565, val_acc: 0.59574467\n",
      "Epoch [412], train_loss: 0.0000, val_loss: 3.2534, val_acc: 0.61702126\n",
      "Epoch [413], train_loss: 0.0000, val_loss: 3.2541, val_acc: 0.61702126\n",
      "Epoch [414], train_loss: 0.0000, val_loss: 3.2573, val_acc: 0.61702126\n",
      "Epoch [415], train_loss: 0.0000, val_loss: 3.2526, val_acc: 0.61702126\n",
      "Epoch [416], train_loss: 0.0000, val_loss: 3.2625, val_acc: 0.61702126\n",
      "Epoch [417], train_loss: 0.0000, val_loss: 3.2803, val_acc: 0.59574467\n",
      "Epoch [418], train_loss: 0.0000, val_loss: 3.2736, val_acc: 0.61702126\n",
      "Epoch [419], train_loss: 0.0000, val_loss: 3.2731, val_acc: 0.61702126\n",
      "Epoch [420], train_loss: 0.0000, val_loss: 3.2823, val_acc: 0.59574467\n",
      "Epoch [421], train_loss: 0.0000, val_loss: 3.2737, val_acc: 0.61702126\n",
      "Epoch [422], train_loss: 0.0000, val_loss: 3.2663, val_acc: 0.61702126\n",
      "Epoch [423], train_loss: 0.0000, val_loss: 3.2759, val_acc: 0.59574467\n",
      "Epoch [424], train_loss: 0.0000, val_loss: 3.2724, val_acc: 0.59574467\n",
      "Epoch [425], train_loss: 0.0000, val_loss: 3.2755, val_acc: 0.61702126\n",
      "Epoch [426], train_loss: 0.0000, val_loss: 3.2778, val_acc: 0.61702126\n",
      "Epoch [427], train_loss: 0.0000, val_loss: 3.2782, val_acc: 0.61702126\n",
      "Epoch [428], train_loss: 0.0000, val_loss: 3.2869, val_acc: 0.59574467\n",
      "Epoch [429], train_loss: 0.0000, val_loss: 3.2967, val_acc: 0.59574467\n",
      "Epoch [430], train_loss: 0.0000, val_loss: 3.2919, val_acc: 0.59574467\n",
      "Epoch [431], train_loss: 0.0000, val_loss: 3.2816, val_acc: 0.59574467\n",
      "Epoch [432], train_loss: 0.0000, val_loss: 3.2841, val_acc: 0.59574467\n",
      "Epoch [433], train_loss: 0.0000, val_loss: 3.2897, val_acc: 0.59574467\n",
      "Epoch [434], train_loss: 0.0000, val_loss: 3.3023, val_acc: 0.59574467\n",
      "Epoch [435], train_loss: 0.0000, val_loss: 3.3022, val_acc: 0.59574467\n",
      "Epoch [436], train_loss: 0.0000, val_loss: 3.2980, val_acc: 0.59574467\n",
      "Epoch [437], train_loss: 0.0000, val_loss: 3.2996, val_acc: 0.59574467\n",
      "Epoch [438], train_loss: 0.0000, val_loss: 3.2903, val_acc: 0.59574467\n",
      "Epoch [439], train_loss: 0.0000, val_loss: 3.2964, val_acc: 0.59574467\n",
      "Epoch [440], train_loss: 0.0000, val_loss: 3.2933, val_acc: 0.59574467\n",
      "Epoch [441], train_loss: 0.0000, val_loss: 3.2935, val_acc: 0.59574467\n",
      "Epoch [442], train_loss: 0.0000, val_loss: 3.2942, val_acc: 0.59574467\n",
      "Epoch [443], train_loss: 0.0000, val_loss: 3.2902, val_acc: 0.59574467\n",
      "Epoch [444], train_loss: 0.0000, val_loss: 3.2925, val_acc: 0.59574467\n",
      "Epoch [445], train_loss: 0.0000, val_loss: 3.2995, val_acc: 0.59574467\n",
      "Epoch [446], train_loss: 0.0000, val_loss: 3.2976, val_acc: 0.59574467\n",
      "Epoch [447], train_loss: 0.0000, val_loss: 3.2849, val_acc: 0.59574467\n",
      "Epoch [448], train_loss: 0.0000, val_loss: 3.2835, val_acc: 0.59574467\n",
      "Epoch [449], train_loss: 0.0000, val_loss: 3.2878, val_acc: 0.59574467\n",
      "Epoch [450], train_loss: 0.0000, val_loss: 3.2973, val_acc: 0.59574467\n",
      "Epoch [451], train_loss: 0.0000, val_loss: 3.3142, val_acc: 0.59574467\n",
      "Epoch [452], train_loss: 0.0000, val_loss: 3.3045, val_acc: 0.59574467\n",
      "Epoch [453], train_loss: 0.0000, val_loss: 3.3096, val_acc: 0.59574467\n",
      "Epoch [454], train_loss: 0.0000, val_loss: 3.3122, val_acc: 0.59574467\n",
      "Epoch [455], train_loss: 0.0000, val_loss: 3.3209, val_acc: 0.59574467\n",
      "Epoch [456], train_loss: 0.0000, val_loss: 3.3277, val_acc: 0.59574467\n",
      "Epoch [457], train_loss: 0.0000, val_loss: 3.3361, val_acc: 0.59574467\n",
      "Epoch [458], train_loss: 0.0000, val_loss: 3.3393, val_acc: 0.59574467\n",
      "Epoch [459], train_loss: 0.0000, val_loss: 3.3301, val_acc: 0.59574467\n",
      "Epoch [460], train_loss: 0.0000, val_loss: 3.3195, val_acc: 0.59574467\n",
      "Epoch [461], train_loss: 0.0000, val_loss: 3.3215, val_acc: 0.59574467\n",
      "Epoch [462], train_loss: 0.0000, val_loss: 3.3345, val_acc: 0.59574467\n",
      "Epoch [463], train_loss: 0.0000, val_loss: 3.3537, val_acc: 0.59574467\n",
      "Epoch [464], train_loss: 0.0000, val_loss: 3.3561, val_acc: 0.59574467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [465], train_loss: 0.0000, val_loss: 3.3548, val_acc: 0.59574467\n",
      "Epoch [466], train_loss: 0.0000, val_loss: 3.3490, val_acc: 0.59574467\n",
      "Epoch [467], train_loss: 0.0000, val_loss: 3.3481, val_acc: 0.59574467\n",
      "Epoch [468], train_loss: 0.0000, val_loss: 3.3594, val_acc: 0.59574467\n",
      "Epoch [469], train_loss: 0.0000, val_loss: 3.3547, val_acc: 0.59574467\n",
      "Epoch [470], train_loss: 0.0000, val_loss: 3.3582, val_acc: 0.59574467\n",
      "Epoch [471], train_loss: 0.0000, val_loss: 3.3539, val_acc: 0.59574467\n",
      "Epoch [472], train_loss: 0.0000, val_loss: 3.3517, val_acc: 0.59574467\n",
      "Epoch [473], train_loss: 0.0000, val_loss: 3.3448, val_acc: 0.59574467\n",
      "Epoch [474], train_loss: 0.0000, val_loss: 3.3360, val_acc: 0.59574467\n",
      "Epoch [475], train_loss: 0.0000, val_loss: 3.3413, val_acc: 0.59574467\n",
      "Epoch [476], train_loss: 0.0000, val_loss: 3.3367, val_acc: 0.59574467\n",
      "Epoch [477], train_loss: 0.0000, val_loss: 3.3366, val_acc: 0.59574467\n",
      "Epoch [478], train_loss: 0.0000, val_loss: 3.3204, val_acc: 0.59574467\n",
      "Epoch [479], train_loss: 0.0000, val_loss: 3.3293, val_acc: 0.59574467\n",
      "Epoch [480], train_loss: 0.0000, val_loss: 3.3297, val_acc: 0.59574467\n",
      "Epoch [481], train_loss: 0.0000, val_loss: 3.3319, val_acc: 0.59574467\n",
      "Epoch [482], train_loss: 0.0000, val_loss: 3.3341, val_acc: 0.59574467\n",
      "Epoch [483], train_loss: 0.0000, val_loss: 3.3555, val_acc: 0.59574467\n",
      "Epoch [484], train_loss: 0.0000, val_loss: 3.3514, val_acc: 0.59574467\n",
      "Epoch [485], train_loss: 0.0000, val_loss: 3.3487, val_acc: 0.59574467\n",
      "Epoch [486], train_loss: 0.0000, val_loss: 3.3566, val_acc: 0.59574467\n",
      "Epoch [487], train_loss: 0.0000, val_loss: 3.3571, val_acc: 0.59574467\n",
      "Epoch [488], train_loss: 0.0000, val_loss: 3.3532, val_acc: 0.59574467\n",
      "Epoch [489], train_loss: 0.0000, val_loss: 3.3671, val_acc: 0.59574467\n",
      "Epoch [490], train_loss: 0.0000, val_loss: 3.3597, val_acc: 0.59574467\n",
      "Epoch [491], train_loss: 0.0000, val_loss: 3.3553, val_acc: 0.59574467\n",
      "Epoch [492], train_loss: 0.0000, val_loss: 3.3530, val_acc: 0.59574467\n",
      "Epoch [493], train_loss: 0.0000, val_loss: 3.3525, val_acc: 0.59574467\n",
      "Epoch [494], train_loss: 0.0000, val_loss: 3.3556, val_acc: 0.59574467\n",
      "Epoch [495], train_loss: 0.0000, val_loss: 3.3551, val_acc: 0.59574467\n",
      "Epoch [496], train_loss: 0.0000, val_loss: 3.3650, val_acc: 0.59574467\n",
      "Epoch [497], train_loss: 0.0000, val_loss: 3.3625, val_acc: 0.59574467\n",
      "Epoch [498], train_loss: 0.0000, val_loss: 3.3612, val_acc: 0.59574467\n",
      "Epoch [499], train_loss: 0.0000, val_loss: 3.3676, val_acc: 0.59574467\n",
      "Epoch [500], train_loss: 0.0000, val_loss: 3.3704, val_acc: 0.59574467\n",
      "Epoch [501], train_loss: 0.0000, val_loss: 3.3655, val_acc: 0.59574467\n",
      "Epoch [502], train_loss: 0.0000, val_loss: 3.3722, val_acc: 0.59574467\n",
      "Epoch [503], train_loss: 0.0000, val_loss: 3.3605, val_acc: 0.59574467\n",
      "Epoch [504], train_loss: 0.0000, val_loss: 3.3458, val_acc: 0.59574467\n",
      "Epoch [505], train_loss: 0.0000, val_loss: 3.3541, val_acc: 0.59574467\n",
      "Epoch [506], train_loss: 0.0000, val_loss: 3.3537, val_acc: 0.59574467\n",
      "Epoch [507], train_loss: 0.0000, val_loss: 3.3581, val_acc: 0.59574467\n",
      "Epoch [508], train_loss: 0.0000, val_loss: 3.3508, val_acc: 0.59574467\n",
      "Epoch [509], train_loss: 0.0000, val_loss: 3.3447, val_acc: 0.59574467\n",
      "Epoch [510], train_loss: 0.0000, val_loss: 3.3445, val_acc: 0.59574467\n",
      "Epoch [511], train_loss: 0.0000, val_loss: 3.3618, val_acc: 0.59574467\n",
      "Epoch [512], train_loss: 0.0000, val_loss: 3.3631, val_acc: 0.59574467\n",
      "Epoch [513], train_loss: 0.0000, val_loss: 3.3643, val_acc: 0.59574467\n",
      "Epoch [514], train_loss: 0.0000, val_loss: 3.3491, val_acc: 0.59574467\n",
      "Epoch [515], train_loss: 0.0000, val_loss: 3.3486, val_acc: 0.59574467\n",
      "Epoch [516], train_loss: 0.0000, val_loss: 3.3694, val_acc: 0.59574467\n",
      "Epoch [517], train_loss: 0.0000, val_loss: 3.3689, val_acc: 0.59574467\n",
      "Epoch [518], train_loss: 0.0000, val_loss: 3.3684, val_acc: 0.59574467\n",
      "Epoch [519], train_loss: 0.0000, val_loss: 3.3697, val_acc: 0.59574467\n",
      "Epoch [520], train_loss: 0.0000, val_loss: 3.3637, val_acc: 0.59574467\n",
      "Epoch [521], train_loss: 0.0000, val_loss: 3.3582, val_acc: 0.59574467\n",
      "Epoch [522], train_loss: 0.0000, val_loss: 3.3598, val_acc: 0.59574467\n",
      "Epoch [523], train_loss: 0.0000, val_loss: 3.3549, val_acc: 0.59574467\n",
      "Epoch [524], train_loss: 0.0000, val_loss: 3.3600, val_acc: 0.59574467\n",
      "Epoch [525], train_loss: 0.0000, val_loss: 3.3593, val_acc: 0.59574467\n",
      "Epoch [526], train_loss: 0.0000, val_loss: 3.3622, val_acc: 0.59574467\n",
      "Epoch [527], train_loss: 0.0000, val_loss: 3.3524, val_acc: 0.59574467\n",
      "Epoch [528], train_loss: 0.0000, val_loss: 3.3600, val_acc: 0.59574467\n",
      "Epoch [529], train_loss: 0.0000, val_loss: 3.3647, val_acc: 0.59574467\n",
      "Epoch [530], train_loss: 0.0000, val_loss: 3.3777, val_acc: 0.59574467\n",
      "Epoch [531], train_loss: 0.0000, val_loss: 3.3772, val_acc: 0.59574467\n",
      "Epoch [532], train_loss: 0.0000, val_loss: 3.3843, val_acc: 0.59574467\n",
      "Epoch [533], train_loss: 0.0000, val_loss: 3.3891, val_acc: 0.59574467\n",
      "Epoch [534], train_loss: 0.0000, val_loss: 3.3890, val_acc: 0.59574467\n",
      "Epoch [535], train_loss: 0.0000, val_loss: 3.3895, val_acc: 0.59574467\n",
      "Epoch [536], train_loss: 0.0000, val_loss: 3.3870, val_acc: 0.59574467\n",
      "Epoch [537], train_loss: 0.0000, val_loss: 3.3810, val_acc: 0.59574467\n",
      "Epoch [538], train_loss: 0.0000, val_loss: 3.3737, val_acc: 0.59574467\n",
      "Epoch [539], train_loss: 0.0000, val_loss: 3.3757, val_acc: 0.59574467\n",
      "Epoch [540], train_loss: 0.0000, val_loss: 3.3742, val_acc: 0.59574467\n",
      "Epoch [541], train_loss: 0.0000, val_loss: 3.3774, val_acc: 0.59574467\n",
      "Epoch [542], train_loss: 0.0000, val_loss: 3.3716, val_acc: 0.59574467\n",
      "Epoch [543], train_loss: 0.0000, val_loss: 3.3804, val_acc: 0.59574467\n",
      "Epoch [544], train_loss: 0.0000, val_loss: 3.3745, val_acc: 0.59574467\n",
      "Epoch [545], train_loss: 0.0000, val_loss: 3.3754, val_acc: 0.61702126\n",
      "Epoch [546], train_loss: 0.0000, val_loss: 3.3906, val_acc: 0.59574467\n",
      "Epoch [547], train_loss: 0.0000, val_loss: 3.3857, val_acc: 0.59574467\n",
      "Epoch [548], train_loss: 0.0000, val_loss: 3.3845, val_acc: 0.59574467\n",
      "Epoch [549], train_loss: 0.0000, val_loss: 3.3830, val_acc: 0.59574467\n",
      "Epoch [550], train_loss: 0.0000, val_loss: 3.3795, val_acc: 0.59574467\n",
      "Epoch [551], train_loss: 0.0000, val_loss: 3.3720, val_acc: 0.59574467\n",
      "Epoch [552], train_loss: 0.0000, val_loss: 3.3964, val_acc: 0.59574467\n",
      "Epoch [553], train_loss: 0.0000, val_loss: 3.4001, val_acc: 0.59574467\n",
      "Epoch [554], train_loss: 0.0000, val_loss: 3.3991, val_acc: 0.59574467\n",
      "Epoch [555], train_loss: 0.0000, val_loss: 3.3948, val_acc: 0.59574467\n",
      "Epoch [556], train_loss: 0.0000, val_loss: 3.3828, val_acc: 0.59574467\n",
      "Epoch [557], train_loss: 0.0000, val_loss: 3.3877, val_acc: 0.59574467\n",
      "Epoch [558], train_loss: 0.0000, val_loss: 3.3974, val_acc: 0.59574467\n",
      "Epoch [559], train_loss: 0.0000, val_loss: 3.3982, val_acc: 0.59574467\n",
      "Epoch [560], train_loss: 0.0000, val_loss: 3.3878, val_acc: 0.59574467\n",
      "Epoch [561], train_loss: 0.0000, val_loss: 3.3910, val_acc: 0.59574467\n",
      "Epoch [562], train_loss: 0.0000, val_loss: 3.3829, val_acc: 0.59574467\n",
      "Epoch [563], train_loss: 0.0000, val_loss: 3.3728, val_acc: 0.59574467\n",
      "Epoch [564], train_loss: 0.0000, val_loss: 3.3751, val_acc: 0.59574467\n",
      "Epoch [565], train_loss: 0.0000, val_loss: 3.3586, val_acc: 0.61702126\n",
      "Epoch [566], train_loss: 0.0000, val_loss: 3.3597, val_acc: 0.61702126\n",
      "Epoch [567], train_loss: 0.0000, val_loss: 3.3596, val_acc: 0.61702126\n",
      "Epoch [568], train_loss: 0.0000, val_loss: 3.3666, val_acc: 0.61702126\n",
      "Epoch [569], train_loss: 0.0000, val_loss: 3.3655, val_acc: 0.61702126\n",
      "Epoch [570], train_loss: 0.0000, val_loss: 3.3590, val_acc: 0.61702126\n",
      "Epoch [571], train_loss: 0.0000, val_loss: 3.3551, val_acc: 0.61702126\n",
      "Epoch [572], train_loss: 0.0000, val_loss: 3.3539, val_acc: 0.61702126\n",
      "Epoch [573], train_loss: 0.0000, val_loss: 3.3534, val_acc: 0.61702126\n",
      "Epoch [574], train_loss: 0.0000, val_loss: 3.3502, val_acc: 0.61702126\n",
      "Epoch [575], train_loss: 0.0000, val_loss: 3.3583, val_acc: 0.61702126\n",
      "Epoch [576], train_loss: 0.0000, val_loss: 3.3672, val_acc: 0.61702126\n",
      "Epoch [577], train_loss: 0.0000, val_loss: 3.3658, val_acc: 0.61702126\n",
      "Epoch [578], train_loss: 0.0000, val_loss: 3.3653, val_acc: 0.61702126\n",
      "Epoch [579], train_loss: 0.0000, val_loss: 3.3753, val_acc: 0.61702126\n",
      "Epoch [580], train_loss: 0.0000, val_loss: 3.3773, val_acc: 0.61702126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [581], train_loss: 0.0000, val_loss: 3.3790, val_acc: 0.59574467\n",
      "Epoch [582], train_loss: 0.0000, val_loss: 3.3833, val_acc: 0.59574467\n",
      "Epoch [583], train_loss: 0.0000, val_loss: 3.3788, val_acc: 0.59574467\n",
      "Epoch [584], train_loss: 0.0000, val_loss: 3.3810, val_acc: 0.59574467\n",
      "Epoch [585], train_loss: 0.0000, val_loss: 3.3759, val_acc: 0.59574467\n",
      "Epoch [586], train_loss: 0.0000, val_loss: 3.3851, val_acc: 0.59574467\n",
      "Epoch [587], train_loss: 0.0000, val_loss: 3.3840, val_acc: 0.59574467\n",
      "Epoch [588], train_loss: 0.0000, val_loss: 3.3796, val_acc: 0.59574467\n",
      "Epoch [589], train_loss: 0.0000, val_loss: 3.3775, val_acc: 0.59574467\n",
      "Epoch [590], train_loss: 0.0000, val_loss: 3.3804, val_acc: 0.59574467\n",
      "Epoch [591], train_loss: 0.0000, val_loss: 3.3834, val_acc: 0.59574467\n",
      "Epoch [592], train_loss: 0.0000, val_loss: 3.3938, val_acc: 0.59574467\n",
      "Epoch [593], train_loss: 0.0000, val_loss: 3.3896, val_acc: 0.59574467\n",
      "Epoch [594], train_loss: 0.0000, val_loss: 3.3961, val_acc: 0.59574467\n",
      "Epoch [595], train_loss: 0.0000, val_loss: 3.4085, val_acc: 0.59574467\n",
      "Epoch [596], train_loss: 0.0000, val_loss: 3.4131, val_acc: 0.59574467\n",
      "Epoch [597], train_loss: 0.0000, val_loss: 3.3973, val_acc: 0.59574467\n",
      "Epoch [598], train_loss: 0.0000, val_loss: 3.4022, val_acc: 0.59574467\n",
      "Epoch [599], train_loss: 0.0000, val_loss: 3.4181, val_acc: 0.59574467\n",
      "Epoch [600], train_loss: 0.0000, val_loss: 3.4134, val_acc: 0.59574467\n",
      "Epoch [601], train_loss: 0.0000, val_loss: 3.4168, val_acc: 0.59574467\n",
      "Epoch [602], train_loss: 0.0000, val_loss: 3.4179, val_acc: 0.59574467\n",
      "Epoch [603], train_loss: 0.0000, val_loss: 3.4069, val_acc: 0.59574467\n",
      "Epoch [604], train_loss: 0.0000, val_loss: 3.4091, val_acc: 0.59574467\n",
      "Epoch [605], train_loss: 0.0000, val_loss: 3.4026, val_acc: 0.59574467\n",
      "Epoch [606], train_loss: 0.0000, val_loss: 3.3973, val_acc: 0.59574467\n",
      "Epoch [607], train_loss: 0.0000, val_loss: 3.3986, val_acc: 0.59574467\n",
      "Epoch [608], train_loss: 0.0000, val_loss: 3.3980, val_acc: 0.61702126\n",
      "Epoch [609], train_loss: 0.0000, val_loss: 3.3992, val_acc: 0.61702126\n",
      "Epoch [610], train_loss: 0.0000, val_loss: 3.4062, val_acc: 0.61702126\n",
      "Epoch [611], train_loss: 0.0000, val_loss: 3.4098, val_acc: 0.59574467\n",
      "Epoch [612], train_loss: 0.0000, val_loss: 3.4090, val_acc: 0.59574467\n",
      "Epoch [613], train_loss: 0.0000, val_loss: 3.4107, val_acc: 0.59574467\n",
      "Epoch [614], train_loss: 0.0000, val_loss: 3.4032, val_acc: 0.59574467\n",
      "Epoch [615], train_loss: 0.0000, val_loss: 3.4034, val_acc: 0.59574467\n",
      "Epoch [616], train_loss: 0.0000, val_loss: 3.4306, val_acc: 0.59574467\n",
      "Epoch [617], train_loss: 0.0000, val_loss: 3.4280, val_acc: 0.59574467\n",
      "Epoch [618], train_loss: 0.0000, val_loss: 3.4189, val_acc: 0.59574467\n",
      "Epoch [619], train_loss: 0.0000, val_loss: 3.4165, val_acc: 0.59574467\n",
      "Epoch [620], train_loss: 0.0000, val_loss: 3.4151, val_acc: 0.59574467\n",
      "Epoch [621], train_loss: 0.0000, val_loss: 3.4113, val_acc: 0.61702126\n",
      "Epoch [622], train_loss: 0.0000, val_loss: 3.4137, val_acc: 0.61702126\n",
      "Epoch [623], train_loss: 0.0000, val_loss: 3.4151, val_acc: 0.59574467\n",
      "Epoch [624], train_loss: 0.0000, val_loss: 3.4119, val_acc: 0.59574467\n",
      "Epoch [625], train_loss: 0.0000, val_loss: 3.4125, val_acc: 0.59574467\n",
      "Epoch [626], train_loss: 0.0000, val_loss: 3.4117, val_acc: 0.59574467\n",
      "Epoch [627], train_loss: 0.0000, val_loss: 3.4171, val_acc: 0.59574467\n",
      "Epoch [628], train_loss: 0.0000, val_loss: 3.4162, val_acc: 0.59574467\n",
      "Epoch [629], train_loss: 0.0000, val_loss: 3.4099, val_acc: 0.59574467\n",
      "Epoch [630], train_loss: 0.0000, val_loss: 3.4105, val_acc: 0.59574467\n",
      "Epoch [631], train_loss: 0.0000, val_loss: 3.4064, val_acc: 0.59574467\n",
      "Epoch [632], train_loss: 0.0000, val_loss: 3.4009, val_acc: 0.59574467\n",
      "Epoch [633], train_loss: 0.0000, val_loss: 3.3997, val_acc: 0.59574467\n",
      "Epoch [634], train_loss: 0.0000, val_loss: 3.4048, val_acc: 0.59574467\n",
      "Epoch [635], train_loss: 0.0000, val_loss: 3.4089, val_acc: 0.61702126\n",
      "Epoch [636], train_loss: 0.0000, val_loss: 3.4110, val_acc: 0.61702126\n",
      "Epoch [637], train_loss: 0.0000, val_loss: 3.4155, val_acc: 0.61702126\n",
      "Epoch [638], train_loss: 0.0000, val_loss: 3.4219, val_acc: 0.59574467\n",
      "Epoch [639], train_loss: 0.0000, val_loss: 3.4277, val_acc: 0.59574467\n",
      "Epoch [640], train_loss: 0.0000, val_loss: 3.4367, val_acc: 0.59574467\n",
      "Epoch [641], train_loss: 0.0000, val_loss: 3.4246, val_acc: 0.59574467\n",
      "Epoch [642], train_loss: 0.0000, val_loss: 3.4239, val_acc: 0.59574467\n",
      "Epoch [643], train_loss: 0.0000, val_loss: 3.4258, val_acc: 0.59574467\n",
      "Epoch [644], train_loss: 0.0000, val_loss: 3.4252, val_acc: 0.59574467\n",
      "Epoch [645], train_loss: 0.0000, val_loss: 3.4217, val_acc: 0.61702126\n",
      "Epoch [646], train_loss: 0.0000, val_loss: 3.4201, val_acc: 0.61702126\n",
      "Epoch [647], train_loss: 0.0000, val_loss: 3.4273, val_acc: 0.59574467\n",
      "Epoch [648], train_loss: 0.0000, val_loss: 3.4352, val_acc: 0.61702126\n",
      "Epoch [649], train_loss: 0.0000, val_loss: 3.4297, val_acc: 0.61702126\n",
      "Epoch [650], train_loss: 0.0000, val_loss: 3.4191, val_acc: 0.61702126\n",
      "Epoch [651], train_loss: 0.0000, val_loss: 3.4243, val_acc: 0.61702126\n",
      "Epoch [652], train_loss: 0.0000, val_loss: 3.4163, val_acc: 0.61702126\n",
      "Epoch [653], train_loss: 0.0000, val_loss: 3.4127, val_acc: 0.61702126\n",
      "Epoch [654], train_loss: 0.0000, val_loss: 3.4163, val_acc: 0.61702126\n",
      "Epoch [655], train_loss: 0.0000, val_loss: 3.4210, val_acc: 0.61702126\n",
      "Epoch [656], train_loss: 0.0000, val_loss: 3.4261, val_acc: 0.61702126\n",
      "Epoch [657], train_loss: 0.0000, val_loss: 3.4380, val_acc: 0.61702126\n",
      "Epoch [658], train_loss: 0.0000, val_loss: 3.4341, val_acc: 0.61702126\n",
      "Epoch [659], train_loss: 0.0000, val_loss: 3.4379, val_acc: 0.61702126\n",
      "Epoch [660], train_loss: 0.0000, val_loss: 3.4331, val_acc: 0.61702126\n",
      "Epoch [661], train_loss: 0.0000, val_loss: 3.4323, val_acc: 0.61702126\n",
      "Epoch [662], train_loss: 0.0000, val_loss: 3.4351, val_acc: 0.61702126\n",
      "Epoch [663], train_loss: 0.0000, val_loss: 3.4410, val_acc: 0.61702126\n",
      "Epoch [664], train_loss: 0.0000, val_loss: 3.4473, val_acc: 0.59574467\n",
      "Epoch [665], train_loss: 0.0000, val_loss: 3.4671, val_acc: 0.59574467\n",
      "Epoch [666], train_loss: 0.0000, val_loss: 3.4708, val_acc: 0.59574467\n",
      "Epoch [667], train_loss: 0.0000, val_loss: 3.4606, val_acc: 0.61702126\n",
      "Epoch [668], train_loss: 0.0000, val_loss: 3.4540, val_acc: 0.61702126\n",
      "Epoch [669], train_loss: 0.0000, val_loss: 3.4590, val_acc: 0.61702126\n",
      "Epoch [670], train_loss: 0.0000, val_loss: 3.4492, val_acc: 0.61702126\n",
      "Epoch [671], train_loss: 0.0000, val_loss: 3.4515, val_acc: 0.61702126\n",
      "Epoch [672], train_loss: 0.0000, val_loss: 3.4844, val_acc: 0.59574467\n",
      "Epoch [673], train_loss: 0.0000, val_loss: 3.4819, val_acc: 0.59574467\n",
      "Epoch [674], train_loss: 0.0000, val_loss: 3.4844, val_acc: 0.59574467\n",
      "Epoch [675], train_loss: 0.0000, val_loss: 3.4806, val_acc: 0.59574467\n",
      "Epoch [676], train_loss: 0.0000, val_loss: 3.4800, val_acc: 0.59574467\n",
      "Epoch [677], train_loss: 0.0000, val_loss: 3.4812, val_acc: 0.59574467\n",
      "Epoch [678], train_loss: 0.0000, val_loss: 3.4817, val_acc: 0.61702126\n",
      "Epoch [679], train_loss: 0.0000, val_loss: 3.4823, val_acc: 0.61702126\n",
      "Epoch [680], train_loss: 0.0000, val_loss: 3.4783, val_acc: 0.61702126\n",
      "Epoch [681], train_loss: 0.0000, val_loss: 3.4849, val_acc: 0.59574467\n",
      "Epoch [682], train_loss: 0.0000, val_loss: 3.4894, val_acc: 0.61702126\n",
      "Epoch [683], train_loss: 0.0000, val_loss: 3.4952, val_acc: 0.61702126\n",
      "Epoch [684], train_loss: 0.0000, val_loss: 3.4950, val_acc: 0.61702126\n",
      "Epoch [685], train_loss: 0.0000, val_loss: 3.4959, val_acc: 0.61702126\n",
      "Epoch [686], train_loss: 0.0000, val_loss: 3.5046, val_acc: 0.59574467\n",
      "Epoch [687], train_loss: 0.0000, val_loss: 3.5065, val_acc: 0.59574467\n",
      "Epoch [688], train_loss: 0.0000, val_loss: 3.5077, val_acc: 0.59574467\n",
      "Epoch [689], train_loss: 0.0000, val_loss: 3.5070, val_acc: 0.59574467\n",
      "Epoch [690], train_loss: 0.0000, val_loss: 3.5031, val_acc: 0.59574467\n",
      "Epoch [691], train_loss: 0.0000, val_loss: 3.4950, val_acc: 0.61702126\n",
      "Epoch [692], train_loss: 0.0000, val_loss: 3.4973, val_acc: 0.61702126\n",
      "Epoch [693], train_loss: 0.0000, val_loss: 3.4995, val_acc: 0.61702126\n",
      "Epoch [694], train_loss: 0.0000, val_loss: 3.4973, val_acc: 0.61702126\n",
      "Epoch [695], train_loss: 0.0000, val_loss: 3.4969, val_acc: 0.61702126\n",
      "Epoch [696], train_loss: 0.0000, val_loss: 3.4901, val_acc: 0.61702126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [697], train_loss: 0.0000, val_loss: 3.4892, val_acc: 0.61702126\n",
      "Epoch [698], train_loss: 0.0000, val_loss: 3.5058, val_acc: 0.61702126\n",
      "Epoch [699], train_loss: 0.0000, val_loss: 3.4951, val_acc: 0.61702126\n",
      "Epoch [700], train_loss: 0.0000, val_loss: 3.4950, val_acc: 0.61702126\n",
      "Epoch [701], train_loss: 0.0000, val_loss: 3.4889, val_acc: 0.61702126\n",
      "Epoch [702], train_loss: 0.0000, val_loss: 3.4943, val_acc: 0.61702126\n",
      "Epoch [703], train_loss: 0.0000, val_loss: 3.5004, val_acc: 0.61702126\n",
      "Epoch [704], train_loss: 0.0000, val_loss: 3.4980, val_acc: 0.61702126\n",
      "Epoch [705], train_loss: 0.0000, val_loss: 3.4944, val_acc: 0.61702126\n",
      "Epoch [706], train_loss: 0.0000, val_loss: 3.4906, val_acc: 0.61702126\n",
      "Epoch [707], train_loss: 0.0000, val_loss: 3.4929, val_acc: 0.61702126\n",
      "Epoch [708], train_loss: 0.0000, val_loss: 3.4977, val_acc: 0.61702126\n",
      "Epoch [709], train_loss: 0.0000, val_loss: 3.4916, val_acc: 0.61702126\n",
      "Epoch [710], train_loss: 0.0000, val_loss: 3.4935, val_acc: 0.61702126\n",
      "Epoch [711], train_loss: 0.0000, val_loss: 3.4961, val_acc: 0.61702126\n",
      "Epoch [712], train_loss: 0.0000, val_loss: 3.5049, val_acc: 0.61702126\n",
      "Epoch [713], train_loss: 0.0000, val_loss: 3.5150, val_acc: 0.59574467\n",
      "Epoch [714], train_loss: 0.0000, val_loss: 3.5117, val_acc: 0.59574467\n",
      "Epoch [715], train_loss: 0.0000, val_loss: 3.5144, val_acc: 0.59574467\n",
      "Epoch [716], train_loss: 0.0000, val_loss: 3.5024, val_acc: 0.59574467\n",
      "Epoch [717], train_loss: 0.0000, val_loss: 3.5010, val_acc: 0.61702126\n",
      "Epoch [718], train_loss: 0.0000, val_loss: 3.5165, val_acc: 0.59574467\n",
      "Epoch [719], train_loss: 0.0000, val_loss: 3.5151, val_acc: 0.59574467\n",
      "Epoch [720], train_loss: 0.0000, val_loss: 3.5205, val_acc: 0.59574467\n",
      "Epoch [721], train_loss: 0.0000, val_loss: 3.5088, val_acc: 0.59574467\n",
      "Epoch [722], train_loss: 0.0000, val_loss: 3.5099, val_acc: 0.59574467\n",
      "Epoch [723], train_loss: 0.0000, val_loss: 3.5037, val_acc: 0.61702126\n",
      "Epoch [724], train_loss: 0.0000, val_loss: 3.5017, val_acc: 0.61702126\n",
      "Epoch [725], train_loss: 0.0000, val_loss: 3.5155, val_acc: 0.59574467\n",
      "Epoch [726], train_loss: 0.0000, val_loss: 3.5123, val_acc: 0.59574467\n",
      "Epoch [727], train_loss: 0.0000, val_loss: 3.5109, val_acc: 0.59574467\n",
      "Epoch [728], train_loss: 0.0000, val_loss: 3.5321, val_acc: 0.59574467\n",
      "Epoch [729], train_loss: 0.0000, val_loss: 3.5264, val_acc: 0.59574467\n",
      "Epoch [730], train_loss: 0.0000, val_loss: 3.5241, val_acc: 0.59574467\n",
      "Epoch [731], train_loss: 0.0000, val_loss: 3.5155, val_acc: 0.59574467\n",
      "Epoch [732], train_loss: 0.0000, val_loss: 3.5100, val_acc: 0.59574467\n",
      "Epoch [733], train_loss: 0.0000, val_loss: 3.4994, val_acc: 0.59574467\n",
      "Epoch [734], train_loss: 0.0000, val_loss: 3.5024, val_acc: 0.61702126\n",
      "Epoch [735], train_loss: 0.0000, val_loss: 3.5180, val_acc: 0.59574467\n",
      "Epoch [736], train_loss: 0.0000, val_loss: 3.5173, val_acc: 0.61702126\n",
      "Epoch [737], train_loss: 0.0000, val_loss: 3.5229, val_acc: 0.61702126\n",
      "Epoch [738], train_loss: 0.0000, val_loss: 3.5299, val_acc: 0.61702126\n",
      "Epoch [739], train_loss: 0.0000, val_loss: 3.5407, val_acc: 0.61702126\n",
      "Epoch [740], train_loss: 0.0000, val_loss: 3.5327, val_acc: 0.61702126\n",
      "Epoch [741], train_loss: 0.0000, val_loss: 3.5483, val_acc: 0.61702126\n",
      "Epoch [742], train_loss: 0.0000, val_loss: 3.5434, val_acc: 0.61702126\n",
      "Epoch [743], train_loss: 0.0000, val_loss: 3.5399, val_acc: 0.61702126\n",
      "Epoch [744], train_loss: 0.0000, val_loss: 3.5494, val_acc: 0.61702126\n",
      "Epoch [745], train_loss: 0.0000, val_loss: 3.5483, val_acc: 0.61702126\n",
      "Epoch [746], train_loss: 0.0000, val_loss: 3.5370, val_acc: 0.59574467\n",
      "Epoch [747], train_loss: 0.0000, val_loss: 3.5339, val_acc: 0.61702126\n",
      "Epoch [748], train_loss: 0.0000, val_loss: 3.5201, val_acc: 0.61702126\n",
      "Epoch [749], train_loss: 0.0000, val_loss: 3.5180, val_acc: 0.61702126\n",
      "Epoch [750], train_loss: 0.0000, val_loss: 3.5333, val_acc: 0.61702126\n",
      "Epoch [751], train_loss: 0.0000, val_loss: 3.5383, val_acc: 0.61702126\n",
      "Epoch [752], train_loss: 0.0000, val_loss: 3.5319, val_acc: 0.61702126\n",
      "Epoch [753], train_loss: 0.0000, val_loss: 3.5239, val_acc: 0.61702126\n",
      "Epoch [754], train_loss: 0.0000, val_loss: 3.5217, val_acc: 0.61702126\n",
      "Epoch [755], train_loss: 0.0000, val_loss: 3.5241, val_acc: 0.61702126\n",
      "Epoch [756], train_loss: 0.0000, val_loss: 3.5179, val_acc: 0.61702126\n",
      "Epoch [757], train_loss: 0.0000, val_loss: 3.5295, val_acc: 0.61702126\n",
      "Epoch [758], train_loss: 0.0000, val_loss: 3.5190, val_acc: 0.61702126\n",
      "Epoch [759], train_loss: 0.0000, val_loss: 3.5295, val_acc: 0.61702126\n",
      "Epoch [760], train_loss: 0.0000, val_loss: 3.5327, val_acc: 0.61702126\n",
      "Epoch [761], train_loss: 0.0000, val_loss: 3.5280, val_acc: 0.61702126\n",
      "Epoch [762], train_loss: 0.0000, val_loss: 3.5316, val_acc: 0.61702126\n",
      "Epoch [763], train_loss: 0.0000, val_loss: 3.5351, val_acc: 0.61702126\n",
      "Epoch [764], train_loss: 0.0000, val_loss: 3.5301, val_acc: 0.61702126\n",
      "Epoch [765], train_loss: 0.0000, val_loss: 3.5231, val_acc: 0.61702126\n",
      "Epoch [766], train_loss: 0.0000, val_loss: 3.5256, val_acc: 0.61702126\n",
      "Epoch [767], train_loss: 0.0000, val_loss: 3.5271, val_acc: 0.61702126\n",
      "Epoch [768], train_loss: 0.0000, val_loss: 3.5352, val_acc: 0.61702126\n",
      "Epoch [769], train_loss: 0.0000, val_loss: 3.5390, val_acc: 0.61702126\n",
      "Epoch [770], train_loss: 0.0000, val_loss: 3.5363, val_acc: 0.61702126\n",
      "Epoch [771], train_loss: 0.0000, val_loss: 3.5358, val_acc: 0.61702126\n",
      "Epoch [772], train_loss: 0.0000, val_loss: 3.5318, val_acc: 0.61702126\n",
      "Epoch [773], train_loss: 0.0000, val_loss: 3.5276, val_acc: 0.61702126\n",
      "Epoch [774], train_loss: 0.0000, val_loss: 3.5323, val_acc: 0.61702126\n",
      "Epoch [775], train_loss: 0.0000, val_loss: 3.5400, val_acc: 0.61702126\n",
      "Epoch [776], train_loss: 0.0000, val_loss: 3.5267, val_acc: 0.61702126\n",
      "Epoch [777], train_loss: 0.0000, val_loss: 3.5293, val_acc: 0.61702126\n",
      "Epoch [778], train_loss: 0.0000, val_loss: 3.5355, val_acc: 0.61702126\n",
      "Epoch [779], train_loss: 0.0000, val_loss: 3.5300, val_acc: 0.61702126\n",
      "Epoch [780], train_loss: 0.0000, val_loss: 3.5397, val_acc: 0.61702126\n",
      "Epoch [781], train_loss: 0.0000, val_loss: 3.5377, val_acc: 0.61702126\n",
      "Epoch [782], train_loss: 0.0000, val_loss: 3.5419, val_acc: 0.61702126\n",
      "Epoch [783], train_loss: 0.0000, val_loss: 3.5464, val_acc: 0.61702126\n",
      "Epoch [784], train_loss: 0.0000, val_loss: 3.5496, val_acc: 0.59574467\n",
      "Epoch [785], train_loss: 0.0000, val_loss: 3.5419, val_acc: 0.61702126\n",
      "Epoch [786], train_loss: 0.0000, val_loss: 3.5365, val_acc: 0.61702126\n",
      "Epoch [787], train_loss: 0.0000, val_loss: 3.5436, val_acc: 0.61702126\n",
      "Epoch [788], train_loss: 0.0000, val_loss: 3.5355, val_acc: 0.61702126\n",
      "Epoch [789], train_loss: 0.0000, val_loss: 3.5388, val_acc: 0.61702126\n",
      "Epoch [790], train_loss: 0.0000, val_loss: 3.5391, val_acc: 0.61702126\n",
      "Epoch [791], train_loss: 0.0000, val_loss: 3.5423, val_acc: 0.61702126\n",
      "Epoch [792], train_loss: 0.0000, val_loss: 3.5419, val_acc: 0.61702126\n",
      "Epoch [793], train_loss: 0.0000, val_loss: 3.5504, val_acc: 0.59574467\n",
      "Epoch [794], train_loss: 0.0000, val_loss: 3.5453, val_acc: 0.61702126\n",
      "Epoch [795], train_loss: 0.0000, val_loss: 3.5504, val_acc: 0.61702126\n",
      "Epoch [796], train_loss: 0.0000, val_loss: 3.5423, val_acc: 0.61702126\n",
      "Epoch [797], train_loss: 0.0000, val_loss: 3.5422, val_acc: 0.61702126\n",
      "Epoch [798], train_loss: 0.0000, val_loss: 3.5423, val_acc: 0.61702126\n",
      "Epoch [799], train_loss: 0.0000, val_loss: 3.5580, val_acc: 0.59574467\n",
      "Epoch [800], train_loss: 0.0000, val_loss: 3.5563, val_acc: 0.61702126\n",
      "Epoch [801], train_loss: 0.0000, val_loss: 3.5547, val_acc: 0.61702126\n",
      "Epoch [802], train_loss: 0.0000, val_loss: 3.5620, val_acc: 0.61702126\n",
      "Epoch [803], train_loss: 0.0000, val_loss: 3.5613, val_acc: 0.61702126\n",
      "Epoch [804], train_loss: 0.0000, val_loss: 3.5634, val_acc: 0.61702126\n",
      "Epoch [805], train_loss: 0.0000, val_loss: 3.5670, val_acc: 0.61702126\n",
      "Epoch [806], train_loss: 0.0000, val_loss: 3.5598, val_acc: 0.61702126\n",
      "Epoch [807], train_loss: 0.0000, val_loss: 3.5625, val_acc: 0.61702126\n",
      "Epoch [808], train_loss: 0.0000, val_loss: 3.5643, val_acc: 0.61702126\n",
      "Epoch [809], train_loss: 0.0000, val_loss: 3.5562, val_acc: 0.61702126\n",
      "Epoch [810], train_loss: 0.0000, val_loss: 3.5638, val_acc: 0.61702126\n",
      "Epoch [811], train_loss: 0.0000, val_loss: 3.5668, val_acc: 0.61702126\n",
      "Epoch [812], train_loss: 0.0000, val_loss: 3.5625, val_acc: 0.61702126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [813], train_loss: 0.0000, val_loss: 3.5533, val_acc: 0.61702126\n",
      "Epoch [814], train_loss: 0.0000, val_loss: 3.5564, val_acc: 0.61702126\n",
      "Epoch [815], train_loss: 0.0000, val_loss: 3.5596, val_acc: 0.61702126\n",
      "Epoch [816], train_loss: 0.0000, val_loss: 3.5601, val_acc: 0.61702126\n",
      "Epoch [817], train_loss: 0.0000, val_loss: 3.5677, val_acc: 0.61702126\n",
      "Epoch [818], train_loss: 0.0000, val_loss: 3.5634, val_acc: 0.61702126\n",
      "Epoch [819], train_loss: 0.0000, val_loss: 3.5615, val_acc: 0.61702126\n",
      "Epoch [820], train_loss: 0.0000, val_loss: 3.5569, val_acc: 0.61702126\n",
      "Epoch [821], train_loss: 0.0000, val_loss: 3.5494, val_acc: 0.61702126\n",
      "Epoch [822], train_loss: 0.0000, val_loss: 3.5472, val_acc: 0.61702126\n",
      "Epoch [823], train_loss: 0.0000, val_loss: 3.5556, val_acc: 0.61702126\n",
      "Epoch [824], train_loss: 0.0000, val_loss: 3.5605, val_acc: 0.61702126\n",
      "Epoch [825], train_loss: 0.0000, val_loss: 3.5758, val_acc: 0.61702126\n",
      "Epoch [826], train_loss: 0.0000, val_loss: 3.5703, val_acc: 0.61702126\n",
      "Epoch [827], train_loss: 0.0000, val_loss: 3.5656, val_acc: 0.61702126\n",
      "Epoch [828], train_loss: 0.0000, val_loss: 3.5611, val_acc: 0.61702126\n",
      "Epoch [829], train_loss: 0.0000, val_loss: 3.5567, val_acc: 0.61702126\n",
      "Epoch [830], train_loss: 0.0000, val_loss: 3.5553, val_acc: 0.61702126\n",
      "Epoch [831], train_loss: 0.0000, val_loss: 3.5544, val_acc: 0.61702126\n",
      "Epoch [832], train_loss: 0.0000, val_loss: 3.5537, val_acc: 0.61702126\n",
      "Epoch [833], train_loss: 0.0000, val_loss: 3.5490, val_acc: 0.61702126\n",
      "Epoch [834], train_loss: 0.0000, val_loss: 3.5573, val_acc: 0.61702126\n",
      "Epoch [835], train_loss: 0.0000, val_loss: 3.6168, val_acc: 0.59574467\n",
      "Epoch [836], train_loss: 0.0000, val_loss: 3.6112, val_acc: 0.61702126\n",
      "Epoch [837], train_loss: 0.0000, val_loss: 3.6097, val_acc: 0.61702126\n",
      "Epoch [838], train_loss: 0.0000, val_loss: 3.6175, val_acc: 0.61702126\n",
      "Epoch [839], train_loss: 0.0000, val_loss: 3.6121, val_acc: 0.61702126\n",
      "Epoch [840], train_loss: 0.0000, val_loss: 3.6056, val_acc: 0.61702126\n",
      "Epoch [841], train_loss: 0.0000, val_loss: 3.6124, val_acc: 0.61702126\n",
      "Epoch [842], train_loss: 0.0000, val_loss: 3.6035, val_acc: 0.61702126\n",
      "Epoch [843], train_loss: 0.0000, val_loss: 3.5955, val_acc: 0.61702126\n",
      "Epoch [844], train_loss: 0.0000, val_loss: 3.5917, val_acc: 0.61702126\n",
      "Epoch [845], train_loss: 0.0000, val_loss: 3.5994, val_acc: 0.61702126\n",
      "Epoch [846], train_loss: 0.0000, val_loss: 3.5935, val_acc: 0.61702126\n",
      "Epoch [847], train_loss: 0.0000, val_loss: 3.5845, val_acc: 0.61702126\n",
      "Epoch [848], train_loss: 0.0000, val_loss: 3.5958, val_acc: 0.61702126\n",
      "Epoch [849], train_loss: 0.0000, val_loss: 3.5822, val_acc: 0.61702126\n",
      "Epoch [850], train_loss: 0.0000, val_loss: 3.5888, val_acc: 0.61702126\n",
      "Epoch [851], train_loss: 0.0000, val_loss: 3.6005, val_acc: 0.61702126\n",
      "Epoch [852], train_loss: 0.0000, val_loss: 3.5920, val_acc: 0.61702126\n",
      "Epoch [853], train_loss: 0.0000, val_loss: 3.5870, val_acc: 0.61702126\n",
      "Epoch [854], train_loss: 0.0000, val_loss: 3.5762, val_acc: 0.61702126\n",
      "Epoch [855], train_loss: 0.0000, val_loss: 3.5788, val_acc: 0.61702126\n",
      "Epoch [856], train_loss: 0.0000, val_loss: 3.5780, val_acc: 0.61702126\n",
      "Epoch [857], train_loss: 0.0000, val_loss: 3.5808, val_acc: 0.61702126\n",
      "Epoch [858], train_loss: 0.0000, val_loss: 3.5867, val_acc: 0.61702126\n",
      "Epoch [859], train_loss: 0.0000, val_loss: 3.5770, val_acc: 0.61702126\n",
      "Epoch [860], train_loss: 0.0000, val_loss: 3.5781, val_acc: 0.61702126\n",
      "Epoch [861], train_loss: 0.0001, val_loss: 3.6415, val_acc: 0.59574467\n",
      "Epoch [862], train_loss: 0.0000, val_loss: 3.6767, val_acc: 0.59574467\n",
      "Epoch [863], train_loss: 0.0000, val_loss: 3.6987, val_acc: 0.59574467\n",
      "Epoch [864], train_loss: 0.0000, val_loss: 3.7223, val_acc: 0.59574467\n",
      "Epoch [865], train_loss: 0.0000, val_loss: 3.7102, val_acc: 0.59574467\n",
      "Epoch [866], train_loss: 0.0000, val_loss: 3.7087, val_acc: 0.59574467\n",
      "Epoch [867], train_loss: 0.0000, val_loss: 3.7107, val_acc: 0.59574467\n",
      "Epoch [868], train_loss: 0.0000, val_loss: 3.7130, val_acc: 0.61702126\n",
      "Epoch [869], train_loss: 0.0000, val_loss: 3.7252, val_acc: 0.61702126\n",
      "Epoch [870], train_loss: 0.0000, val_loss: 3.7387, val_acc: 0.61702126\n",
      "Epoch [871], train_loss: 0.0000, val_loss: 3.7404, val_acc: 0.61702126\n",
      "Epoch [872], train_loss: 0.0000, val_loss: 3.7375, val_acc: 0.61702126\n",
      "Epoch [873], train_loss: 0.0000, val_loss: 3.7376, val_acc: 0.61702126\n",
      "Epoch [874], train_loss: 0.0000, val_loss: 3.7229, val_acc: 0.61702126\n",
      "Epoch [875], train_loss: 0.0000, val_loss: 3.7252, val_acc: 0.61702126\n",
      "Epoch [876], train_loss: 0.0000, val_loss: 3.7082, val_acc: 0.61702126\n",
      "Epoch [877], train_loss: 0.0000, val_loss: 3.7027, val_acc: 0.61702126\n",
      "Epoch [878], train_loss: 0.0000, val_loss: 3.7009, val_acc: 0.61702126\n",
      "Epoch [879], train_loss: 0.0000, val_loss: 3.6886, val_acc: 0.61702126\n",
      "Epoch [880], train_loss: 0.0000, val_loss: 3.6861, val_acc: 0.61702126\n",
      "Epoch [881], train_loss: 0.0000, val_loss: 3.6869, val_acc: 0.59574467\n",
      "Epoch [882], train_loss: 0.0000, val_loss: 3.6638, val_acc: 0.61702126\n",
      "Epoch [883], train_loss: 0.0000, val_loss: 3.6600, val_acc: 0.61702126\n",
      "Epoch [884], train_loss: 0.0000, val_loss: 3.6593, val_acc: 0.61702126\n",
      "Epoch [885], train_loss: 0.0000, val_loss: 3.6679, val_acc: 0.61702126\n",
      "Epoch [886], train_loss: 0.0000, val_loss: 3.6809, val_acc: 0.61702126\n",
      "Epoch [887], train_loss: 0.0000, val_loss: 3.6715, val_acc: 0.61702126\n",
      "Epoch [888], train_loss: 0.0000, val_loss: 3.6781, val_acc: 0.61702126\n",
      "Epoch [889], train_loss: 0.0000, val_loss: 3.6747, val_acc: 0.61702126\n",
      "Epoch [890], train_loss: 0.0000, val_loss: 3.6541, val_acc: 0.61702126\n",
      "Epoch [891], train_loss: 0.0000, val_loss: 3.6538, val_acc: 0.61702126\n",
      "Epoch [892], train_loss: 0.0000, val_loss: 3.6486, val_acc: 0.61702126\n",
      "Epoch [893], train_loss: 0.0000, val_loss: 3.6547, val_acc: 0.61702126\n",
      "Epoch [894], train_loss: 0.0000, val_loss: 3.6560, val_acc: 0.61702126\n",
      "Epoch [895], train_loss: 0.0000, val_loss: 3.6581, val_acc: 0.61702126\n",
      "Epoch [896], train_loss: 0.0000, val_loss: 3.6622, val_acc: 0.61702126\n",
      "Epoch [897], train_loss: 0.0000, val_loss: 3.6562, val_acc: 0.61702126\n",
      "Epoch [898], train_loss: 0.0000, val_loss: 3.6750, val_acc: 0.61702126\n",
      "Epoch [899], train_loss: 0.0000, val_loss: 3.6786, val_acc: 0.61702126\n",
      "Epoch [900], train_loss: 0.0000, val_loss: 3.6870, val_acc: 0.61702126\n",
      "Epoch [901], train_loss: 0.0000, val_loss: 3.6704, val_acc: 0.61702126\n",
      "Epoch [902], train_loss: 0.0000, val_loss: 3.6756, val_acc: 0.61702126\n",
      "Epoch [903], train_loss: 0.0000, val_loss: 3.6842, val_acc: 0.61702126\n",
      "Epoch [904], train_loss: 0.0000, val_loss: 3.6863, val_acc: 0.61702126\n",
      "Epoch [905], train_loss: 0.0000, val_loss: 3.6852, val_acc: 0.61702126\n",
      "Epoch [906], train_loss: 0.0000, val_loss: 3.6890, val_acc: 0.61702126\n",
      "Epoch [907], train_loss: 0.0000, val_loss: 3.6788, val_acc: 0.61702126\n",
      "Epoch [908], train_loss: 0.0000, val_loss: 3.6793, val_acc: 0.61702126\n",
      "Epoch [909], train_loss: 0.0000, val_loss: 3.6588, val_acc: 0.61702126\n",
      "Epoch [910], train_loss: 0.0000, val_loss: 3.6671, val_acc: 0.61702126\n",
      "Epoch [911], train_loss: 0.0000, val_loss: 3.6716, val_acc: 0.61702126\n",
      "Epoch [912], train_loss: 0.0000, val_loss: 3.6737, val_acc: 0.61702126\n",
      "Epoch [913], train_loss: 0.0000, val_loss: 3.6713, val_acc: 0.61702126\n",
      "Epoch [914], train_loss: 0.0000, val_loss: 3.6728, val_acc: 0.61702126\n",
      "Epoch [915], train_loss: 0.0000, val_loss: 3.6776, val_acc: 0.61702126\n",
      "Epoch [916], train_loss: 0.0000, val_loss: 3.6908, val_acc: 0.61702126\n",
      "Epoch [917], train_loss: 0.0000, val_loss: 3.6835, val_acc: 0.61702126\n",
      "Epoch [918], train_loss: 0.0000, val_loss: 3.6776, val_acc: 0.61702126\n",
      "Epoch [919], train_loss: 0.0000, val_loss: 3.6809, val_acc: 0.61702126\n",
      "Epoch [920], train_loss: 0.0000, val_loss: 3.6841, val_acc: 0.61702126\n",
      "Epoch [921], train_loss: 0.0000, val_loss: 3.6793, val_acc: 0.61702126\n",
      "Epoch [922], train_loss: 0.0000, val_loss: 3.6769, val_acc: 0.61702126\n",
      "Epoch [923], train_loss: 0.0000, val_loss: 3.6955, val_acc: 0.61702126\n",
      "Epoch [924], train_loss: 0.0000, val_loss: 3.6931, val_acc: 0.61702126\n",
      "Epoch [925], train_loss: 0.0000, val_loss: 3.6947, val_acc: 0.61702126\n",
      "Epoch [926], train_loss: 0.0000, val_loss: 3.6862, val_acc: 0.61702126\n",
      "Epoch [927], train_loss: 0.0000, val_loss: 3.6736, val_acc: 0.61702126\n",
      "Epoch [928], train_loss: 0.0000, val_loss: 3.6744, val_acc: 0.61702126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [929], train_loss: 0.0000, val_loss: 3.6565, val_acc: 0.61702126\n",
      "Epoch [930], train_loss: 0.0000, val_loss: 3.6385, val_acc: 0.61702126\n",
      "Epoch [931], train_loss: 0.0000, val_loss: 3.6364, val_acc: 0.61702126\n",
      "Epoch [932], train_loss: 0.0000, val_loss: 3.6591, val_acc: 0.61702126\n",
      "Epoch [933], train_loss: 0.0000, val_loss: 3.6581, val_acc: 0.61702126\n",
      "Epoch [934], train_loss: 0.0000, val_loss: 3.6503, val_acc: 0.61702126\n",
      "Epoch [935], train_loss: 0.0000, val_loss: 3.6522, val_acc: 0.61702126\n",
      "Epoch [936], train_loss: 0.0000, val_loss: 3.6526, val_acc: 0.61702126\n",
      "Epoch [937], train_loss: 0.0000, val_loss: 3.6544, val_acc: 0.61702126\n",
      "Epoch [938], train_loss: 0.0000, val_loss: 3.6554, val_acc: 0.61702126\n",
      "Epoch [939], train_loss: 0.0000, val_loss: 3.6558, val_acc: 0.61702126\n",
      "Epoch [940], train_loss: 0.0000, val_loss: 3.6564, val_acc: 0.61702126\n",
      "Epoch [941], train_loss: 0.0000, val_loss: 3.6478, val_acc: 0.61702126\n",
      "Epoch [942], train_loss: 0.0001, val_loss: 3.8613, val_acc: 0.59574467\n",
      "Epoch [943], train_loss: 0.0000, val_loss: 4.1644, val_acc: 0.61702126\n",
      "Epoch [944], train_loss: 0.0000, val_loss: 4.3641, val_acc: 0.61702126\n",
      "Epoch [945], train_loss: 0.0000, val_loss: 4.5109, val_acc: 0.61702126\n",
      "Epoch [946], train_loss: 0.0000, val_loss: 4.5997, val_acc: 0.57446808\n",
      "Epoch [947], train_loss: 0.0000, val_loss: 4.5749, val_acc: 0.59574467\n",
      "Epoch [948], train_loss: 0.0000, val_loss: 4.5158, val_acc: 0.57446808\n",
      "Epoch [949], train_loss: 0.0000, val_loss: 4.4428, val_acc: 0.57446808\n",
      "Epoch [950], train_loss: 0.0000, val_loss: 4.2489, val_acc: 0.57446808\n",
      "Epoch [951], train_loss: 0.0000, val_loss: 4.1170, val_acc: 0.51063830\n",
      "Epoch [952], train_loss: 0.0000, val_loss: 4.0253, val_acc: 0.51063830\n",
      "Epoch [953], train_loss: 0.0000, val_loss: 3.9309, val_acc: 0.55319148\n",
      "Epoch [954], train_loss: 0.0000, val_loss: 3.8779, val_acc: 0.55319148\n",
      "Epoch [955], train_loss: 0.0000, val_loss: 3.8434, val_acc: 0.55319148\n",
      "Epoch [956], train_loss: 0.0000, val_loss: 3.8169, val_acc: 0.55319148\n",
      "Epoch [957], train_loss: 0.0000, val_loss: 3.7904, val_acc: 0.55319148\n",
      "Epoch [958], train_loss: 0.0000, val_loss: 3.7656, val_acc: 0.55319148\n",
      "Epoch [959], train_loss: 0.0000, val_loss: 3.7576, val_acc: 0.55319148\n",
      "Epoch [960], train_loss: 0.0000, val_loss: 3.7387, val_acc: 0.55319148\n",
      "Epoch [961], train_loss: 0.0000, val_loss: 3.7249, val_acc: 0.55319148\n",
      "Epoch [962], train_loss: 0.0000, val_loss: 3.7099, val_acc: 0.55319148\n",
      "Epoch [963], train_loss: 0.0000, val_loss: 3.7010, val_acc: 0.55319148\n",
      "Epoch [964], train_loss: 0.0000, val_loss: 3.6983, val_acc: 0.57446808\n",
      "Epoch [965], train_loss: 0.0000, val_loss: 3.7062, val_acc: 0.57446808\n",
      "Epoch [966], train_loss: 0.0000, val_loss: 3.6979, val_acc: 0.57446808\n",
      "Epoch [967], train_loss: 0.0000, val_loss: 3.7060, val_acc: 0.57446808\n",
      "Epoch [968], train_loss: 0.0000, val_loss: 3.7107, val_acc: 0.57446808\n",
      "Epoch [969], train_loss: 0.0000, val_loss: 3.7181, val_acc: 0.57446808\n",
      "Epoch [970], train_loss: 0.0000, val_loss: 3.7029, val_acc: 0.57446808\n",
      "Epoch [971], train_loss: 0.0000, val_loss: 3.7221, val_acc: 0.57446808\n",
      "Epoch [972], train_loss: 0.0000, val_loss: 3.7271, val_acc: 0.57446808\n",
      "Epoch [973], train_loss: 0.0000, val_loss: 3.7430, val_acc: 0.55319148\n",
      "Epoch [974], train_loss: 0.0000, val_loss: 3.7539, val_acc: 0.55319148\n",
      "Epoch [975], train_loss: 0.0000, val_loss: 3.7610, val_acc: 0.55319148\n",
      "Epoch [976], train_loss: 0.0000, val_loss: 3.7685, val_acc: 0.55319148\n",
      "Epoch [977], train_loss: 0.0000, val_loss: 3.7725, val_acc: 0.55319148\n",
      "Epoch [978], train_loss: 0.0000, val_loss: 3.7648, val_acc: 0.55319148\n",
      "Epoch [979], train_loss: 0.0000, val_loss: 3.7567, val_acc: 0.57446808\n",
      "Epoch [980], train_loss: 0.0000, val_loss: 3.7601, val_acc: 0.57446808\n",
      "Epoch [981], train_loss: 0.0000, val_loss: 3.7616, val_acc: 0.57446808\n",
      "Epoch [982], train_loss: 0.0000, val_loss: 3.7736, val_acc: 0.55319148\n",
      "Epoch [983], train_loss: 0.0000, val_loss: 3.7813, val_acc: 0.55319148\n",
      "Epoch [984], train_loss: 0.0000, val_loss: 3.7947, val_acc: 0.55319148\n",
      "Epoch [985], train_loss: 0.0000, val_loss: 3.8043, val_acc: 0.53191489\n",
      "Epoch [986], train_loss: 0.0000, val_loss: 3.8126, val_acc: 0.53191489\n",
      "Epoch [987], train_loss: 0.0000, val_loss: 3.8080, val_acc: 0.53191489\n",
      "Epoch [988], train_loss: 0.0000, val_loss: 3.8130, val_acc: 0.57446808\n",
      "Epoch [989], train_loss: 0.0000, val_loss: 3.8241, val_acc: 0.55319148\n",
      "Epoch [990], train_loss: 0.0000, val_loss: 3.8245, val_acc: 0.53191489\n",
      "Epoch [991], train_loss: 0.0000, val_loss: 3.8346, val_acc: 0.53191489\n",
      "Epoch [992], train_loss: 0.0000, val_loss: 3.8392, val_acc: 0.55319148\n",
      "Epoch [993], train_loss: 0.0000, val_loss: 3.8242, val_acc: 0.55319148\n",
      "Epoch [994], train_loss: 0.0000, val_loss: 3.8251, val_acc: 0.55319148\n",
      "Epoch [995], train_loss: 0.0000, val_loss: 3.8259, val_acc: 0.57446808\n",
      "Epoch [996], train_loss: 0.0000, val_loss: 3.8264, val_acc: 0.55319148\n",
      "Epoch [997], train_loss: 0.0000, val_loss: 3.8392, val_acc: 0.55319148\n",
      "Epoch [998], train_loss: 0.0000, val_loss: 3.8247, val_acc: 0.57446808\n",
      "Epoch [999], train_loss: 0.0000, val_loss: 3.8357, val_acc: 0.57446808\n"
     ]
    }
   ],
   "source": [
    "# CTX = torch.device('cuda')\n",
    "# train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # train_dl.train_labels.to(CTX)\n",
    "# # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # val_dl.train_labels.to(CTX)\n",
    "num_epochs = 1000\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26874318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "    \n",
    "\n",
    "plot_accuracies(history)\n",
    "\n",
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60afc6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7021276354789734"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([x['val_acc'] for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3164b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
