{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50965d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, mode='NORM', k=1, dilation=1):\n",
    "        \"\"\"\n",
    "        Pre-act residual block, the middle transformations are bottle-necked\n",
    "        :param inplanes:\n",
    "        :param planes:\n",
    "        :param stride:\n",
    "        :param downsample:\n",
    "        :param mode: NORM | UP\n",
    "        :param k: times of additive\n",
    "        \"\"\"\n",
    "\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.k = k\n",
    "\n",
    "        btnk_ch = planes // 4\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, btnk_ch, kernel_size=1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(btnk_ch)\n",
    "        self.conv2 = nn.Conv2d(btnk_ch, btnk_ch, kernel_size=3, stride=stride, padding=dilation,\n",
    "                               dilation=dilation, bias=False)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(btnk_ch)\n",
    "        self.conv3 = nn.Conv2d(btnk_ch, planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if mode == 'UP':\n",
    "            self.shortcut = None\n",
    "        elif inplanes != planes or stride > 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.BatchNorm2d(inplanes),\n",
    "                self.relu,\n",
    "                nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = None\n",
    "\n",
    "    def _pre_act_forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.mode == 'UP':\n",
    "            residual = self.squeeze_idt(x)\n",
    "        elif self.shortcut is not None:\n",
    "            residual = self.shortcut(residual)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        return out\n",
    "\n",
    "    def squeeze_idt(self, idt):\n",
    "        n, c, h, w = idt.size()\n",
    "        return idt.view(n, c // self.k, self.k, h, w).sum(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self._pre_act_forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac708861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['fish']\n",
    "\n",
    "\n",
    "class Fish(nn.Module):\n",
    "    def __init__(self, block, num_cls=1000, num_down_sample=5, num_up_sample=3, trans_map=(2, 1, 0, 6, 5, 4),\n",
    "                 network_planes=None, num_res_blks=None, num_trans_blks=None):\n",
    "        super(Fish, self).__init__()\n",
    "        self.block = block\n",
    "        self.trans_map = trans_map\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.down_sample = nn.MaxPool2d(2, stride=2)\n",
    "        self.num_cls = num_cls\n",
    "        self.num_down = num_down_sample\n",
    "        self.num_up = num_up_sample\n",
    "        self.network_planes = network_planes[1:]\n",
    "        self.depth = len(self.network_planes)\n",
    "        self.num_trans_blks = num_trans_blks\n",
    "        self.num_res_blks = num_res_blks\n",
    "        self.fish = self._make_fish(network_planes[0])\n",
    "\n",
    "    def _make_score(self, in_ch, out_ch=1000, has_pool=False):\n",
    "        bn = nn.BatchNorm2d(in_ch)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        conv_trans = nn.Conv2d(in_ch, in_ch // 2, kernel_size=1, bias=False)\n",
    "        bn_out = nn.BatchNorm2d(in_ch // 2)\n",
    "        conv = nn.Sequential(bn, relu, conv_trans, bn_out, relu)\n",
    "        if has_pool:\n",
    "            fc = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(in_ch // 2, out_ch, kernel_size=1, bias=True))\n",
    "        else:\n",
    "            fc = nn.Conv2d(in_ch // 2, out_ch, kernel_size=1, bias=True)\n",
    "        return [conv, fc]\n",
    "\n",
    "    def _make_se_block(self, in_ch, out_ch):\n",
    "        bn = nn.BatchNorm2d(in_ch)\n",
    "        sq_conv = nn.Conv2d(in_ch, out_ch // 16, kernel_size=1)\n",
    "        ex_conv = nn.Conv2d(out_ch // 16, out_ch, kernel_size=1)\n",
    "        return nn.Sequential(bn,\n",
    "                             nn.ReLU(inplace=True),\n",
    "                             nn.AdaptiveAvgPool2d(1),\n",
    "                             sq_conv,\n",
    "                             nn.ReLU(inplace=True),\n",
    "                             ex_conv,\n",
    "                             nn.Sigmoid())\n",
    "\n",
    "    def _make_residual_block(self, inplanes, outplanes, nstage, is_up=False, k=1, dilation=1):\n",
    "        layers = []\n",
    "\n",
    "        if is_up:\n",
    "            layers.append(self.block(inplanes, outplanes, mode='UP', dilation=dilation, k=k))\n",
    "        else:\n",
    "            layers.append(self.block(inplanes, outplanes, stride=1))\n",
    "        for i in range(1, nstage):\n",
    "            layers.append(self.block(outplanes, outplanes, stride=1, dilation=dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, is_down_sample, inplanes, outplanes, n_blk, has_trans=True,\n",
    "                    has_score=False, trans_planes=0, no_sampling=False, num_trans=2, **kwargs):\n",
    "        sample_block = []\n",
    "        if has_score:\n",
    "            sample_block.extend(self._make_score(outplanes, outplanes * 2, has_pool=False))\n",
    "\n",
    "        if no_sampling or is_down_sample:\n",
    "            res_block = self._make_residual_block(inplanes, outplanes, n_blk, **kwargs)\n",
    "        else:\n",
    "            res_block = self._make_residual_block(inplanes, outplanes, n_blk, is_up=True, **kwargs)\n",
    "\n",
    "        sample_block.append(res_block)\n",
    "\n",
    "        if has_trans:\n",
    "            trans_in_planes = self.in_planes if trans_planes == 0 else trans_planes\n",
    "            sample_block.append(self._make_residual_block(trans_in_planes, trans_in_planes, num_trans))\n",
    "\n",
    "        if not no_sampling and is_down_sample:\n",
    "            sample_block.append(self.down_sample)\n",
    "        elif not no_sampling:  # Up-Sample\n",
    "            sample_block.append(self.upsample)\n",
    "\n",
    "        return nn.ModuleList(sample_block)\n",
    "\n",
    "    def _make_fish(self, in_planes):\n",
    "        def get_trans_planes(index):\n",
    "            map_id = self.trans_map[index-self.num_down-1] - 1\n",
    "            p = in_planes if map_id == -1 else cated_planes[map_id]\n",
    "            return p\n",
    "\n",
    "        def get_trans_blk(index):\n",
    "            return self.num_trans_blks[index-self.num_down-1]\n",
    "\n",
    "        def get_cur_planes(index):\n",
    "            return self.network_planes[index]\n",
    "\n",
    "        def get_blk_num(index):\n",
    "            return self.num_res_blks[index]\n",
    "\n",
    "        cated_planes, fish = [in_planes] * self.depth, []\n",
    "        for i in range(self.depth):\n",
    "            # even num for down-sample, odd for up-sample\n",
    "            is_down, has_trans, no_sampling = i not in range(self.num_down, self.num_down+self.num_up+1),\\\n",
    "                                              i > self.num_down, i == self.num_down\n",
    "            cur_planes, trans_planes, cur_blocks, num_trans =\\\n",
    "                get_cur_planes(i), get_trans_planes(i), get_blk_num(i), get_trans_blk(i)\n",
    "\n",
    "            stg_args = [is_down, cated_planes[i - 1], cur_planes, cur_blocks]\n",
    "\n",
    "            if is_down or no_sampling:\n",
    "                k, dilation = 1, 1\n",
    "            else:\n",
    "                k, dilation = cated_planes[i - 1] // cur_planes, 2 ** (i-self.num_down-1)\n",
    "\n",
    "            sample_block = self._make_stage(*stg_args, has_trans=has_trans, trans_planes=trans_planes,\n",
    "                                            has_score=(i==self.num_down), num_trans=num_trans, k=k, dilation=dilation,\n",
    "                                            no_sampling=no_sampling)\n",
    "            if i == self.depth - 1:\n",
    "                sample_block.extend(self._make_score(cur_planes + trans_planes, out_ch=self.num_cls, has_pool=True))\n",
    "            elif i == self.num_down:\n",
    "                sample_block.append(nn.Sequential(self._make_se_block(cur_planes*2, cur_planes)))\n",
    "\n",
    "            if i == self.num_down-1:\n",
    "                cated_planes[i] = cur_planes * 2\n",
    "            elif has_trans:\n",
    "                cated_planes[i] = cur_planes + trans_planes\n",
    "            else:\n",
    "                cated_planes[i] = cur_planes\n",
    "            fish.append(sample_block)\n",
    "        return nn.ModuleList(fish)\n",
    "\n",
    "    def _fish_forward(self, all_feat):\n",
    "        def _concat(a, b):\n",
    "            return torch.cat([a, b], dim=1)\n",
    "\n",
    "        def stage_factory(*blks):\n",
    "            def stage_forward(*inputs):\n",
    "                if stg_id < self.num_down:  # tail\n",
    "                    tail_blk = nn.Sequential(*blks[:2])\n",
    "                    return tail_blk(*inputs)\n",
    "                elif stg_id == self.num_down:\n",
    "                    score_blks = nn.Sequential(*blks[:2])\n",
    "                    score_feat = score_blks(inputs[0])\n",
    "                    att_feat = blks[3](score_feat)\n",
    "                    return blks[2](score_feat) * att_feat + att_feat\n",
    "                else:  # refine\n",
    "                    feat_trunk = blks[2](blks[0](inputs[0]))\n",
    "                    feat_branch = blks[1](inputs[1])\n",
    "                return _concat(feat_trunk, feat_branch)\n",
    "            return stage_forward\n",
    "\n",
    "        stg_id = 0\n",
    "        # tail:\n",
    "        while stg_id < self.depth:\n",
    "            stg_blk = stage_factory(*self.fish[stg_id])\n",
    "            if stg_id <= self.num_down:\n",
    "                in_feat = [all_feat[stg_id]]\n",
    "            else:\n",
    "                trans_id = self.trans_map[stg_id-self.num_down-1]\n",
    "                in_feat = [all_feat[stg_id], all_feat[trans_id]]\n",
    "\n",
    "            all_feat[stg_id + 1] = stg_blk(*in_feat)\n",
    "            stg_id += 1\n",
    "            # loop exit\n",
    "            if stg_id == self.depth:\n",
    "                score_feat = self.fish[self.depth-1][-2](all_feat[-1])\n",
    "                score = self.fish[self.depth-1][-1](score_feat)\n",
    "                return score\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_feat = [None] * (self.depth + 1)\n",
    "        all_feat[0] = x\n",
    "        return self._fish_forward(all_feat)\n",
    "\n",
    "\n",
    "class FishNet(nn.Module):\n",
    "    def __init__(self, block, **kwargs):\n",
    "        super(FishNet, self).__init__()\n",
    "\n",
    "        inplanes = kwargs['network_planes'][0]\n",
    "        # resolution: 224x224\n",
    "        self.conv1 = self._conv_bn_relu(3, inplanes // 2, stride=2)\n",
    "        self.conv2 = self._conv_bn_relu(inplanes // 2, inplanes // 2)\n",
    "        self.conv3 = self._conv_bn_relu(inplanes // 2, inplanes)\n",
    "        self.pool1 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "        # construct fish, resolution 56x56\n",
    "        self.fish = Fish(block, **kwargs)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _conv_bn_relu(self, in_ch, out_ch, stride=1):\n",
    "        return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "                             nn.BatchNorm2d(out_ch),\n",
    "                             nn.ReLU(inplace=True))\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool1(x)\n",
    "        score = self.fish(x)\n",
    "        # 1*1 output\n",
    "        out = score.view(x.size(0), -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def fish(**kwargs):\n",
    "    return FishNet(Bottleneck, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8112ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fishnet99(**kwargs):\n",
    "#     \"\"\"\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     net_cfg = {\n",
    "#         #  input size:   [224, 56, 28,  14  |  7,   7,  14,  28 | 56,   28,  14]\n",
    "#         # output size:   [56,  28, 14,   7  |  7,  14,  28,  56 | 28,   14,   7]\n",
    "#         #                  |    |    |   |     |    |    |    |    |     |    |\n",
    "#         'network_planes': [64, 128, 256, 512, 512, 512, 384, 256, 320, 832, 1600],\n",
    "#         'num_res_blks': [2, 2, 6, 2, 1, 1, 1, 1, 2, 2],\n",
    "#         'num_trans_blks': [1, 1, 1, 1, 1, 4],\n",
    "#         'num_cls': 1000,\n",
    "#         'num_down_sample': 3,\n",
    "#         'num_up_sample': 3,\n",
    "#     }\n",
    "#     cfg = {**net_cfg, **kwargs}\n",
    "#     return fish(**cfg)\n",
    "\n",
    "\n",
    "# def fishnet150(**kwargs):\n",
    "#     \"\"\"\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     net_cfg = {\n",
    "#         #  input size:   [224, 56, 28,  14  |  7,   7,  14,  28 | 56,   28,  14]\n",
    "#         # output size:   [56,  28, 14,   7  |  7,  14,  28,  56 | 28,   14,   7]\n",
    "#         #                  |    |    |   |     |    |    |    |    |     |    |\n",
    "#         'network_planes': [64, 128, 256, 512, 512, 512, 384, 256, 320, 832, 1600],\n",
    "#         'num_res_blks': [2, 4, 8, 4, 2, 2, 2, 2, 2, 4],\n",
    "#         'num_trans_blks': [2, 2, 2, 2, 2, 4],\n",
    "#         'num_cls': 1000,\n",
    "#         'num_down_sample': 3,\n",
    "#         'num_up_sample': 3,\n",
    "#     }\n",
    "#     cfg = {**net_cfg, **kwargs}\n",
    "#     return fish(**cfg)\n",
    "\n",
    "\n",
    "# def fishnet201(**kwargs):\n",
    "#     \"\"\"\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     net_cfg = {\n",
    "#         #  input size:   [224, 56, 28,  14  |  7,   7,  14,  28 | 56,   28,  14]\n",
    "#         # output size:   [56,  28, 14,   7  |  7,  14,  28,  56 | 28,   14,   7]\n",
    "#         #                  |    |    |   |     |    |    |    |    |     |    |\n",
    "#         'network_planes': [64, 128, 256, 512, 512, 512, 384, 256, 320, 832, 1600],\n",
    "#         'num_res_blks': [3, 4, 12, 4, 2, 2, 2, 2, 3, 10],\n",
    "#         'num_trans_blks': [2, 2, 2, 2, 2, 9],\n",
    "#         'num_cls': 1000,\n",
    "#         'num_down_sample': 3,\n",
    "#         'num_up_sample': 3,\n",
    "#     }\n",
    "#     cfg = {**net_cfg, **kwargs}\n",
    "#     return fish(**cfg)\n",
    "\n",
    "def fishnet_AD( **kwargs):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    net_cfg = {\n",
    "        #  input size:   [224, 56, 28,  14  |  7,   7,  14,  28 | 56,   28,  14]\n",
    "        # output size:   [56,  28, 14,   7  |  7,  14,  28,  56 | 28,   14,   7]\n",
    "        #                  |    |    |   |     |    |    |    |    |     |    |\n",
    "        'network_planes': [64, 128, 256, 512, 512, 512, 384, 256, 320, 832, 1600],\n",
    "        'num_res_blks': [2, 2, 6, 2, 1, 1, 1, 1, 2, 2],\n",
    "        'num_trans_blks': [1, 1, 1, 1, 1, 4],\n",
    "        'num_cls': 2,\n",
    "        'num_down_sample': 3,\n",
    "        'num_up_sample': 3,\n",
    "    }\n",
    "    cfg = {**net_cfg, **kwargs}\n",
    "    return fish(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79faa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,test_data,test_label = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label\n",
    "\n",
    "data_path = 'adress_3_256_64.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be840ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2841, 3, 256, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e14b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(2841).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(device = CTX, dtype=torch.float),train_y.to(device = CTX, dtype=torch.float)) # create your datset\n",
    "\n",
    " # create your dataloader\n",
    "    \n",
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(150).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "# val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1738b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(device = CTX, dtype=torch.float))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.6f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fdb0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FishNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (fish): Fish(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (down_sample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (fish): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (shortcut): Sequential(\n",
      "              (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (shortcut): Sequential(\n",
      "              (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (shortcut): Sequential(\n",
      "              (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (shortcut): Sequential(\n",
      "              (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): AdaptiveAvgPool2d(output_size=1)\n",
      "            (3): Conv2d(1024, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (4): ReLU(inplace=True)\n",
      "            (5): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (6): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(768, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "            (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      )\n",
      "      (6): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "            (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      )\n",
      "      (7): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (8): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(832, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(208, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(208, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(832, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(208, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(208, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(1056, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = fishnet_AD()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aec850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 128, 32]             864\n",
      "       BatchNorm2d-2          [-1, 32, 128, 32]              64\n",
      "              ReLU-3          [-1, 32, 128, 32]               0\n",
      "            Conv2d-4          [-1, 32, 128, 32]           9,216\n",
      "       BatchNorm2d-5          [-1, 32, 128, 32]              64\n",
      "              ReLU-6          [-1, 32, 128, 32]               0\n",
      "            Conv2d-7          [-1, 64, 128, 32]          18,432\n",
      "       BatchNorm2d-8          [-1, 64, 128, 32]             128\n",
      "              ReLU-9          [-1, 64, 128, 32]               0\n",
      "        MaxPool2d-10           [-1, 64, 64, 16]               0\n",
      "      BatchNorm2d-11           [-1, 64, 64, 16]             128\n",
      "             ReLU-12           [-1, 64, 64, 16]               0\n",
      "             ReLU-13           [-1, 64, 64, 16]               0\n",
      "           Conv2d-14           [-1, 32, 64, 16]           2,048\n",
      "      BatchNorm2d-15           [-1, 32, 64, 16]              64\n",
      "             ReLU-16           [-1, 32, 64, 16]               0\n",
      "             ReLU-17           [-1, 32, 64, 16]               0\n",
      "           Conv2d-18           [-1, 32, 64, 16]           9,216\n",
      "      BatchNorm2d-19           [-1, 32, 64, 16]              64\n",
      "             ReLU-20           [-1, 32, 64, 16]               0\n",
      "             ReLU-21           [-1, 32, 64, 16]               0\n",
      "           Conv2d-22          [-1, 128, 64, 16]           4,096\n",
      "      BatchNorm2d-23           [-1, 64, 64, 16]             128\n",
      "             ReLU-24           [-1, 64, 64, 16]               0\n",
      "             ReLU-25           [-1, 64, 64, 16]               0\n",
      "           Conv2d-26          [-1, 128, 64, 16]           8,192\n",
      "       Bottleneck-27          [-1, 128, 64, 16]               0\n",
      "      BatchNorm2d-28          [-1, 128, 64, 16]             256\n",
      "             ReLU-29          [-1, 128, 64, 16]               0\n",
      "           Conv2d-30           [-1, 32, 64, 16]           4,096\n",
      "      BatchNorm2d-31           [-1, 32, 64, 16]              64\n",
      "             ReLU-32           [-1, 32, 64, 16]               0\n",
      "           Conv2d-33           [-1, 32, 64, 16]           9,216\n",
      "      BatchNorm2d-34           [-1, 32, 64, 16]              64\n",
      "             ReLU-35           [-1, 32, 64, 16]               0\n",
      "           Conv2d-36          [-1, 128, 64, 16]           4,096\n",
      "       Bottleneck-37          [-1, 128, 64, 16]               0\n",
      "        MaxPool2d-38           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-39           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-40           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-41           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-42           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-43           [-1, 128, 32, 8]               0\n",
      "        MaxPool2d-44           [-1, 128, 32, 8]               0\n",
      "      BatchNorm2d-45           [-1, 128, 32, 8]             256\n",
      "             ReLU-46           [-1, 128, 32, 8]               0\n",
      "             ReLU-47           [-1, 128, 32, 8]               0\n",
      "           Conv2d-48            [-1, 64, 32, 8]           8,192\n",
      "      BatchNorm2d-49            [-1, 64, 32, 8]             128\n",
      "             ReLU-50            [-1, 64, 32, 8]               0\n",
      "             ReLU-51            [-1, 64, 32, 8]               0\n",
      "           Conv2d-52            [-1, 64, 32, 8]          36,864\n",
      "      BatchNorm2d-53            [-1, 64, 32, 8]             128\n",
      "             ReLU-54            [-1, 64, 32, 8]               0\n",
      "             ReLU-55            [-1, 64, 32, 8]               0\n",
      "           Conv2d-56           [-1, 256, 32, 8]          16,384\n",
      "      BatchNorm2d-57           [-1, 128, 32, 8]             256\n",
      "             ReLU-58           [-1, 128, 32, 8]               0\n",
      "             ReLU-59           [-1, 128, 32, 8]               0\n",
      "           Conv2d-60           [-1, 256, 32, 8]          32,768\n",
      "       Bottleneck-61           [-1, 256, 32, 8]               0\n",
      "      BatchNorm2d-62           [-1, 256, 32, 8]             512\n",
      "             ReLU-63           [-1, 256, 32, 8]               0\n",
      "           Conv2d-64            [-1, 64, 32, 8]          16,384\n",
      "      BatchNorm2d-65            [-1, 64, 32, 8]             128\n",
      "             ReLU-66            [-1, 64, 32, 8]               0\n",
      "           Conv2d-67            [-1, 64, 32, 8]          36,864\n",
      "      BatchNorm2d-68            [-1, 64, 32, 8]             128\n",
      "             ReLU-69            [-1, 64, 32, 8]               0\n",
      "           Conv2d-70           [-1, 256, 32, 8]          16,384\n",
      "       Bottleneck-71           [-1, 256, 32, 8]               0\n",
      "        MaxPool2d-72           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-73           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-74           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-75           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-76           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-77           [-1, 256, 16, 4]               0\n",
      "        MaxPool2d-78           [-1, 256, 16, 4]               0\n",
      "      BatchNorm2d-79           [-1, 256, 16, 4]             512\n",
      "             ReLU-80           [-1, 256, 16, 4]               0\n",
      "             ReLU-81           [-1, 256, 16, 4]               0\n",
      "           Conv2d-82           [-1, 128, 16, 4]          32,768\n",
      "      BatchNorm2d-83           [-1, 128, 16, 4]             256\n",
      "             ReLU-84           [-1, 128, 16, 4]               0\n",
      "             ReLU-85           [-1, 128, 16, 4]               0\n",
      "           Conv2d-86           [-1, 128, 16, 4]         147,456\n",
      "      BatchNorm2d-87           [-1, 128, 16, 4]             256\n",
      "             ReLU-88           [-1, 128, 16, 4]               0\n",
      "             ReLU-89           [-1, 128, 16, 4]               0\n",
      "           Conv2d-90           [-1, 512, 16, 4]          65,536\n",
      "      BatchNorm2d-91           [-1, 256, 16, 4]             512\n",
      "             ReLU-92           [-1, 256, 16, 4]               0\n",
      "             ReLU-93           [-1, 256, 16, 4]               0\n",
      "           Conv2d-94           [-1, 512, 16, 4]         131,072\n",
      "       Bottleneck-95           [-1, 512, 16, 4]               0\n",
      "      BatchNorm2d-96           [-1, 512, 16, 4]           1,024\n",
      "             ReLU-97           [-1, 512, 16, 4]               0\n",
      "           Conv2d-98           [-1, 128, 16, 4]          65,536\n",
      "      BatchNorm2d-99           [-1, 128, 16, 4]             256\n",
      "            ReLU-100           [-1, 128, 16, 4]               0\n",
      "          Conv2d-101           [-1, 128, 16, 4]         147,456\n",
      "     BatchNorm2d-102           [-1, 128, 16, 4]             256\n",
      "            ReLU-103           [-1, 128, 16, 4]               0\n",
      "          Conv2d-104           [-1, 512, 16, 4]          65,536\n",
      "      Bottleneck-105           [-1, 512, 16, 4]               0\n",
      "     BatchNorm2d-106           [-1, 512, 16, 4]           1,024\n",
      "            ReLU-107           [-1, 512, 16, 4]               0\n",
      "          Conv2d-108           [-1, 128, 16, 4]          65,536\n",
      "     BatchNorm2d-109           [-1, 128, 16, 4]             256\n",
      "            ReLU-110           [-1, 128, 16, 4]               0\n",
      "          Conv2d-111           [-1, 128, 16, 4]         147,456\n",
      "     BatchNorm2d-112           [-1, 128, 16, 4]             256\n",
      "            ReLU-113           [-1, 128, 16, 4]               0\n",
      "          Conv2d-114           [-1, 512, 16, 4]          65,536\n",
      "      Bottleneck-115           [-1, 512, 16, 4]               0\n",
      "     BatchNorm2d-116           [-1, 512, 16, 4]           1,024\n",
      "            ReLU-117           [-1, 512, 16, 4]               0\n",
      "          Conv2d-118           [-1, 128, 16, 4]          65,536\n",
      "     BatchNorm2d-119           [-1, 128, 16, 4]             256\n",
      "            ReLU-120           [-1, 128, 16, 4]               0\n",
      "          Conv2d-121           [-1, 128, 16, 4]         147,456\n",
      "     BatchNorm2d-122           [-1, 128, 16, 4]             256\n",
      "            ReLU-123           [-1, 128, 16, 4]               0\n",
      "          Conv2d-124           [-1, 512, 16, 4]          65,536\n",
      "      Bottleneck-125           [-1, 512, 16, 4]               0\n",
      "     BatchNorm2d-126           [-1, 512, 16, 4]           1,024\n",
      "            ReLU-127           [-1, 512, 16, 4]               0\n",
      "          Conv2d-128           [-1, 128, 16, 4]          65,536\n",
      "     BatchNorm2d-129           [-1, 128, 16, 4]             256\n",
      "            ReLU-130           [-1, 128, 16, 4]               0\n",
      "          Conv2d-131           [-1, 128, 16, 4]         147,456\n",
      "     BatchNorm2d-132           [-1, 128, 16, 4]             256\n",
      "            ReLU-133           [-1, 128, 16, 4]               0\n",
      "          Conv2d-134           [-1, 512, 16, 4]          65,536\n",
      "      Bottleneck-135           [-1, 512, 16, 4]               0\n",
      "     BatchNorm2d-136           [-1, 512, 16, 4]           1,024\n",
      "            ReLU-137           [-1, 512, 16, 4]               0\n",
      "          Conv2d-138           [-1, 128, 16, 4]          65,536\n",
      "     BatchNorm2d-139           [-1, 128, 16, 4]             256\n",
      "            ReLU-140           [-1, 128, 16, 4]               0\n",
      "          Conv2d-141           [-1, 128, 16, 4]         147,456\n",
      "     BatchNorm2d-142           [-1, 128, 16, 4]             256\n",
      "            ReLU-143           [-1, 128, 16, 4]               0\n",
      "          Conv2d-144           [-1, 512, 16, 4]          65,536\n",
      "      Bottleneck-145           [-1, 512, 16, 4]               0\n",
      "       MaxPool2d-146            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-147            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-148            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-149            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-150            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-151            [-1, 512, 8, 2]               0\n",
      "       MaxPool2d-152            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-153            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-154            [-1, 512, 8, 2]               0\n",
      "          Conv2d-155            [-1, 256, 8, 2]         131,072\n",
      "     BatchNorm2d-156            [-1, 256, 8, 2]             512\n",
      "            ReLU-157            [-1, 256, 8, 2]               0\n",
      "          Conv2d-158           [-1, 1024, 8, 2]         263,168\n",
      "     BatchNorm2d-159           [-1, 1024, 8, 2]           2,048\n",
      "            ReLU-160           [-1, 1024, 8, 2]               0\n",
      "AdaptiveAvgPool2d-161           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-162             [-1, 32, 1, 1]          32,800\n",
      "            ReLU-163             [-1, 32, 1, 1]               0\n",
      "          Conv2d-164            [-1, 512, 1, 1]          16,896\n",
      "         Sigmoid-165            [-1, 512, 1, 1]               0\n",
      "     BatchNorm2d-166           [-1, 1024, 8, 2]           2,048\n",
      "            ReLU-167           [-1, 1024, 8, 2]               0\n",
      "            ReLU-168           [-1, 1024, 8, 2]               0\n",
      "          Conv2d-169            [-1, 128, 8, 2]         131,072\n",
      "     BatchNorm2d-170            [-1, 128, 8, 2]             256\n",
      "            ReLU-171            [-1, 128, 8, 2]               0\n",
      "            ReLU-172            [-1, 128, 8, 2]               0\n",
      "          Conv2d-173            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-174            [-1, 128, 8, 2]             256\n",
      "            ReLU-175            [-1, 128, 8, 2]               0\n",
      "            ReLU-176            [-1, 128, 8, 2]               0\n",
      "          Conv2d-177            [-1, 512, 8, 2]          65,536\n",
      "     BatchNorm2d-178           [-1, 1024, 8, 2]           2,048\n",
      "            ReLU-179           [-1, 1024, 8, 2]               0\n",
      "            ReLU-180           [-1, 1024, 8, 2]               0\n",
      "          Conv2d-181            [-1, 512, 8, 2]         524,288\n",
      "      Bottleneck-182            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-183            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-184            [-1, 512, 8, 2]               0\n",
      "          Conv2d-185            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-186            [-1, 128, 8, 2]             256\n",
      "            ReLU-187            [-1, 128, 8, 2]               0\n",
      "          Conv2d-188            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-189            [-1, 128, 8, 2]             256\n",
      "            ReLU-190            [-1, 128, 8, 2]               0\n",
      "          Conv2d-191            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-192            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-193            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-194            [-1, 512, 8, 2]               0\n",
      "          Conv2d-195            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-196            [-1, 128, 8, 2]             256\n",
      "            ReLU-197            [-1, 128, 8, 2]               0\n",
      "          Conv2d-198            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-199            [-1, 128, 8, 2]             256\n",
      "            ReLU-200            [-1, 128, 8, 2]               0\n",
      "          Conv2d-201            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-202            [-1, 512, 8, 2]               0\n",
      "        Upsample-203           [-1, 512, 16, 4]               0\n",
      "        Upsample-204           [-1, 512, 16, 4]               0\n",
      "        Upsample-205           [-1, 512, 16, 4]               0\n",
      "        Upsample-206           [-1, 512, 16, 4]               0\n",
      "     BatchNorm2d-207           [-1, 256, 16, 4]             512\n",
      "            ReLU-208           [-1, 256, 16, 4]               0\n",
      "          Conv2d-209            [-1, 64, 16, 4]          16,384\n",
      "     BatchNorm2d-210            [-1, 64, 16, 4]             128\n",
      "            ReLU-211            [-1, 64, 16, 4]               0\n",
      "          Conv2d-212            [-1, 64, 16, 4]          36,864\n",
      "     BatchNorm2d-213            [-1, 64, 16, 4]             128\n",
      "            ReLU-214            [-1, 64, 16, 4]               0\n",
      "          Conv2d-215           [-1, 256, 16, 4]          16,384\n",
      "      Bottleneck-216           [-1, 256, 16, 4]               0\n",
      "     BatchNorm2d-217           [-1, 768, 16, 4]           1,536\n",
      "            ReLU-218           [-1, 768, 16, 4]               0\n",
      "          Conv2d-219            [-1, 96, 16, 4]          73,728\n",
      "     BatchNorm2d-220            [-1, 96, 16, 4]             192\n",
      "            ReLU-221            [-1, 96, 16, 4]               0\n",
      "          Conv2d-222            [-1, 96, 16, 4]          82,944\n",
      "     BatchNorm2d-223            [-1, 96, 16, 4]             192\n",
      "            ReLU-224            [-1, 96, 16, 4]               0\n",
      "          Conv2d-225           [-1, 384, 16, 4]          36,864\n",
      "      Bottleneck-226           [-1, 384, 16, 4]               0\n",
      "        Upsample-227           [-1, 384, 32, 8]               0\n",
      "        Upsample-228           [-1, 384, 32, 8]               0\n",
      "        Upsample-229           [-1, 384, 32, 8]               0\n",
      "        Upsample-230           [-1, 384, 32, 8]               0\n",
      "     BatchNorm2d-231           [-1, 128, 32, 8]             256\n",
      "            ReLU-232           [-1, 128, 32, 8]               0\n",
      "          Conv2d-233            [-1, 32, 32, 8]           4,096\n",
      "     BatchNorm2d-234            [-1, 32, 32, 8]              64\n",
      "            ReLU-235            [-1, 32, 32, 8]               0\n",
      "          Conv2d-236            [-1, 32, 32, 8]           9,216\n",
      "     BatchNorm2d-237            [-1, 32, 32, 8]              64\n",
      "            ReLU-238            [-1, 32, 32, 8]               0\n",
      "          Conv2d-239           [-1, 128, 32, 8]           4,096\n",
      "      Bottleneck-240           [-1, 128, 32, 8]               0\n",
      "     BatchNorm2d-241           [-1, 512, 32, 8]           1,024\n",
      "            ReLU-242           [-1, 512, 32, 8]               0\n",
      "          Conv2d-243            [-1, 64, 32, 8]          32,768\n",
      "     BatchNorm2d-244            [-1, 64, 32, 8]             128\n",
      "            ReLU-245            [-1, 64, 32, 8]               0\n",
      "          Conv2d-246            [-1, 64, 32, 8]          36,864\n",
      "     BatchNorm2d-247            [-1, 64, 32, 8]             128\n",
      "            ReLU-248            [-1, 64, 32, 8]               0\n",
      "          Conv2d-249           [-1, 256, 32, 8]          16,384\n",
      "      Bottleneck-250           [-1, 256, 32, 8]               0\n",
      "        Upsample-251          [-1, 256, 64, 16]               0\n",
      "        Upsample-252          [-1, 256, 64, 16]               0\n",
      "        Upsample-253          [-1, 256, 64, 16]               0\n",
      "        Upsample-254          [-1, 256, 64, 16]               0\n",
      "     BatchNorm2d-255           [-1, 64, 64, 16]             128\n",
      "            ReLU-256           [-1, 64, 64, 16]               0\n",
      "          Conv2d-257           [-1, 16, 64, 16]           1,024\n",
      "     BatchNorm2d-258           [-1, 16, 64, 16]              32\n",
      "            ReLU-259           [-1, 16, 64, 16]               0\n",
      "          Conv2d-260           [-1, 16, 64, 16]           2,304\n",
      "     BatchNorm2d-261           [-1, 16, 64, 16]              32\n",
      "            ReLU-262           [-1, 16, 64, 16]               0\n",
      "          Conv2d-263           [-1, 64, 64, 16]           1,024\n",
      "      Bottleneck-264           [-1, 64, 64, 16]               0\n",
      "     BatchNorm2d-265          [-1, 320, 64, 16]             640\n",
      "            ReLU-266          [-1, 320, 64, 16]               0\n",
      "          Conv2d-267           [-1, 80, 64, 16]          25,600\n",
      "     BatchNorm2d-268           [-1, 80, 64, 16]             160\n",
      "            ReLU-269           [-1, 80, 64, 16]               0\n",
      "          Conv2d-270           [-1, 80, 64, 16]          57,600\n",
      "     BatchNorm2d-271           [-1, 80, 64, 16]             160\n",
      "            ReLU-272           [-1, 80, 64, 16]               0\n",
      "          Conv2d-273          [-1, 320, 64, 16]          25,600\n",
      "      Bottleneck-274          [-1, 320, 64, 16]               0\n",
      "       MaxPool2d-275           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-276           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-277           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-278           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-279           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-280           [-1, 320, 32, 8]               0\n",
      "       MaxPool2d-281           [-1, 320, 32, 8]               0\n",
      "     BatchNorm2d-282           [-1, 512, 32, 8]           1,024\n",
      "            ReLU-283           [-1, 512, 32, 8]               0\n",
      "          Conv2d-284           [-1, 128, 32, 8]          65,536\n",
      "     BatchNorm2d-285           [-1, 128, 32, 8]             256\n",
      "            ReLU-286           [-1, 128, 32, 8]               0\n",
      "          Conv2d-287           [-1, 128, 32, 8]         147,456\n",
      "     BatchNorm2d-288           [-1, 128, 32, 8]             256\n",
      "            ReLU-289           [-1, 128, 32, 8]               0\n",
      "          Conv2d-290           [-1, 512, 32, 8]          65,536\n",
      "      Bottleneck-291           [-1, 512, 32, 8]               0\n",
      "     BatchNorm2d-292           [-1, 832, 32, 8]           1,664\n",
      "            ReLU-293           [-1, 832, 32, 8]               0\n",
      "          Conv2d-294           [-1, 208, 32, 8]         173,056\n",
      "     BatchNorm2d-295           [-1, 208, 32, 8]             416\n",
      "            ReLU-296           [-1, 208, 32, 8]               0\n",
      "          Conv2d-297           [-1, 208, 32, 8]         389,376\n",
      "     BatchNorm2d-298           [-1, 208, 32, 8]             416\n",
      "            ReLU-299           [-1, 208, 32, 8]               0\n",
      "          Conv2d-300           [-1, 832, 32, 8]         173,056\n",
      "      Bottleneck-301           [-1, 832, 32, 8]               0\n",
      "     BatchNorm2d-302           [-1, 832, 32, 8]           1,664\n",
      "            ReLU-303           [-1, 832, 32, 8]               0\n",
      "          Conv2d-304           [-1, 208, 32, 8]         173,056\n",
      "     BatchNorm2d-305           [-1, 208, 32, 8]             416\n",
      "            ReLU-306           [-1, 208, 32, 8]               0\n",
      "          Conv2d-307           [-1, 208, 32, 8]         389,376\n",
      "     BatchNorm2d-308           [-1, 208, 32, 8]             416\n",
      "            ReLU-309           [-1, 208, 32, 8]               0\n",
      "          Conv2d-310           [-1, 832, 32, 8]         173,056\n",
      "      Bottleneck-311           [-1, 832, 32, 8]               0\n",
      "       MaxPool2d-312           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-313           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-314           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-315           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-316           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-317           [-1, 832, 16, 4]               0\n",
      "       MaxPool2d-318           [-1, 832, 16, 4]               0\n",
      "     BatchNorm2d-319           [-1, 768, 16, 4]           1,536\n",
      "            ReLU-320           [-1, 768, 16, 4]               0\n",
      "          Conv2d-321           [-1, 192, 16, 4]         147,456\n",
      "     BatchNorm2d-322           [-1, 192, 16, 4]             384\n",
      "            ReLU-323           [-1, 192, 16, 4]               0\n",
      "          Conv2d-324           [-1, 192, 16, 4]         331,776\n",
      "     BatchNorm2d-325           [-1, 192, 16, 4]             384\n",
      "            ReLU-326           [-1, 192, 16, 4]               0\n",
      "          Conv2d-327           [-1, 768, 16, 4]         147,456\n",
      "      Bottleneck-328           [-1, 768, 16, 4]               0\n",
      "     BatchNorm2d-329          [-1, 1600, 16, 4]           3,200\n",
      "            ReLU-330          [-1, 1600, 16, 4]               0\n",
      "          Conv2d-331           [-1, 400, 16, 4]         640,000\n",
      "     BatchNorm2d-332           [-1, 400, 16, 4]             800\n",
      "            ReLU-333           [-1, 400, 16, 4]               0\n",
      "          Conv2d-334           [-1, 400, 16, 4]       1,440,000\n",
      "     BatchNorm2d-335           [-1, 400, 16, 4]             800\n",
      "            ReLU-336           [-1, 400, 16, 4]               0\n",
      "          Conv2d-337          [-1, 1600, 16, 4]         640,000\n",
      "      Bottleneck-338          [-1, 1600, 16, 4]               0\n",
      "     BatchNorm2d-339          [-1, 1600, 16, 4]           3,200\n",
      "            ReLU-340          [-1, 1600, 16, 4]               0\n",
      "          Conv2d-341           [-1, 400, 16, 4]         640,000\n",
      "     BatchNorm2d-342           [-1, 400, 16, 4]             800\n",
      "            ReLU-343           [-1, 400, 16, 4]               0\n",
      "          Conv2d-344           [-1, 400, 16, 4]       1,440,000\n",
      "     BatchNorm2d-345           [-1, 400, 16, 4]             800\n",
      "            ReLU-346           [-1, 400, 16, 4]               0\n",
      "          Conv2d-347          [-1, 1600, 16, 4]         640,000\n",
      "      Bottleneck-348          [-1, 1600, 16, 4]               0\n",
      "       MaxPool2d-349           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-350           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-351           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-352           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-353           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-354           [-1, 1600, 8, 2]               0\n",
      "       MaxPool2d-355           [-1, 1600, 8, 2]               0\n",
      "     BatchNorm2d-356            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-357            [-1, 512, 8, 2]               0\n",
      "          Conv2d-358            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-359            [-1, 128, 8, 2]             256\n",
      "            ReLU-360            [-1, 128, 8, 2]               0\n",
      "          Conv2d-361            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-362            [-1, 128, 8, 2]             256\n",
      "            ReLU-363            [-1, 128, 8, 2]               0\n",
      "          Conv2d-364            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-365            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-366            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-367            [-1, 512, 8, 2]               0\n",
      "          Conv2d-368            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-369            [-1, 128, 8, 2]             256\n",
      "            ReLU-370            [-1, 128, 8, 2]               0\n",
      "          Conv2d-371            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-372            [-1, 128, 8, 2]             256\n",
      "            ReLU-373            [-1, 128, 8, 2]               0\n",
      "          Conv2d-374            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-375            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-376            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-377            [-1, 512, 8, 2]               0\n",
      "          Conv2d-378            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-379            [-1, 128, 8, 2]             256\n",
      "            ReLU-380            [-1, 128, 8, 2]               0\n",
      "          Conv2d-381            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-382            [-1, 128, 8, 2]             256\n",
      "            ReLU-383            [-1, 128, 8, 2]               0\n",
      "          Conv2d-384            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-385            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-386            [-1, 512, 8, 2]           1,024\n",
      "            ReLU-387            [-1, 512, 8, 2]               0\n",
      "          Conv2d-388            [-1, 128, 8, 2]          65,536\n",
      "     BatchNorm2d-389            [-1, 128, 8, 2]             256\n",
      "            ReLU-390            [-1, 128, 8, 2]               0\n",
      "          Conv2d-391            [-1, 128, 8, 2]         147,456\n",
      "     BatchNorm2d-392            [-1, 128, 8, 2]             256\n",
      "            ReLU-393            [-1, 128, 8, 2]               0\n",
      "          Conv2d-394            [-1, 512, 8, 2]          65,536\n",
      "      Bottleneck-395            [-1, 512, 8, 2]               0\n",
      "     BatchNorm2d-396           [-1, 2112, 8, 2]           4,224\n",
      "            ReLU-397           [-1, 2112, 8, 2]               0\n",
      "          Conv2d-398           [-1, 1056, 8, 2]       2,230,272\n",
      "     BatchNorm2d-399           [-1, 1056, 8, 2]           2,112\n",
      "            ReLU-400           [-1, 1056, 8, 2]               0\n",
      "AdaptiveAvgPool2d-401           [-1, 1056, 1, 1]               0\n",
      "          Conv2d-402              [-1, 2, 1, 1]           2,114\n",
      "            Fish-403              [-1, 2, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 15,574,018\n",
      "Trainable params: 15,574,018\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 127.67\n",
      "Params size (MB): 59.41\n",
      "Estimated Total Size (MB): 187.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 256, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc83b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99886213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 400\n",
    "# opt_func = torch.optim.Adam\n",
    "# lr = 0.001\n",
    "# #fitting the model on training data and record the result after each epoch\n",
    "# history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2857d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e5b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affdbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 0.000\n",
      "[1,     2] Acc: 0.000\n",
      "[1,     4] loss: 0.001\n",
      "[1,     4] Acc: 0.000\n",
      "[1,     6] loss: 0.000\n",
      "[1,     6] Acc: 0.000\n",
      "[1,     8] loss: 0.001\n",
      "[1,     8] Acc: 0.000\n",
      "[1,    10] loss: 0.000\n",
      "[1,    10] Acc: 0.000\n",
      "[1,    12] loss: 0.000\n",
      "[1,    12] Acc: 0.000\n",
      "[1,    14] loss: 0.000\n",
      "[1,    14] Acc: 0.000\n",
      "[1,    16] loss: 0.001\n",
      "[1,    16] Acc: 0.000\n",
      "[1,    18] loss: 0.001\n",
      "[1,    18] Acc: 0.000\n",
      "[1,    20] loss: 0.000\n",
      "[1,    20] Acc: 0.000\n",
      "[1,    22] loss: 0.000\n",
      "[1,    22] Acc: 0.000\n",
      "[1,    24] loss: 0.000\n",
      "[1,    24] Acc: 0.000\n",
      "[1,    26] loss: 0.000\n",
      "[1,    26] Acc: 0.000\n",
      "[1,    28] loss: 0.000\n",
      "[1,    28] Acc: 0.000\n",
      "[1,    30] loss: 0.001\n",
      "[1,    30] Acc: 0.000\n",
      "[1,    32] loss: 0.000\n",
      "[1,    32] Acc: 0.000\n",
      "[1,    34] loss: 0.000\n",
      "[1,    34] Acc: 0.000\n",
      "[1,    36] loss: 0.000\n",
      "[1,    36] Acc: 0.000\n",
      "[1,    38] loss: 0.001\n",
      "[1,    38] Acc: 0.000\n",
      "[1,    40] loss: 0.000\n",
      "[1,    40] Acc: 0.000\n",
      "[1,    42] loss: 0.001\n",
      "[1,    42] Acc: 0.000\n",
      "[1,    44] loss: 0.000\n",
      "[1,    44] Acc: 0.000\n",
      "[2,     2] loss: 0.001\n",
      "[2,     2] Acc: 0.000\n",
      "[2,     4] loss: 0.000\n",
      "[2,     4] Acc: 0.000\n",
      "[2,     6] loss: 0.000\n",
      "[2,     6] Acc: 0.000\n",
      "[2,     8] loss: 0.001\n",
      "[2,     8] Acc: 0.000\n",
      "[2,    10] loss: 0.000\n",
      "[2,    10] Acc: 0.000\n",
      "[2,    12] loss: 0.000\n",
      "[2,    12] Acc: 0.000\n",
      "[2,    14] loss: 0.000\n",
      "[2,    14] Acc: 0.000\n",
      "[2,    16] loss: 0.001\n",
      "[2,    16] Acc: 0.000\n",
      "[2,    18] loss: 0.000\n",
      "[2,    18] Acc: 0.000\n",
      "[2,    20] loss: 0.000\n",
      "[2,    20] Acc: 0.000\n",
      "[2,    22] loss: 0.000\n",
      "[2,    22] Acc: 0.000\n",
      "[2,    24] loss: 0.000\n",
      "[2,    24] Acc: 0.000\n",
      "[2,    26] loss: 0.001\n",
      "[2,    26] Acc: 0.000\n",
      "[2,    28] loss: 0.001\n",
      "[2,    28] Acc: 0.000\n",
      "[2,    30] loss: 0.001\n",
      "[2,    30] Acc: 0.000\n",
      "[2,    32] loss: 0.001\n",
      "[2,    32] Acc: 0.000\n",
      "[2,    34] loss: 0.001\n",
      "[2,    34] Acc: 0.000\n",
      "[2,    36] loss: 0.001\n",
      "[2,    36] Acc: 0.000\n",
      "[2,    38] loss: 0.001\n",
      "[2,    38] Acc: 0.000\n",
      "[2,    40] loss: 0.001\n",
      "[2,    40] Acc: 0.000\n",
      "[2,    42] loss: 0.000\n",
      "[2,    42] Acc: 0.000\n",
      "[2,    44] loss: 0.001\n",
      "[2,    44] Acc: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.type(torch.LongTensor).to(CTX))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(outputs, labels)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] Acc: {acc / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50c980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/fishnet_{}'.format(timestamp))\n",
    "# epoch_number = 0\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# best_vloss = 1_000_000.\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "#     # Make sure gradient tracking is on, and do a pass over the data\n",
    "#     model.train(True)\n",
    "#     avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "#     # We don't need gradients on to do reporting\n",
    "#     model.train(False)\n",
    "\n",
    "#     running_vloss = 0.0\n",
    "#     for i, vdata in enumerate(val_dl):\n",
    "#         vinputs, vlabels = vdata\n",
    "#         voutputs = model(vinputs.to(CTX))\n",
    "#         vloss = loss_fn(voutputs, vlabels.type(torch.LongTensor).to(CTX))\n",
    "#         running_vloss += vloss\n",
    "\n",
    "#     avg_vloss = running_vloss / (i + 1)\n",
    "#     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "#     # Log the running loss averaged per batch\n",
    "#     # for both training and validation\n",
    "#     writer.add_scalars('Training vs. Validation Loss',\n",
    "#                     { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "#                     epoch_number + 1)\n",
    "#     writer.flush()\n",
    "\n",
    "#     # Track best performance, and save the model's state\n",
    "#     if avg_vloss < best_vloss:\n",
    "#         best_vloss = avg_vloss\n",
    "#         model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "\n",
    "#     epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb23ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
