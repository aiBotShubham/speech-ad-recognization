{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e092a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,valid_data,valid_label = pickle.load(f)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "data_path = 'adress_spectograms.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02aeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(2267).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(device = CTX, dtype=torch.float),train_y.to(device = CTX, dtype=torch.float)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74ffcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(567).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeebeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 16\n",
    "# val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26bad205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(device = CTX, dtype=torch.float))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.6f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "        \n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "#         print(x.shape())\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.SpatialGate = SpatialGate()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        M_c = self.ChannelGate(x)\n",
    "        M_s = self.SpatialGate(x)\n",
    "        \n",
    "        M_f = self.sigmoid(M_c + M_s)\n",
    "        \n",
    "        \n",
    "        return x + x*M_f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7c1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "        #The LW_CNN module utilizes three convolutions (C), two max-pooling\n",
    "        # (MP), one average-pooling (AP), and one batch normalization\n",
    "        # (BN) layer.\n",
    "        self.LW_CNN = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "#             Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             BatchNorm2d(256),\n",
    "#             ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "#             # Defining another 2D convolution layer\n",
    "#             Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             BatchNorm2d(128),\n",
    "#             ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "              #C1, we used 120 number of kernels with size (11\n",
    "              # × 11) using (4 × 4) stride setting without padding to extract\n",
    "              # initially hidden patterns from input data. \n",
    "              Conv2d(2, 120, kernel_size=(11,11), stride=(4,4), padding=0),\n",
    "              MaxPool2d(kernel_size=(3,3), stride=None),\n",
    "              Conv2d(120, 256, kernel_size=(5,5), stride=(1,1), padding='same'),\n",
    "              MaxPool2d(kernel_size=(3,3)),\n",
    "              Conv2d(256, 384, kernel_size=(3,3), padding='same'),\n",
    "#               MaxPool2d(kernel_size=1, stride=0),\n",
    "#               Conv2d(128, 1, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "              torch.nn.AvgPool2d(kernel_size=(1,1)),\n",
    "              BatchNorm2d(384),\n",
    "              ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(384 * 9 * 1, 256),\n",
    "            Linear(256, 64),\n",
    "            Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.attention = SAM(gate_channels=384)\n",
    "        \n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\n",
    "#         print(x.size)\n",
    "        x = self.LW_CNN(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ab46cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (LW_CNN): Sequential(\n",
      "    (0): Conv2d(2, 120, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(120, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "    (3): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (5): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
      "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=3456, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention): SAM(\n",
      "    (ChannelGate): ChannelGate(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten()\n",
      "        (1): Linear(in_features=384, out_features=24, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=24, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (SpatialGate): SpatialGate(\n",
      "      (compress): ChannelPool()\n",
      "      (spatial): BasicConv(\n",
      "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87fd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 120, 14, 84]          29,160\n",
      "         MaxPool2d-2           [-1, 120, 4, 28]               0\n",
      "            Conv2d-3           [-1, 256, 4, 28]         768,256\n",
      "         MaxPool2d-4            [-1, 256, 1, 9]               0\n",
      "            Conv2d-5            [-1, 384, 1, 9]         885,120\n",
      "         AvgPool2d-6            [-1, 384, 1, 9]               0\n",
      "       BatchNorm2d-7            [-1, 384, 1, 9]             768\n",
      "              ReLU-8            [-1, 384, 1, 9]               0\n",
      "           Flatten-9                  [-1, 384]               0\n",
      "           Linear-10                   [-1, 24]           9,240\n",
      "             ReLU-11                   [-1, 24]               0\n",
      "           Linear-12                  [-1, 384]           9,600\n",
      "          Flatten-13                  [-1, 384]               0\n",
      "           Linear-14                   [-1, 24]           9,240\n",
      "             ReLU-15                   [-1, 24]               0\n",
      "           Linear-16                  [-1, 384]           9,600\n",
      "      ChannelGate-17            [-1, 384, 1, 9]               0\n",
      "      ChannelPool-18              [-1, 2, 1, 9]               0\n",
      "           Conv2d-19              [-1, 1, 1, 9]              98\n",
      "      BatchNorm2d-20              [-1, 1, 1, 9]               2\n",
      "        BasicConv-21              [-1, 1, 1, 9]               0\n",
      "      SpatialGate-22            [-1, 384, 1, 9]               0\n",
      "          Sigmoid-23            [-1, 384, 1, 9]               0\n",
      "              SAM-24            [-1, 384, 1, 9]               0\n",
      "           Linear-25                  [-1, 256]         884,992\n",
      "           Linear-26                   [-1, 64]          16,448\n",
      "           Linear-27                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 2,622,654\n",
      "Trainable params: 2,622,654\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.17\n",
      "Forward/backward pass size (MB): 1.64\n",
      "Params size (MB): 10.00\n",
      "Estimated Total Size (MB): 11.81\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (2, 64, 344))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380379a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09dd2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.7890, val_loss: 0.7011, val_acc: 0.5613\n",
      "Epoch [1], train_loss: 0.7147, val_loss: 0.7241, val_acc: 0.5206\n",
      "Epoch [2], train_loss: 0.7110, val_loss: 0.7041, val_acc: 0.5457\n",
      "Epoch [3], train_loss: 0.6897, val_loss: 0.6760, val_acc: 0.5620\n",
      "Epoch [4], train_loss: 0.6840, val_loss: 0.6794, val_acc: 0.5831\n",
      "Epoch [5], train_loss: 0.6802, val_loss: 0.7106, val_acc: 0.5008\n",
      "Epoch [6], train_loss: 0.6746, val_loss: 0.9277, val_acc: 0.4783\n",
      "Epoch [7], train_loss: 0.6739, val_loss: 1.0432, val_acc: 0.4783\n",
      "Epoch [8], train_loss: 0.6615, val_loss: 0.7177, val_acc: 0.5636\n",
      "Epoch [9], train_loss: 0.6446, val_loss: 0.6757, val_acc: 0.6008\n",
      "Epoch [10], train_loss: 0.6416, val_loss: 0.6804, val_acc: 0.5817\n",
      "Epoch [11], train_loss: 0.6226, val_loss: 0.8687, val_acc: 0.4935\n",
      "Epoch [12], train_loss: 0.6100, val_loss: 0.6679, val_acc: 0.6199\n",
      "Epoch [13], train_loss: 0.5898, val_loss: 0.7893, val_acc: 0.5428\n",
      "Epoch [14], train_loss: 0.5556, val_loss: 0.6801, val_acc: 0.5994\n",
      "Epoch [15], train_loss: 0.5067, val_loss: 0.7633, val_acc: 0.5995\n",
      "Epoch [16], train_loss: 0.4564, val_loss: 0.9415, val_acc: 0.6106\n",
      "Epoch [17], train_loss: 0.3824, val_loss: 1.2462, val_acc: 0.5922\n",
      "Epoch [18], train_loss: 0.3400, val_loss: 1.3207, val_acc: 0.5265\n",
      "Epoch [19], train_loss: 0.2743, val_loss: 1.9466, val_acc: 0.5790\n",
      "Epoch [20], train_loss: 0.2176, val_loss: 1.4160, val_acc: 0.5661\n",
      "Epoch [21], train_loss: 0.1872, val_loss: 1.9743, val_acc: 0.5796\n",
      "Epoch [22], train_loss: 0.1446, val_loss: 2.2386, val_acc: 0.5828\n",
      "Epoch [23], train_loss: 0.1299, val_loss: 2.0249, val_acc: 0.5807\n",
      "Epoch [24], train_loss: 0.1138, val_loss: 1.6096, val_acc: 0.6046\n",
      "Epoch [25], train_loss: 0.1018, val_loss: 2.7320, val_acc: 0.5807\n",
      "Epoch [26], train_loss: 0.1113, val_loss: 2.1241, val_acc: 0.5672\n",
      "Epoch [27], train_loss: 0.0827, val_loss: 2.6087, val_acc: 0.5630\n",
      "Epoch [28], train_loss: 0.0739, val_loss: 2.9495, val_acc: 0.5248\n",
      "Epoch [29], train_loss: 0.0986, val_loss: 2.3133, val_acc: 0.6054\n",
      "Epoch [30], train_loss: 0.0637, val_loss: 3.1463, val_acc: 0.5897\n",
      "Epoch [31], train_loss: 0.0629, val_loss: 2.9218, val_acc: 0.5786\n",
      "Epoch [32], train_loss: 0.0508, val_loss: 3.8143, val_acc: 0.5480\n",
      "Epoch [33], train_loss: 0.0450, val_loss: 3.5481, val_acc: 0.5824\n",
      "Epoch [34], train_loss: 0.0588, val_loss: 2.9384, val_acc: 0.5953\n",
      "Epoch [35], train_loss: 0.0433, val_loss: 2.9350, val_acc: 0.6057\n",
      "Epoch [36], train_loss: 0.0520, val_loss: 2.1865, val_acc: 0.5990\n",
      "Epoch [37], train_loss: 0.0761, val_loss: 2.7625, val_acc: 0.5866\n",
      "Epoch [38], train_loss: 0.0308, val_loss: 3.6033, val_acc: 0.5866\n",
      "Epoch [39], train_loss: 0.0568, val_loss: 3.4428, val_acc: 0.6098\n",
      "Epoch [40], train_loss: 0.0400, val_loss: 2.3094, val_acc: 0.5963\n",
      "Epoch [41], train_loss: 0.0755, val_loss: 3.2358, val_acc: 0.6043\n",
      "Epoch [42], train_loss: 0.0412, val_loss: 3.5969, val_acc: 0.5783\n",
      "Epoch [43], train_loss: 0.0499, val_loss: 2.8904, val_acc: 0.6018\n",
      "Epoch [44], train_loss: 0.0199, val_loss: 4.1095, val_acc: 0.5894\n",
      "Epoch [45], train_loss: 0.0094, val_loss: 5.1873, val_acc: 0.6050\n",
      "Epoch [46], train_loss: 0.0277, val_loss: 3.7990, val_acc: 0.5845\n",
      "Epoch [47], train_loss: 0.0288, val_loss: 3.5271, val_acc: 0.5911\n",
      "Epoch [48], train_loss: 0.0268, val_loss: 3.2505, val_acc: 0.5942\n",
      "Epoch [49], train_loss: 0.0059, val_loss: 6.3740, val_acc: 0.5397\n",
      "Epoch [50], train_loss: 0.0478, val_loss: 3.1844, val_acc: 0.5904\n",
      "Epoch [51], train_loss: 0.1096, val_loss: 3.6271, val_acc: 0.5824\n",
      "Epoch [52], train_loss: 0.0506, val_loss: 3.8431, val_acc: 0.5765\n",
      "Epoch [53], train_loss: 0.0354, val_loss: 3.5000, val_acc: 0.5987\n",
      "Epoch [54], train_loss: 0.0133, val_loss: 4.2451, val_acc: 0.5970\n",
      "Epoch [55], train_loss: 0.0074, val_loss: 4.4720, val_acc: 0.5765\n",
      "Epoch [56], train_loss: 0.0179, val_loss: 3.3268, val_acc: 0.5776\n",
      "Epoch [57], train_loss: 0.0275, val_loss: 4.0153, val_acc: 0.6123\n",
      "Epoch [58], train_loss: 0.0432, val_loss: 3.1807, val_acc: 0.5942\n",
      "Epoch [59], train_loss: 0.0241, val_loss: 4.0526, val_acc: 0.5842\n",
      "Epoch [60], train_loss: 0.0347, val_loss: 3.2400, val_acc: 0.5950\n",
      "Epoch [61], train_loss: 0.0469, val_loss: 4.0637, val_acc: 0.5713\n",
      "Epoch [62], train_loss: 0.0468, val_loss: 3.1046, val_acc: 0.5772\n",
      "Epoch [63], train_loss: 0.0196, val_loss: 3.2057, val_acc: 0.5762\n",
      "Epoch [64], train_loss: 0.0075, val_loss: 4.3797, val_acc: 0.5661\n",
      "Epoch [65], train_loss: 0.0004, val_loss: 4.9600, val_acc: 0.5644\n",
      "Epoch [66], train_loss: 0.0001, val_loss: 5.2974, val_acc: 0.5609\n",
      "Epoch [67], train_loss: 0.0001, val_loss: 5.3371, val_acc: 0.5679\n",
      "Epoch [68], train_loss: 0.0001, val_loss: 5.4086, val_acc: 0.5627\n",
      "Epoch [69], train_loss: 0.0001, val_loss: 5.5773, val_acc: 0.5696\n",
      "Epoch [70], train_loss: 0.0000, val_loss: 5.9227, val_acc: 0.5679\n",
      "Epoch [71], train_loss: 0.0000, val_loss: 5.8015, val_acc: 0.5696\n",
      "Epoch [72], train_loss: 0.0000, val_loss: 6.2632, val_acc: 0.5661\n",
      "Epoch [73], train_loss: 0.0000, val_loss: 6.0373, val_acc: 0.5627\n",
      "Epoch [74], train_loss: 0.0000, val_loss: 6.1518, val_acc: 0.5679\n",
      "Epoch [75], train_loss: 0.0000, val_loss: 6.2776, val_acc: 0.5661\n",
      "Epoch [76], train_loss: 0.0000, val_loss: 6.3016, val_acc: 0.5644\n",
      "Epoch [77], train_loss: 0.0000, val_loss: 6.2594, val_acc: 0.5627\n",
      "Epoch [78], train_loss: 0.0000, val_loss: 6.3222, val_acc: 0.5679\n",
      "Epoch [79], train_loss: 0.0000, val_loss: 6.4285, val_acc: 0.5644\n",
      "Epoch [80], train_loss: 0.0000, val_loss: 6.5984, val_acc: 0.5661\n",
      "Epoch [81], train_loss: 0.0000, val_loss: 6.7744, val_acc: 0.5627\n",
      "Epoch [82], train_loss: 0.0000, val_loss: 6.9800, val_acc: 0.5574\n",
      "Epoch [83], train_loss: 0.0000, val_loss: 6.8009, val_acc: 0.5644\n",
      "Epoch [84], train_loss: 0.0000, val_loss: 7.0563, val_acc: 0.5627\n",
      "Epoch [85], train_loss: 0.0000, val_loss: 7.0450, val_acc: 0.5609\n",
      "Epoch [86], train_loss: 0.0000, val_loss: 7.0973, val_acc: 0.5574\n",
      "Epoch [87], train_loss: 0.0000, val_loss: 7.2456, val_acc: 0.5592\n",
      "Epoch [88], train_loss: 0.0000, val_loss: 7.2525, val_acc: 0.5574\n",
      "Epoch [89], train_loss: 0.0000, val_loss: 7.1780, val_acc: 0.5609\n",
      "Epoch [90], train_loss: 0.0000, val_loss: 7.2729, val_acc: 0.5661\n",
      "Epoch [91], train_loss: 0.0000, val_loss: 7.5439, val_acc: 0.5609\n",
      "Epoch [92], train_loss: 0.0000, val_loss: 7.5533, val_acc: 0.5661\n",
      "Epoch [93], train_loss: 0.0000, val_loss: 7.5170, val_acc: 0.5644\n",
      "Epoch [94], train_loss: 0.0000, val_loss: 7.5964, val_acc: 0.5661\n",
      "Epoch [95], train_loss: 0.0000, val_loss: 7.6201, val_acc: 0.5679\n",
      "Epoch [96], train_loss: 0.0000, val_loss: 7.6901, val_acc: 0.5731\n",
      "Epoch [97], train_loss: 0.0000, val_loss: 7.7238, val_acc: 0.5696\n",
      "Epoch [98], train_loss: 0.0000, val_loss: 7.5998, val_acc: 0.5627\n",
      "Epoch [99], train_loss: 0.3575, val_loss: 2.1402, val_acc: 0.5609\n",
      "Epoch [100], train_loss: 0.1086, val_loss: 2.5807, val_acc: 0.5688\n",
      "Epoch [101], train_loss: 0.0511, val_loss: 3.0184, val_acc: 0.5796\n",
      "Epoch [102], train_loss: 0.0173, val_loss: 3.9535, val_acc: 0.5685\n",
      "Epoch [103], train_loss: 0.0445, val_loss: 3.4345, val_acc: 0.5651\n",
      "Epoch [104], train_loss: 0.0132, val_loss: 3.7238, val_acc: 0.6036\n",
      "Epoch [105], train_loss: 0.0017, val_loss: 4.6129, val_acc: 0.5765\n",
      "Epoch [106], train_loss: 0.0004, val_loss: 4.8820, val_acc: 0.5783\n",
      "Epoch [107], train_loss: 0.0004, val_loss: 5.1565, val_acc: 0.5776\n",
      "Epoch [108], train_loss: 0.0001, val_loss: 5.3513, val_acc: 0.5852\n",
      "Epoch [109], train_loss: 0.0003, val_loss: 5.7306, val_acc: 0.5620\n",
      "Epoch [110], train_loss: 0.0002, val_loss: 5.8160, val_acc: 0.5665\n",
      "Epoch [111], train_loss: 0.0239, val_loss: 2.6687, val_acc: 0.5533\n",
      "Epoch [112], train_loss: 0.0845, val_loss: 2.7304, val_acc: 0.5550\n",
      "Epoch [113], train_loss: 0.0462, val_loss: 2.3154, val_acc: 0.5744\n",
      "Epoch [114], train_loss: 0.0183, val_loss: 4.1723, val_acc: 0.5612\n",
      "Epoch [115], train_loss: 0.0458, val_loss: 2.9109, val_acc: 0.5550\n",
      "Epoch [116], train_loss: 0.0070, val_loss: 4.3447, val_acc: 0.5696\n",
      "Epoch [117], train_loss: 0.0007, val_loss: 4.8209, val_acc: 0.5759\n",
      "Epoch [118], train_loss: 0.0005, val_loss: 5.0361, val_acc: 0.5689\n",
      "Epoch [119], train_loss: 0.0001, val_loss: 5.2794, val_acc: 0.5724\n",
      "Epoch [120], train_loss: 0.0001, val_loss: 5.4886, val_acc: 0.5793\n",
      "Epoch [121], train_loss: 0.0000, val_loss: 5.2837, val_acc: 0.5759\n",
      "Epoch [122], train_loss: 0.0001, val_loss: 5.5847, val_acc: 0.5835\n",
      "Epoch [123], train_loss: 0.0000, val_loss: 5.7074, val_acc: 0.5793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 0.0000, val_loss: 5.8729, val_acc: 0.5765\n",
      "Epoch [125], train_loss: 0.0000, val_loss: 5.7818, val_acc: 0.5845\n",
      "Epoch [126], train_loss: 0.0000, val_loss: 6.0353, val_acc: 0.5776\n",
      "Epoch [127], train_loss: 0.0000, val_loss: 6.1616, val_acc: 0.5717\n",
      "Epoch [128], train_loss: 0.0000, val_loss: 6.0794, val_acc: 0.5682\n",
      "Epoch [129], train_loss: 0.0000, val_loss: 6.2709, val_acc: 0.5665\n",
      "Epoch [130], train_loss: 0.0000, val_loss: 6.3082, val_acc: 0.5682\n",
      "Epoch [131], train_loss: 0.0000, val_loss: 6.3669, val_acc: 0.5700\n",
      "Epoch [132], train_loss: 0.0000, val_loss: 6.4474, val_acc: 0.5700\n",
      "Epoch [133], train_loss: 0.0000, val_loss: 6.4645, val_acc: 0.5700\n",
      "Epoch [134], train_loss: 0.0000, val_loss: 6.5214, val_acc: 0.5724\n",
      "Epoch [135], train_loss: 0.0000, val_loss: 6.7639, val_acc: 0.5793\n",
      "Epoch [136], train_loss: 0.0000, val_loss: 6.7460, val_acc: 0.5776\n",
      "Epoch [137], train_loss: 0.0000, val_loss: 6.9229, val_acc: 0.5741\n",
      "Epoch [138], train_loss: 0.0000, val_loss: 6.6310, val_acc: 0.5682\n",
      "Epoch [139], train_loss: 0.0000, val_loss: 6.8206, val_acc: 0.5707\n",
      "Epoch [140], train_loss: 0.0000, val_loss: 6.7755, val_acc: 0.5717\n",
      "Epoch [141], train_loss: 0.0000, val_loss: 6.9400, val_acc: 0.5682\n",
      "Epoch [142], train_loss: 0.0000, val_loss: 6.9035, val_acc: 0.5759\n",
      "Epoch [143], train_loss: 0.0961, val_loss: 3.0321, val_acc: 0.5700\n",
      "Epoch [144], train_loss: 0.1975, val_loss: 2.6187, val_acc: 0.5668\n",
      "Epoch [145], train_loss: 0.0526, val_loss: 3.0834, val_acc: 0.5787\n",
      "Epoch [146], train_loss: 0.0176, val_loss: 3.4280, val_acc: 0.5800\n",
      "Epoch [147], train_loss: 0.0148, val_loss: 3.8442, val_acc: 0.5765\n",
      "Epoch [148], train_loss: 0.0203, val_loss: 3.4239, val_acc: 0.5609\n",
      "Epoch [149], train_loss: 0.0207, val_loss: 3.4489, val_acc: 0.5991\n",
      "Epoch [150], train_loss: 0.0023, val_loss: 3.7923, val_acc: 0.5894\n",
      "Epoch [151], train_loss: 0.0004, val_loss: 4.0520, val_acc: 0.5918\n",
      "Epoch [152], train_loss: 0.0003, val_loss: 4.4835, val_acc: 0.5883\n",
      "Epoch [153], train_loss: 0.0003, val_loss: 4.6519, val_acc: 0.5883\n",
      "Epoch [154], train_loss: 0.0001, val_loss: 4.7996, val_acc: 0.5866\n",
      "Epoch [155], train_loss: 0.0001, val_loss: 4.9087, val_acc: 0.5918\n",
      "Epoch [156], train_loss: 0.0001, val_loss: 5.0267, val_acc: 0.5953\n",
      "Epoch [157], train_loss: 0.0001, val_loss: 5.1675, val_acc: 0.5918\n",
      "Epoch [158], train_loss: 0.0000, val_loss: 5.1814, val_acc: 0.5870\n",
      "Epoch [159], train_loss: 0.0000, val_loss: 5.2036, val_acc: 0.5859\n",
      "Epoch [160], train_loss: 0.0000, val_loss: 5.2534, val_acc: 0.5911\n",
      "Epoch [161], train_loss: 0.0000, val_loss: 5.3317, val_acc: 0.5894\n",
      "Epoch [162], train_loss: 0.0000, val_loss: 5.4370, val_acc: 0.5876\n",
      "Epoch [163], train_loss: 0.0000, val_loss: 5.5710, val_acc: 0.5894\n",
      "Epoch [164], train_loss: 0.0000, val_loss: 5.5713, val_acc: 0.5859\n",
      "Epoch [165], train_loss: 0.0000, val_loss: 5.5596, val_acc: 0.5939\n",
      "Epoch [166], train_loss: 0.0000, val_loss: 5.7132, val_acc: 0.5870\n",
      "Epoch [167], train_loss: 0.0000, val_loss: 5.6628, val_acc: 0.5887\n",
      "Epoch [168], train_loss: 0.0000, val_loss: 5.7641, val_acc: 0.5887\n",
      "Epoch [169], train_loss: 0.0000, val_loss: 5.7785, val_acc: 0.5904\n",
      "Epoch [170], train_loss: 0.0000, val_loss: 5.8768, val_acc: 0.5876\n",
      "Epoch [171], train_loss: 0.0000, val_loss: 6.0243, val_acc: 0.5870\n",
      "Epoch [172], train_loss: 0.0000, val_loss: 5.9243, val_acc: 0.5870\n",
      "Epoch [173], train_loss: 0.0000, val_loss: 5.9804, val_acc: 0.5887\n",
      "Epoch [174], train_loss: 0.0000, val_loss: 5.9835, val_acc: 0.5870\n",
      "Epoch [175], train_loss: 0.0000, val_loss: 6.2043, val_acc: 0.5835\n",
      "Epoch [176], train_loss: 0.0000, val_loss: 6.4011, val_acc: 0.5852\n",
      "Epoch [177], train_loss: 0.0000, val_loss: 6.2861, val_acc: 0.5852\n",
      "Epoch [178], train_loss: 0.0000, val_loss: 6.4685, val_acc: 0.5887\n",
      "Epoch [179], train_loss: 0.0000, val_loss: 6.3023, val_acc: 0.5870\n",
      "Epoch [180], train_loss: 0.0000, val_loss: 6.6362, val_acc: 0.5963\n",
      "Epoch [181], train_loss: 0.0000, val_loss: 6.7395, val_acc: 0.5911\n",
      "Epoch [182], train_loss: 0.0000, val_loss: 6.5255, val_acc: 0.5859\n",
      "Epoch [183], train_loss: 0.0000, val_loss: 6.6782, val_acc: 0.5946\n",
      "Epoch [184], train_loss: 0.0000, val_loss: 6.6346, val_acc: 0.5963\n",
      "Epoch [185], train_loss: 0.0000, val_loss: 6.6777, val_acc: 0.5963\n",
      "Epoch [186], train_loss: 0.0000, val_loss: 6.7356, val_acc: 0.5911\n",
      "Epoch [187], train_loss: 0.0000, val_loss: 7.0525, val_acc: 0.5998\n",
      "Epoch [188], train_loss: 0.0000, val_loss: 6.9860, val_acc: 0.6015\n",
      "Epoch [189], train_loss: 0.0000, val_loss: 6.9549, val_acc: 0.5928\n",
      "Epoch [190], train_loss: 0.0000, val_loss: 7.0825, val_acc: 0.6015\n",
      "Epoch [191], train_loss: 0.0000, val_loss: 7.1158, val_acc: 0.5998\n",
      "Epoch [192], train_loss: 0.0000, val_loss: 7.2481, val_acc: 0.5998\n",
      "Epoch [193], train_loss: 0.0000, val_loss: 7.2170, val_acc: 0.5963\n",
      "Epoch [194], train_loss: 0.0000, val_loss: 7.5072, val_acc: 0.5946\n",
      "Epoch [195], train_loss: 0.0000, val_loss: 7.4121, val_acc: 0.5963\n",
      "Epoch [196], train_loss: 0.0000, val_loss: 7.2255, val_acc: 0.5963\n",
      "Epoch [197], train_loss: 0.0000, val_loss: 6.9702, val_acc: 0.5928\n",
      "Epoch [198], train_loss: 0.0000, val_loss: 7.3290, val_acc: 0.5963\n",
      "Epoch [199], train_loss: 0.0000, val_loss: 7.5277, val_acc: 0.5998\n",
      "Epoch [200], train_loss: 0.0000, val_loss: 7.4452, val_acc: 0.5894\n",
      "Epoch [201], train_loss: 0.0000, val_loss: 7.4907, val_acc: 0.5870\n",
      "Epoch [202], train_loss: 0.0000, val_loss: 7.4031, val_acc: 0.5928\n",
      "Epoch [203], train_loss: 0.0000, val_loss: 7.7014, val_acc: 0.5981\n",
      "Epoch [204], train_loss: 0.0000, val_loss: 7.6438, val_acc: 0.5998\n",
      "Epoch [205], train_loss: 0.0000, val_loss: 7.7782, val_acc: 0.6015\n",
      "Epoch [206], train_loss: 0.0000, val_loss: 7.7777, val_acc: 0.5981\n",
      "Epoch [207], train_loss: 0.0000, val_loss: 7.8344, val_acc: 0.5998\n",
      "Epoch [208], train_loss: 0.0000, val_loss: 7.8030, val_acc: 0.5981\n",
      "Epoch [209], train_loss: 0.0000, val_loss: 7.9319, val_acc: 0.5963\n",
      "Epoch [210], train_loss: 0.0000, val_loss: 7.7745, val_acc: 0.5870\n",
      "Epoch [211], train_loss: 0.0000, val_loss: 8.2834, val_acc: 0.5981\n",
      "Epoch [212], train_loss: 0.0000, val_loss: 8.0312, val_acc: 0.5963\n",
      "Epoch [213], train_loss: 0.0000, val_loss: 8.0668, val_acc: 0.5946\n",
      "Epoch [214], train_loss: 0.0000, val_loss: 7.9392, val_acc: 0.5928\n",
      "Epoch [215], train_loss: 0.0000, val_loss: 8.0297, val_acc: 0.5870\n",
      "Epoch [216], train_loss: 0.0000, val_loss: 8.0533, val_acc: 0.5876\n",
      "Epoch [217], train_loss: 0.0000, val_loss: 8.3244, val_acc: 0.5946\n",
      "Epoch [218], train_loss: 0.0000, val_loss: 8.2651, val_acc: 0.5981\n",
      "Epoch [219], train_loss: 0.0000, val_loss: 8.0414, val_acc: 0.5998\n",
      "Epoch [220], train_loss: 0.0000, val_loss: 8.1643, val_acc: 0.6033\n",
      "Epoch [221], train_loss: 0.0000, val_loss: 8.6976, val_acc: 0.5939\n",
      "Epoch [222], train_loss: 0.0000, val_loss: 8.5828, val_acc: 0.5946\n",
      "Epoch [223], train_loss: 0.0000, val_loss: 8.4151, val_acc: 0.5946\n",
      "Epoch [224], train_loss: 0.0000, val_loss: 8.3388, val_acc: 0.5911\n",
      "Epoch [225], train_loss: 0.0000, val_loss: 8.6071, val_acc: 0.5904\n",
      "Epoch [226], train_loss: 0.0000, val_loss: 8.6655, val_acc: 0.5963\n",
      "Epoch [227], train_loss: 0.0000, val_loss: 8.5809, val_acc: 0.5946\n",
      "Epoch [228], train_loss: 0.0000, val_loss: 8.7775, val_acc: 0.5946\n",
      "Epoch [229], train_loss: 0.0000, val_loss: 8.6237, val_acc: 0.5939\n",
      "Epoch [230], train_loss: 0.0000, val_loss: 8.6257, val_acc: 0.5956\n",
      "Epoch [231], train_loss: 0.0000, val_loss: 8.6745, val_acc: 0.5991\n",
      "Epoch [232], train_loss: 0.0000, val_loss: 9.1576, val_acc: 0.5991\n",
      "Epoch [233], train_loss: 0.0000, val_loss: 8.7841, val_acc: 0.5887\n",
      "Epoch [234], train_loss: 0.0000, val_loss: 8.9307, val_acc: 0.5939\n",
      "Epoch [235], train_loss: 0.0431, val_loss: 3.9373, val_acc: 0.5647\n",
      "Epoch [236], train_loss: 0.3345, val_loss: 2.3720, val_acc: 0.5713\n",
      "Epoch [237], train_loss: 0.0388, val_loss: 2.6049, val_acc: 0.5696\n",
      "Epoch [238], train_loss: 0.0159, val_loss: 3.9070, val_acc: 0.5623\n",
      "Epoch [239], train_loss: 0.0039, val_loss: 4.2234, val_acc: 0.5807\n",
      "Epoch [240], train_loss: 0.0199, val_loss: 4.3263, val_acc: 0.5543\n",
      "Epoch [241], train_loss: 0.0259, val_loss: 3.6340, val_acc: 0.5720\n",
      "Epoch [242], train_loss: 0.0085, val_loss: 4.3064, val_acc: 0.5651\n",
      "Epoch [243], train_loss: 0.0011, val_loss: 4.8411, val_acc: 0.5685\n",
      "Epoch [244], train_loss: 0.0013, val_loss: 5.1201, val_acc: 0.5627\n",
      "Epoch [245], train_loss: 0.0004, val_loss: 5.3127, val_acc: 0.5675\n",
      "Epoch [246], train_loss: 0.0001, val_loss: 5.4635, val_acc: 0.5675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247], train_loss: 0.0001, val_loss: 5.5161, val_acc: 0.5668\n",
      "Epoch [248], train_loss: 0.0001, val_loss: 5.7647, val_acc: 0.5675\n",
      "Epoch [249], train_loss: 0.0001, val_loss: 5.8897, val_acc: 0.5692\n",
      "Epoch [250], train_loss: 0.0001, val_loss: 6.0081, val_acc: 0.5623\n",
      "Epoch [251], train_loss: 0.0001, val_loss: 6.1667, val_acc: 0.5553\n",
      "Epoch [252], train_loss: 0.0000, val_loss: 6.1461, val_acc: 0.5623\n",
      "Epoch [253], train_loss: 0.0000, val_loss: 6.5247, val_acc: 0.5543\n",
      "Epoch [254], train_loss: 0.0000, val_loss: 6.3586, val_acc: 0.5595\n",
      "Epoch [255], train_loss: 0.0000, val_loss: 6.4625, val_acc: 0.5595\n",
      "Epoch [256], train_loss: 0.0000, val_loss: 6.3901, val_acc: 0.5612\n",
      "Epoch [257], train_loss: 0.0000, val_loss: 6.5311, val_acc: 0.5612\n",
      "Epoch [258], train_loss: 0.0000, val_loss: 6.6263, val_acc: 0.5595\n",
      "Epoch [259], train_loss: 0.0000, val_loss: 6.4759, val_acc: 0.5623\n",
      "Epoch [260], train_loss: 0.0000, val_loss: 6.7901, val_acc: 0.5560\n",
      "Epoch [261], train_loss: 0.0000, val_loss: 6.7000, val_acc: 0.5612\n",
      "Epoch [262], train_loss: 0.0000, val_loss: 6.7103, val_acc: 0.5560\n",
      "Epoch [263], train_loss: 0.0000, val_loss: 6.8386, val_acc: 0.5647\n",
      "Epoch [264], train_loss: 0.0000, val_loss: 7.0639, val_acc: 0.5647\n",
      "Epoch [265], train_loss: 0.0000, val_loss: 6.7917, val_acc: 0.5716\n",
      "Epoch [266], train_loss: 0.0000, val_loss: 6.8493, val_acc: 0.5647\n",
      "Epoch [267], train_loss: 0.0000, val_loss: 7.3733, val_acc: 0.5560\n",
      "Epoch [268], train_loss: 0.0000, val_loss: 7.1777, val_acc: 0.5630\n",
      "Epoch [269], train_loss: 0.0000, val_loss: 7.0196, val_acc: 0.5612\n",
      "Epoch [270], train_loss: 0.0000, val_loss: 7.2497, val_acc: 0.5577\n",
      "Epoch [271], train_loss: 0.0000, val_loss: 7.0669, val_acc: 0.5647\n",
      "Epoch [272], train_loss: 0.0000, val_loss: 7.2434, val_acc: 0.5647\n",
      "Epoch [273], train_loss: 0.0000, val_loss: 7.1725, val_acc: 0.5577\n",
      "Epoch [274], train_loss: 0.0000, val_loss: 7.5237, val_acc: 0.5612\n",
      "Epoch [275], train_loss: 0.0002, val_loss: 7.6842, val_acc: 0.5720\n",
      "Epoch [276], train_loss: 0.2427, val_loss: 2.0876, val_acc: 0.5675\n",
      "Epoch [277], train_loss: 0.0462, val_loss: 3.6669, val_acc: 0.5759\n",
      "Epoch [278], train_loss: 0.0438, val_loss: 3.1832, val_acc: 0.5411\n",
      "Epoch [279], train_loss: 0.0235, val_loss: 4.0757, val_acc: 0.5870\n",
      "Epoch [280], train_loss: 0.0075, val_loss: 4.6820, val_acc: 0.5557\n",
      "Epoch [281], train_loss: 0.0242, val_loss: 3.1701, val_acc: 0.5529\n",
      "Epoch [282], train_loss: 0.0179, val_loss: 4.2677, val_acc: 0.5602\n",
      "Epoch [283], train_loss: 0.0007, val_loss: 4.9904, val_acc: 0.5657\n",
      "Epoch [284], train_loss: 0.0006, val_loss: 5.2177, val_acc: 0.5762\n",
      "Epoch [285], train_loss: 0.0002, val_loss: 5.3721, val_acc: 0.5640\n",
      "Epoch [286], train_loss: 0.0033, val_loss: 5.9427, val_acc: 0.5633\n",
      "Epoch [287], train_loss: 0.0383, val_loss: 3.9357, val_acc: 0.5612\n",
      "Epoch [288], train_loss: 0.0060, val_loss: 4.6523, val_acc: 0.5671\n",
      "Epoch [289], train_loss: 0.0008, val_loss: 5.0876, val_acc: 0.5505\n",
      "Epoch [290], train_loss: 0.0003, val_loss: 5.2995, val_acc: 0.5633\n",
      "Epoch [291], train_loss: 0.0001, val_loss: 5.5740, val_acc: 0.5651\n",
      "Epoch [292], train_loss: 0.0001, val_loss: 5.6319, val_acc: 0.5581\n",
      "Epoch [293], train_loss: 0.0001, val_loss: 5.6283, val_acc: 0.5494\n",
      "Epoch [294], train_loss: 0.0000, val_loss: 6.0289, val_acc: 0.5581\n",
      "Epoch [295], train_loss: 0.0000, val_loss: 5.9815, val_acc: 0.5477\n",
      "Epoch [296], train_loss: 0.0000, val_loss: 6.1447, val_acc: 0.5557\n",
      "Epoch [297], train_loss: 0.0000, val_loss: 6.1785, val_acc: 0.5564\n",
      "Epoch [298], train_loss: 0.0000, val_loss: 6.1655, val_acc: 0.5581\n",
      "Epoch [299], train_loss: 0.0000, val_loss: 6.2475, val_acc: 0.5522\n",
      "Epoch [300], train_loss: 0.0000, val_loss: 6.4234, val_acc: 0.5581\n",
      "Epoch [301], train_loss: 0.0000, val_loss: 6.5075, val_acc: 0.5505\n",
      "Epoch [302], train_loss: 0.0000, val_loss: 6.5115, val_acc: 0.5557\n",
      "Epoch [303], train_loss: 0.0000, val_loss: 6.5276, val_acc: 0.5540\n",
      "Epoch [304], train_loss: 0.0000, val_loss: 6.6293, val_acc: 0.5453\n",
      "Epoch [305], train_loss: 0.0000, val_loss: 6.7853, val_acc: 0.5557\n",
      "Epoch [306], train_loss: 0.0000, val_loss: 6.8915, val_acc: 0.5546\n",
      "Epoch [307], train_loss: 0.0000, val_loss: 6.7548, val_acc: 0.5581\n",
      "Epoch [308], train_loss: 0.0000, val_loss: 6.7520, val_acc: 0.5540\n",
      "Epoch [309], train_loss: 0.0000, val_loss: 6.7124, val_acc: 0.5564\n",
      "Epoch [310], train_loss: 0.0000, val_loss: 6.9294, val_acc: 0.5546\n",
      "Epoch [311], train_loss: 0.0000, val_loss: 7.0394, val_acc: 0.5546\n",
      "Epoch [312], train_loss: 0.0000, val_loss: 7.0792, val_acc: 0.5540\n",
      "Epoch [313], train_loss: 0.0000, val_loss: 7.1837, val_acc: 0.5505\n",
      "Epoch [314], train_loss: 0.0000, val_loss: 7.1316, val_acc: 0.5564\n",
      "Epoch [315], train_loss: 0.0000, val_loss: 7.1183, val_acc: 0.5581\n",
      "Epoch [316], train_loss: 0.0000, val_loss: 7.2951, val_acc: 0.5529\n",
      "Epoch [317], train_loss: 0.0000, val_loss: 7.0346, val_acc: 0.5557\n",
      "Epoch [318], train_loss: 0.0000, val_loss: 7.5248, val_acc: 0.5522\n",
      "Epoch [319], train_loss: 0.0000, val_loss: 7.4007, val_acc: 0.5546\n",
      "Epoch [320], train_loss: 0.0000, val_loss: 7.5647, val_acc: 0.5616\n",
      "Epoch [321], train_loss: 0.0000, val_loss: 7.4602, val_acc: 0.5540\n",
      "Epoch [322], train_loss: 0.0000, val_loss: 7.9514, val_acc: 0.5623\n",
      "Epoch [323], train_loss: 0.0000, val_loss: 8.0721, val_acc: 0.5609\n",
      "Epoch [324], train_loss: 0.0000, val_loss: 7.5872, val_acc: 0.5574\n",
      "Epoch [325], train_loss: 0.0000, val_loss: 7.7166, val_acc: 0.5529\n",
      "Epoch [326], train_loss: 0.0000, val_loss: 7.5421, val_acc: 0.5616\n",
      "Epoch [327], train_loss: 0.0000, val_loss: 7.8628, val_acc: 0.5599\n",
      "Epoch [328], train_loss: 0.0000, val_loss: 8.2750, val_acc: 0.5616\n",
      "Epoch [329], train_loss: 0.2723, val_loss: 1.8737, val_acc: 0.5362\n",
      "Epoch [330], train_loss: 0.0561, val_loss: 2.9605, val_acc: 0.5922\n",
      "Epoch [331], train_loss: 0.0138, val_loss: 3.4977, val_acc: 0.5627\n",
      "Epoch [332], train_loss: 0.0020, val_loss: 4.2182, val_acc: 0.5620\n",
      "Epoch [333], train_loss: 0.0005, val_loss: 4.4855, val_acc: 0.5602\n",
      "Epoch [334], train_loss: 0.0003, val_loss: 4.9168, val_acc: 0.5602\n",
      "Epoch [335], train_loss: 0.0002, val_loss: 5.0681, val_acc: 0.5672\n",
      "Epoch [336], train_loss: 0.0002, val_loss: 5.0237, val_acc: 0.5637\n",
      "Epoch [337], train_loss: 0.0001, val_loss: 5.2007, val_acc: 0.5585\n",
      "Epoch [338], train_loss: 0.0001, val_loss: 5.6302, val_acc: 0.5585\n",
      "Epoch [339], train_loss: 0.0001, val_loss: 5.5727, val_acc: 0.5498\n",
      "Epoch [340], train_loss: 0.0001, val_loss: 5.6409, val_acc: 0.5620\n",
      "Epoch [341], train_loss: 0.0000, val_loss: 5.8382, val_acc: 0.5620\n",
      "Epoch [342], train_loss: 0.0001, val_loss: 5.6451, val_acc: 0.5585\n",
      "Epoch [343], train_loss: 0.0000, val_loss: 5.5600, val_acc: 0.5533\n",
      "Epoch [344], train_loss: 0.0000, val_loss: 5.7803, val_acc: 0.5498\n",
      "Epoch [345], train_loss: 0.0000, val_loss: 6.0094, val_acc: 0.5550\n",
      "Epoch [346], train_loss: 0.0000, val_loss: 5.9195, val_acc: 0.5568\n",
      "Epoch [347], train_loss: 0.0000, val_loss: 6.2056, val_acc: 0.5585\n",
      "Epoch [348], train_loss: 0.0000, val_loss: 6.5153, val_acc: 0.5602\n",
      "Epoch [349], train_loss: 0.0000, val_loss: 6.1627, val_acc: 0.5550\n",
      "Epoch [350], train_loss: 0.0000, val_loss: 6.2618, val_acc: 0.5533\n",
      "Epoch [351], train_loss: 0.0000, val_loss: 6.5580, val_acc: 0.5637\n",
      "Epoch [352], train_loss: 0.0000, val_loss: 6.6940, val_acc: 0.5568\n",
      "Epoch [353], train_loss: 0.0000, val_loss: 6.5955, val_acc: 0.5516\n",
      "Epoch [354], train_loss: 0.0000, val_loss: 6.3269, val_acc: 0.5550\n",
      "Epoch [355], train_loss: 0.0000, val_loss: 6.4236, val_acc: 0.5568\n",
      "Epoch [356], train_loss: 0.0000, val_loss: 6.7791, val_acc: 0.5481\n",
      "Epoch [357], train_loss: 0.0000, val_loss: 6.5631, val_acc: 0.5620\n",
      "Epoch [358], train_loss: 0.0000, val_loss: 6.8233, val_acc: 0.5498\n",
      "Epoch [359], train_loss: 0.0000, val_loss: 7.0013, val_acc: 0.5533\n",
      "Epoch [360], train_loss: 0.0000, val_loss: 6.9466, val_acc: 0.5585\n",
      "Epoch [361], train_loss: 0.0000, val_loss: 7.0126, val_acc: 0.5568\n",
      "Epoch [362], train_loss: 0.0000, val_loss: 7.0376, val_acc: 0.5533\n",
      "Epoch [363], train_loss: 0.0000, val_loss: 6.9464, val_acc: 0.5533\n",
      "Epoch [364], train_loss: 0.0000, val_loss: 7.0927, val_acc: 0.5550\n",
      "Epoch [365], train_loss: 0.0000, val_loss: 7.4121, val_acc: 0.5550\n",
      "Epoch [366], train_loss: 0.0000, val_loss: 7.0008, val_acc: 0.5568\n",
      "Epoch [367], train_loss: 0.0000, val_loss: 7.2706, val_acc: 0.5516\n",
      "Epoch [368], train_loss: 0.0000, val_loss: 7.3701, val_acc: 0.5585\n",
      "Epoch [369], train_loss: 0.0000, val_loss: 7.4289, val_acc: 0.5568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370], train_loss: 0.0000, val_loss: 7.6929, val_acc: 0.5637\n",
      "Epoch [371], train_loss: 0.0000, val_loss: 7.5352, val_acc: 0.5516\n",
      "Epoch [372], train_loss: 0.0000, val_loss: 7.4940, val_acc: 0.5516\n",
      "Epoch [373], train_loss: 0.0000, val_loss: 7.7806, val_acc: 0.5481\n",
      "Epoch [374], train_loss: 0.0000, val_loss: 7.8324, val_acc: 0.5481\n",
      "Epoch [375], train_loss: 0.0000, val_loss: 7.3649, val_acc: 0.5457\n",
      "Epoch [376], train_loss: 0.0000, val_loss: 7.8382, val_acc: 0.5516\n",
      "Epoch [377], train_loss: 0.0000, val_loss: 7.8848, val_acc: 0.5481\n",
      "Epoch [378], train_loss: 0.0000, val_loss: 7.7550, val_acc: 0.5533\n",
      "Epoch [379], train_loss: 0.0000, val_loss: 7.9631, val_acc: 0.5498\n",
      "Epoch [380], train_loss: 0.0000, val_loss: 8.0728, val_acc: 0.5533\n",
      "Epoch [381], train_loss: 0.0000, val_loss: 7.8943, val_acc: 0.5516\n",
      "Epoch [382], train_loss: 0.0000, val_loss: 7.9707, val_acc: 0.5550\n",
      "Epoch [383], train_loss: 0.0000, val_loss: 8.1556, val_acc: 0.5481\n",
      "Epoch [384], train_loss: 0.0000, val_loss: 7.9883, val_acc: 0.5533\n",
      "Epoch [385], train_loss: 0.0000, val_loss: 8.1853, val_acc: 0.5481\n",
      "Epoch [386], train_loss: 0.0000, val_loss: 8.5137, val_acc: 0.5481\n",
      "Epoch [387], train_loss: 0.0000, val_loss: 8.3181, val_acc: 0.5463\n",
      "Epoch [388], train_loss: 0.0000, val_loss: 8.5008, val_acc: 0.5498\n",
      "Epoch [389], train_loss: 0.0000, val_loss: 8.4943, val_acc: 0.5498\n",
      "Epoch [390], train_loss: 0.0000, val_loss: 8.0987, val_acc: 0.5533\n",
      "Epoch [391], train_loss: 0.0000, val_loss: 8.6897, val_acc: 0.5481\n",
      "Epoch [392], train_loss: 0.0000, val_loss: 8.4327, val_acc: 0.5481\n",
      "Epoch [393], train_loss: 0.0000, val_loss: 8.7964, val_acc: 0.5533\n",
      "Epoch [394], train_loss: 0.0000, val_loss: 8.3324, val_acc: 0.5509\n",
      "Epoch [395], train_loss: 0.0000, val_loss: 8.8266, val_acc: 0.5498\n",
      "Epoch [396], train_loss: 0.0000, val_loss: 8.8008, val_acc: 0.5533\n",
      "Epoch [397], train_loss: 0.0000, val_loss: 8.8195, val_acc: 0.5481\n",
      "Epoch [398], train_loss: 0.0000, val_loss: 9.0903, val_acc: 0.5516\n",
      "Epoch [399], train_loss: 0.0000, val_loss: 8.7831, val_acc: 0.5481\n"
     ]
    }
   ],
   "source": [
    "# CTX = torch.device('cuda')\n",
    "# train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # train_dl.train_labels.to(CTX)\n",
    "# # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # val_dl.train_labels.to(CTX)\n",
    "num_epochs = 400\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01884434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
