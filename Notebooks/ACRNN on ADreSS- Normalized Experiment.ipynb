{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab67967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472dfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'adress_norm.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "model_name = 'model.ckpt'\n",
    "pred_name = 'pred0.pkl'\n",
    "checkpoint_secs = 60\n",
    "\n",
    "dropout_conv = 1\n",
    "dropout_linear = 1\n",
    "dropout_lstm = 1\n",
    "dropout_fully1 = 1\n",
    "dropout_fully2 = 1\n",
    "\n",
    "#decayed_learning rate\n",
    "decay_rate = 0.99\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "#Moving Average\n",
    "decay_steps = 570\n",
    "momentum = 0.99\n",
    "num_epoch = 500\n",
    "relu_clip =  20.0\n",
    "\n",
    "# Adam optimizer (http://arxiv.org/abs/1412.6980) parameters\n",
    "\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "epsilon =  1e-8\n",
    "learning_rate =   0.0001\n",
    "\n",
    "# Batch sizes\n",
    "\n",
    "\n",
    "train_batch_size = 40\n",
    "valid_batch_size = 40\n",
    "test_batch_size =  40\n",
    "batch_size =  40\n",
    "save_steps =   10\n",
    "\n",
    "image_height =   300\n",
    "image_width = 40\n",
    "image_channel = 3\n",
    "\n",
    "linear_num =  786\n",
    "seq_len =   150\n",
    "cell_num = 128\n",
    "hidden1 = 64\n",
    "hidden2 =  4\n",
    "attention_size =   1\n",
    "# attention = False\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "    #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "    #v = tf.tanh(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "    v = tf.sigmoid(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu)              # (B,T) shape also\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc02e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "epsilon = 1e-3\n",
    "\n",
    "def leaky_relu(x, leakiness=0.0):\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "\n",
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training is not None:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "def acrnn(inputs, num_classes=2,\n",
    "                  is_training=True,\n",
    "                  L1=128,\n",
    "                  L2=256,\n",
    "                  cell_units=128,\n",
    "                  num_linear=768,\n",
    "                  p=10,\n",
    "                  time_step=150,\n",
    "                  F1=64,\n",
    "                  dropout_keep_prob=1):\n",
    "    \n",
    "    global ndims\n",
    "    layer1_filter = tf.get_variable('layer1_filter', shape=[5, 3, 3, L1], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer1_bias = tf.get_variable('layer1_bias', shape=[L1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer1_stride = [1, 1, 1, 1]\n",
    "    layer2_filter = tf.get_variable('layer2_filter', shape=[5, 3, L1, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer2_bias = tf.get_variable('layer2_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer2_stride = [1, 1, 1, 1]\n",
    "    layer3_filter = tf.get_variable('layer3_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer3_bias = tf.get_variable('layer3_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer3_stride = [1, 1, 1, 1]\n",
    "    layer4_filter = tf.get_variable('layer4_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer4_bias = tf.get_variable('layer4_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer4_stride = [1, 1, 1, 1]\n",
    "    layer5_filter = tf.get_variable('layer5_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer5_bias = tf.get_variable('layer5_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer5_stride = [1, 1, 1, 1]\n",
    "    layer6_filter = tf.get_variable('layer6_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer6_bias = tf.get_variable('layer6_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer6_stride = [1, 1, 1, 1]\n",
    "    \n",
    "    linear1_weight = tf.get_variable('linear1_weight', shape=[p*L2,num_linear], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    linear1_bias = tf.get_variable('linear1_bias', shape=[num_linear], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    " \n",
    "    fully1_weight = tf.get_variable('fully1_weight', shape=[2*cell_units,F1], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully1_bias = tf.get_variable('fully1_bias', shape=[F1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    fully2_weight = tf.get_variable('fully2_weight', shape=[F1,num_classes], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully2_bias = tf.get_variable('fully2_bias', shape=[num_classes], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(inputs, layer1_filter, layer1_stride, padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1,layer1_bias)\n",
    "    layer1 = leaky_relu(layer1, 0.01)\n",
    "    layer1 = tf.nn.max_pool(layer1,ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID', name='max_pool')\n",
    "    layer1 = tf.layers.dropout(layer1, rate = 1 - keep_prob)\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2,layer2_bias)\n",
    "    layer2 = leaky_relu(layer2, 0.01)\n",
    "    layer2 = tf.layers.dropout(layer2, rate = 1 - keep_prob)\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer2, layer3_filter, layer3_stride, padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3,layer3_bias)\n",
    "    layer3 = leaky_relu(layer3, 0.01)\n",
    "    layer3 = tf.layers.dropout(layer3, rate = 1 - keep_prob)\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer3, layer4_filter, layer4_stride, padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4,layer4_bias)\n",
    "    layer4 = leaky_relu(layer4, 0.01)\n",
    "    layer4 = tf.layers.dropout(layer4, rate = 1 - keep_prob)\n",
    "    \n",
    "    layer5 = tf.nn.conv2d(layer4, layer5_filter, layer5_stride, padding='SAME')\n",
    "    layer5 = tf.nn.bias_add(layer5,layer5_bias)\n",
    "    layer5 = leaky_relu(layer5, 0.01)    \n",
    "    layer5 = tf.layers.dropout(layer5, rate = 1 - keep_prob)\n",
    "\n",
    "    layer6 = tf.nn.conv2d(layer5, layer6_filter, layer6_stride, padding='SAME')\n",
    "    layer6 = tf.nn.bias_add(layer6,layer6_bias)\n",
    "    layer6 = leaky_relu(layer6, 0.01)    \n",
    "    layer6 = tf.layers.dropout(layer6, rate = 1 - keep_prob)\n",
    "    \n",
    "    layer6 = tf.reshape(layer6,[-1,time_step,L2*p])\n",
    "    layer6 = tf.reshape(layer6, [-1,p*L2])\n",
    "    \n",
    "    linear1 = tf.matmul(layer6,linear1_weight) + linear1_bias\n",
    "    linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = leaky_relu(linear1, 0.01)\n",
    "    #linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = tf.reshape(linear1, [-1, time_step, num_linear])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.nn.rnn_cell.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.nn.rnn_cell.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    \n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs1, output_states1 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell1,\n",
    "                                                             cell_bw=gru_bw_cell1,\n",
    "                                                             inputs= linear1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM1')\n",
    "\n",
    "    # Attention layer\n",
    "    gru, alphas = attention(outputs1, attention_size, return_alphas=True)\n",
    "    \n",
    "    \n",
    "    fully1 = tf.matmul(gru,fully1_weight) + fully1_bias\n",
    "    fully1 = leaky_relu(fully1, 0.01)\n",
    "    fully1 = tf.nn.dropout(fully1, dropout_keep_prob)\n",
    "    \n",
    "    \n",
    "    Ylogits = tf.matmul(fully1, fully2_weight) + fully2_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    return Ylogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ecaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8974477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = KerasClassifier(build_fn=acrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739087a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# X, y = make_classification()\n",
    "# estimator.fit(X, y)\n",
    "\n",
    "# # This is what you need\n",
    "# estimator.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cfcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,valid_data,valid_label = pickle.load(f)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes = 2):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5510bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\AppData\\Local\\Temp/ipykernel_19428/276794933.py:136: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:438: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:750: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:393: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:699: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "Epoch: 00001\n",
      "Training cost: 0.697\n",
      "Training accuracy: 0.45\n",
      "Valid cost: 0.716\n",
      "Valid_UA: 0.4888\n",
      "Best valid_UA: 0.4888\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[56 17]\n",
      " [60 16]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[56 17]\n",
      " [60 16]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00006\n",
      "Training cost: 0.697\n",
      "Training accuracy: 0.525\n",
      "Valid cost: 0.701\n",
      "Valid_UA: 0.4569\n",
      "Best valid_UA: 0.4888\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[11 62]\n",
      " [18 58]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[56 17]\n",
      " [60 16]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00011\n",
      "Training cost: 0.68\n",
      "Training accuracy: 0.55\n",
      "Valid cost: 0.699\n",
      "Valid_UA: 0.5\n",
      "Best valid_UA: 0.5\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[ 0 73]\n",
      " [ 0 76]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[ 0 73]\n",
      " [ 0 76]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00016\n",
      "Training cost: 0.693\n",
      "Training accuracy: 0.525\n",
      "Valid cost: 0.709\n",
      "Valid_UA: 0.4878\n",
      "Best valid_UA: 0.5\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[28 45]\n",
      " [31 45]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[ 0 73]\n",
      " [ 0 76]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00021\n",
      "Training cost: 0.64\n",
      "Training accuracy: 0.675\n",
      "Valid cost: 0.711\n",
      "Valid_UA: 0.5375\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00026\n",
      "Training cost: 0.702\n",
      "Training accuracy: 0.475\n",
      "Valid cost: 0.71\n",
      "Valid_UA: 0.5183\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[19 54]\n",
      " [17 59]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00031\n",
      "Training cost: 0.653\n",
      "Training accuracy: 0.675\n",
      "Valid cost: 0.707\n",
      "Valid_UA: 0.5087\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[32 41]\n",
      " [32 44]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00036\n",
      "Training cost: 0.624\n",
      "Training accuracy: 0.6\n",
      "Valid cost: 0.723\n",
      "Valid_UA: 0.5\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[73  0]\n",
      " [76  0]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00041\n",
      "Training cost: 0.659\n",
      "Training accuracy: 0.6\n",
      "Valid cost: 0.724\n",
      "Valid_UA: 0.4838\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[13 60]\n",
      " [16 60]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00046\n",
      "Training cost: 0.691\n",
      "Training accuracy: 0.55\n",
      "Valid cost: 0.702\n",
      "Valid_UA: 0.4418\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[28 45]\n",
      " [38 38]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00051\n",
      "Training cost: 0.668\n",
      "Training accuracy: 0.625\n",
      "Valid cost: 0.702\n",
      "Valid_UA: 0.439\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[42 31]\n",
      " [53 23]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00056\n",
      "Training cost: 0.653\n",
      "Training accuracy: 0.65\n",
      "Valid cost: 0.696\n",
      "Valid_UA: 0.5241\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[16 57]\n",
      " [13 63]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00061\n",
      "Training cost: 0.658\n",
      "Training accuracy: 0.6\n",
      "Valid cost: 0.701\n",
      "Valid_UA: 0.5\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[73  0]\n",
      " [76  0]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00066\n",
      "Training cost: 0.642\n",
      "Training accuracy: 0.725\n",
      "Valid cost: 0.702\n",
      "Valid_UA: 0.4676\n",
      "Best valid_UA: 0.5375\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[26 47]\n",
      " [32 44]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[17 56]\n",
      " [12 64]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00071\n",
      "Training cost: 0.65\n",
      "Training accuracy: 0.68\n",
      "Valid cost: 0.704\n",
      "Valid_UA: 0.547\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00076\n",
      "Training cost: 0.679\n",
      "Training accuracy: 0.525\n",
      "Valid cost: 0.701\n",
      "Valid_UA: 0.4944\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[28 45]\n",
      " [30 46]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00081\n",
      "Training cost: 0.644\n",
      "Training accuracy: 0.65\n",
      "Valid cost: 0.698\n",
      "Valid_UA: 0.545\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[69  4]\n",
      " [65 11]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00086\n",
      "Training cost: 0.742\n",
      "Training accuracy: 0.475\n",
      "Valid cost: 0.701\n",
      "Valid_UA: 0.445\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[40 33]\n",
      " [50 26]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00091\n",
      "Training cost: 0.642\n",
      "Training accuracy: 0.625\n",
      "Valid cost: 0.707\n",
      "Valid_UA: 0.4417\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [63 13]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00096\n",
      "Training cost: 0.631\n",
      "Training accuracy: 0.675\n",
      "Valid cost: 0.717\n",
      "Valid_UA: 0.5249\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[19 54]\n",
      " [16 60]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00101\n",
      "Training cost: 0.593\n",
      "Training accuracy: 0.775\n",
      "Valid cost: 0.711\n",
      "Valid_UA: 0.5259\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[47 26]\n",
      " [45 31]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00106\n",
      "Training cost: 0.574\n",
      "Training accuracy: 0.7\n",
      "Valid cost: 0.728\n",
      "Valid_UA: 0.5447\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[68  5]\n",
      " [64 12]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n",
      "*****************************************************************\n",
      "Epoch: 00111\n",
      "Training cost: 0.664\n",
      "Training accuracy: 0.625\n",
      "Valid cost: 0.723\n",
      "Valid_UA: 0.5\n",
      "Best valid_UA: 0.547\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[ 0 73]\n",
      " [ 0 76]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[52 21]\n",
      " [47 29]]\n",
      "*****************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "Epoch: 00116\n",
      "Training cost: 0.657\n",
      "Training accuracy: 0.6\n",
      "Valid cost: 0.696\n",
      "Valid_UA: 0.5657\n",
      "Best valid_UA: 0.5657\n",
      "Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[24 49]\n",
      " [15 61]]\n",
      "Best Valid Confusion Matrix:[\"cc\",\"cd\"]\n",
      "[[24 49]\n",
      " [15 61]]\n",
      "*****************************************************************\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "import os\n",
    "\n",
    "num_classes = 2\n",
    "is_adam = True\n",
    "dropout_keep_prob = 1\n",
    "# data_path = 'adress_train_valid.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)\n",
    "# Valid_label = valid_label\n",
    "# vnum = 149\n",
    "\n",
    "train_label = dense_to_one_hot(train_label,num_classes)\n",
    "valid_label = dense_to_one_hot(valid_label,num_classes)\n",
    "# valid_label = dense_to_one_hot(valid_label,num_classes)\n",
    "\n",
    "valid_size = valid_data.shape[0]\n",
    "dataset_size = train_data.shape[0]\n",
    "vnum = valid_data.shape[0]\n",
    "best_valid_uw = 0\n",
    "\n",
    "\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, image_height,image_width,image_channel])\n",
    "Y = tf.compat.v1.placeholder(tf.int32, shape=[None, num_classes])\n",
    "\n",
    "is_training = tf.compat.v1.placeholder(tf.bool)\n",
    "lr = tf.compat.v1.placeholder(tf.float32)\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "Ylogits = acrnn(X, is_training=is_training, dropout_keep_prob=keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "var_trainable_op = tf.trainable_variables()\n",
    "if is_adam:\n",
    "    # not apply gradient clipping\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(cost)            \n",
    "else:\n",
    "    # apply gradient clipping\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, var_trainable_op), 5)\n",
    "    opti = tf.train.AdamOptimizer(lr)\n",
    "    train_op = opti.apply_gradients(zip(grads, var_trainable_op))\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver=tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epocs = []\n",
    "validation_accuracy = []\n",
    "training_accuracy = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epoch):\n",
    "        #learning_rate = FLAGS.learning_rate            \n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "        [_,tcost,tracc] = sess.run([train_op,cost,accuracy], feed_dict={X:train_data[start:end,:,:,:], Y:train_label[start:end,:],\n",
    "                                        is_training:True, keep_prob:dropout_keep_prob, lr:learning_rate})\n",
    "        if i % 5 == 0:\n",
    "            #for valid data\n",
    "            valid_iter = divmod((valid_size),batch_size)[0]\n",
    "            y_pred_valid = np.empty((valid_size,num_classes),dtype=np.float32)\n",
    "            y_valid = np.empty((vnum,2),dtype=np.float32)\n",
    "            index = 0\n",
    "            cost_valid = 0\n",
    "            if(valid_size < batch_size):\n",
    "                loss, y_pred_valid = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data, Y:valid_label,is_training:False, keep_prob:1})\n",
    "                cost_valid = cost_valid + np.sum(loss)\n",
    "            for v in range(valid_iter):\n",
    "                v_begin = v*batch_size\n",
    "                v_end = (v+1)*batch_size\n",
    "                if(v == valid_iter-1):\n",
    "                    if(v_end < valid_size):\n",
    "                        v_end = valid_size\n",
    "                loss, y_pred_valid[v_begin:v_end,:] = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data[v_begin:v_end],Y:valid_label[v_begin:v_end],is_training:False, keep_prob:1})\n",
    "                cost_valid = cost_valid + np.sum(loss)\n",
    "            cost_valid = cost_valid/valid_size\n",
    "            \n",
    "#             for s in range(vnum):\n",
    "#                 y_valid[s,:] = np.max(y_pred_valid[index:index+ pernums_valid[s][0],:],0)\n",
    "#                 index = index + pernums_valid[s][0]\n",
    "\n",
    "            valid_acc_uw = recall(np.argmax(valid_label,1),np.argmax(y_valid,1),average='macro')\n",
    "            valid_conf = confusion(np.argmax(valid_label, 1),np.argmax(y_valid,1))\n",
    "            \n",
    "            if valid_acc_uw > best_valid_uw:\n",
    "                best_valid_uw = valid_acc_uw\n",
    "                best_valid_conf = valid_conf\n",
    "                saver.save(sess, os.path.join(checkpoint, model_name), global_step = i+1)\n",
    "            \n",
    "            print (\"*****************************************************************\")\n",
    "            print (\"Epoch: %05d\" %(i+1))\n",
    "            epocs.append(i+1)\n",
    "            print (\"Training cost: %2.3g\" %tcost)   \n",
    "            print (\"Training accuracy: %3.4g\" %tracc) \n",
    "            training_accuracy.append(tracc)\n",
    "            print (\"Valid cost: %2.3g\" %cost_valid)\n",
    "            print (\"Valid_UA: %3.4g\" %valid_acc_uw) \n",
    "            validation_accuracy.append(valid_acc_uw)\n",
    "            print (\"Best valid_UA: %3.4g\" %best_valid_uw) \n",
    "            print ('Valid Confusion Matrix:[\"cc\",\"cd\"]')\n",
    "            print (valid_conf)\n",
    "            print ('Best Valid Confusion Matrix:[\"cc\",\"cd\"]')\n",
    "            print (best_valid_conf)\n",
    "            print (\"*****************************************************************\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pernums_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pernums_valid[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid[index:index+ pernums_valid[s][0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid[0:1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867113ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pernums_valid[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "\n",
    "test_iter = divmod((test_size),batch_size)[0]\n",
    "y_pred_valid = np.empty((valid_size,num_classes),dtype=np.float32)\n",
    "y_valid = np.empty((vnum,2),dtype=np.float32)\n",
    "index = 0\n",
    "cost_valid = 0\n",
    "if(valid_size < batch_size):\n",
    "    loss, y_pred_valid = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data, Y:valid_label,is_training:False, keep_prob:1})\n",
    "    cost_valid = cost_valid + np.sum(loss)\n",
    "for v in range(valid_iter):\n",
    "    v_begin = v*batch_size\n",
    "    v_end = (v+1)*batch_size\n",
    "    if(v == valid_iter-1):\n",
    "        if(v_end < valid_size):\n",
    "            v_end = valid_size\n",
    "    loss, y_pred_valid[v_begin:v_end,:] = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data[v_begin:v_end],Y:valid_label[v_begin:v_end],is_training:False, keep_prob:1})\n",
    "    cost_valid = cost_valid + np.sum(loss)\n",
    "cost_valid = cost_valid/valid_size\n",
    "\n",
    "\n",
    "valid_acc_uw = recall(np.argmax(valid_label,1),np.argmax(y_valid,1),average='macro')\n",
    "valid_conf = confusion(np.argmax(valid_label, 1),np.argmax(y_valid,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
