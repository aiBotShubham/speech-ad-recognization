{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,valid_data,valid_label = pickle.load(f)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "# data_path = 'adress_512.pkl'\n",
    "data_path = 'adress_norm.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)\n",
    "\n",
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(train_y.shape[0]).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(CTX),train_y.to(CTX)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09537d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(val_y.shape[0]).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4701d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "# val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d492176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "from sklearn.metrics import recall_score as recall\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out, _softmax= self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out, _softmax = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(_softmax, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        \n",
    "        valid_conf = confusion(np.argmax(_softmax.cpu(),1),labels.cpu())\n",
    "#         valid_conf = 8008\n",
    "#         print(np.argmax(out.cpu(),1))\n",
    "#         print(labels.cpu())\n",
    "        print ('Valid Confusion Matrix:[\"cc\",\"cd\"]')\n",
    "        print(valid_conf)\n",
    "        print ('Recall')\n",
    "        print(recall(np.argmax(_softmax.cpu(),1),labels.cpu(),average='macro'))\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc }\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.8f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "#         print (result['valid_conf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3):\n",
    "        super().__init__()\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(1)\n",
    "        gate_channels=[channel]\n",
    "        gate_channels+=[channel//reduction]*num_layers\n",
    "        gate_channels+=[channel]\n",
    "\n",
    "\n",
    "        self.ca=nn.Sequential()\n",
    "        self.ca.add_module('flatten',Flatten())\n",
    "        for i in range(len(gate_channels)-2):\n",
    "            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))\n",
    "            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))\n",
    "            self.ca.add_module('relu%d'%i,nn.ReLU())\n",
    "        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))\n",
    "        \n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.avgpool(x)\n",
    "        res=self.ca(res)\n",
    "        res=res.unsqueeze(-1).unsqueeze(-1).expand_as(x)\n",
    "        return res\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.sa=nn.Sequential()\n",
    "        self.sa.add_module('conv_reduce1',nn.Conv2d(kernel_size=1,in_channels=channel,out_channels=channel//reduction))\n",
    "        self.sa.add_module('bn_reduce1',nn.BatchNorm2d(channel//reduction))\n",
    "        self.sa.add_module('relu_reduce1',nn.ReLU())\n",
    "        for i in range(num_layers):\n",
    "            self.sa.add_module('conv_%d'%i,nn.Conv2d(kernel_size=3,in_channels=channel//reduction,out_channels=channel//reduction,padding=1,dilation=dia_val))\n",
    "            self.sa.add_module('bn_%d'%i,nn.BatchNorm2d(channel//reduction))\n",
    "            self.sa.add_module('relu_%d'%i,nn.ReLU())\n",
    "        self.sa.add_module('last_conv',nn.Conv2d(channel//reduction,1,kernel_size=1))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.sa(x)\n",
    "        res=res.expand_as(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BAMBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=16,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.ca=ChannelAttention(channel=channel,reduction=reduction)\n",
    "        self.sa=SpatialAttention(channel=channel,reduction=reduction,dia_val=dia_val)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        sa_out=self.sa(x)\n",
    "        ca_out=self.ca(x)\n",
    "        weight=self.sigmoid(sa_out+ca_out)\n",
    "        out=(1+weight)*x\n",
    "        return out\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input=torch.randn(50,512,7,7)\n",
    "#     bam = BAMBlock(channel=512,reduction=16,dia_val=2)\n",
    "#     output=bam(input)\n",
    "#     print(output.shape)\n",
    "    \n",
    "\n",
    "#                   Conv2d(120, 256, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "# #               MaxPool2d(kernel_size=(2,2)),\n",
    "# #               Conv2d(256, 384, kernel_size=(2,2), padding='same'),\n",
    "# #               MaxPool2d(kernel_size=1, stride=0),\n",
    "# #               Conv2d(128, 1, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "#               torch.nn.AvgPool2d(kernel_size=(2,2), stride=1),\n",
    "#               BatchNorm2d(256),\n",
    "#               ReLU(inplace=True),\n",
    "class BAM(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(BAM, self).__init__()\n",
    "        #The LW_CNN module utilizes three convolutions (C), two max-pooling\n",
    "        # (MP), one average-pooling (AP), and one batch normalization\n",
    "        # (BN) layer.\n",
    "        self.LW_CNN1 = Sequential(\n",
    "              Conv2d(3, 256, kernel_size=(2,2), stride=(2,2), padding=0),\n",
    "              MaxPool2d(kernel_size=(2,2), stride=None),\n",
    "        )\n",
    "        self.attention1 = BAMBlock(channel=256,reduction=4,dia_val=1)\n",
    "        self.LW_CNN2 = Sequential(\n",
    "              Conv2d(256, 128, kernel_size=(2,2), stride=(2,2), padding=0),\n",
    "              MaxPool2d(kernel_size=(2,2), stride=None),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(136 , 4),\n",
    "#             Linear(256, 64),\n",
    "            Linear(4, 2)\n",
    "#             Linear(16, 2)\n",
    "        )\n",
    "        self.attention2 = BAMBlock(channel=128,reduction=4,dia_val=1)\n",
    "        self.LW_CNN3 = Sequential(\n",
    "              Conv2d(128, 64, kernel_size=(1,1), stride=(1,1), padding=0),\n",
    "              torch.nn.AvgPool2d(kernel_size=(2,2), stride=1),\n",
    "              BatchNorm2d(64)\n",
    "        )\n",
    "        self.attention3 = BAMBlock(channel=64,reduction=8,dia_val=1)\n",
    "        self.LW_CNN4 = Sequential(\n",
    "              Conv2d(64, 32, kernel_size=(1,1), stride=(1,1), padding=0),\n",
    "              Conv2d(32, 16, kernel_size=(1,1), stride=(1,1), padding=0),\n",
    "              Conv2d(16, 8, kernel_size=(1,1), stride=(1,1), padding=0),\n",
    "        )\n",
    "        self.softmax_out = nn.Softmax(dim=1)\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\n",
    "#         print(x.size)\n",
    "        x = self.LW_CNN1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.LW_CNN2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.LW_CNN3(x)\n",
    "        x = self.attention3(x)\n",
    "        x = self.LW_CNN4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        output_softmax = self.softmax_out(x)\n",
    "        return x,output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f81fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAM(\n",
      "  (LW_CNN1): Sequential(\n",
      "    (0): Conv2d(3, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (attention1): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=64, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN2): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=4, bias=True)\n",
      "    (1): Linear(in_features=4, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention2): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (bn0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=32, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN3): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (attention3): BAMBlock(\n",
      "    (ca): ChannelAttention(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (ca): Sequential(\n",
      "        (flatten): Flatten()\n",
      "        (fc0): Linear(in_features=64, out_features=8, bias=True)\n",
      "        (bn0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu0): ReLU()\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (last_fc): Linear(in_features=8, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (sa): Sequential(\n",
      "        (conv_reduce1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn_reduce1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_reduce1): ReLU()\n",
      "        (conv_0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_0): ReLU()\n",
      "        (conv_1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_1): ReLU()\n",
      "        (conv_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu_2): ReLU()\n",
      "        (last_conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (LW_CNN4): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (softmax_out): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = BAM()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477fd619",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 75, 10]         --\n",
      "|    └─Conv2d: 2-1                       [-1, 256, 150, 20]        3,328\n",
      "|    └─MaxPool2d: 2-2                    [-1, 256, 75, 10]         --\n",
      "├─BAMBlock: 1-2                          [-1, 256, 75, 10]         --\n",
      "|    └─SpatialAttention: 2-3             [-1, 256, 75, 10]         --\n",
      "|    |    └─Sequential: 3-1              [-1, 1, 75, 10]           127,809\n",
      "|    └─ChannelAttention: 2-4             [-1, 256, 75, 10]         --\n",
      "|    |    └─AdaptiveAvgPool2d: 3-2       [-1, 256, 1, 1]           --\n",
      "|    |    └─Sequential: 3-3              [-1, 256]                 41,792\n",
      "|    └─Sigmoid: 2-5                      [-1, 256, 75, 10]         --\n",
      "├─Sequential: 1-3                        [-1, 128, 18, 2]          --\n",
      "|    └─Conv2d: 2-6                       [-1, 128, 37, 5]          131,200\n",
      "|    └─MaxPool2d: 2-7                    [-1, 128, 18, 2]          --\n",
      "├─BAMBlock: 1-4                          [-1, 128, 18, 2]          --\n",
      "|    └─SpatialAttention: 2-8             [-1, 128, 18, 2]          --\n",
      "|    |    └─Sequential: 3-4              [-1, 1, 18, 2]            32,161\n",
      "|    └─ChannelAttention: 2-9             [-1, 128, 18, 2]          --\n",
      "|    |    └─AdaptiveAvgPool2d: 3-5       [-1, 128, 1, 1]           --\n",
      "|    |    └─Sequential: 3-6              [-1, 128]                 10,656\n",
      "|    └─Sigmoid: 2-10                     [-1, 128, 18, 2]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 17, 1]           --\n",
      "|    └─Conv2d: 2-11                      [-1, 64, 18, 2]           8,256\n",
      "|    └─AvgPool2d: 2-12                   [-1, 64, 17, 1]           --\n",
      "|    └─BatchNorm2d: 2-13                 [-1, 64, 17, 1]           128\n",
      "├─BAMBlock: 1-6                          [-1, 64, 17, 1]           --\n",
      "|    └─SpatialAttention: 2-14            [-1, 64, 17, 1]           --\n",
      "|    |    └─Sequential: 3-7              [-1, 1, 17, 1]            2,345\n",
      "|    └─ChannelAttention: 2-15            [-1, 64, 17, 1]           --\n",
      "|    |    └─AdaptiveAvgPool2d: 3-8       [-1, 64, 1, 1]            --\n",
      "|    |    └─Sequential: 3-9              [-1, 64]                  1,288\n",
      "|    └─Sigmoid: 2-16                     [-1, 64, 17, 1]           --\n",
      "├─Sequential: 1-7                        [-1, 8, 17, 1]            --\n",
      "|    └─Conv2d: 2-17                      [-1, 32, 17, 1]           2,080\n",
      "|    └─Conv2d: 2-18                      [-1, 16, 17, 1]           528\n",
      "|    └─Conv2d: 2-19                      [-1, 8, 17, 1]            136\n",
      "├─Sequential: 1-8                        [-1, 2]                   --\n",
      "|    └─Linear: 2-20                      [-1, 4]                   548\n",
      "|    └─Linear: 2-21                      [-1, 2]                   10\n",
      "├─Softmax: 1-9                           [-1, 2]                   --\n",
      "==========================================================================================\n",
      "Total params: 362,265\n",
      "Trainable params: 362,265\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 131.11\n",
      "==========================================================================================\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 9.10\n",
      "Params size (MB): 1.38\n",
      "Estimated Total Size (MB): 10.62\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 256, 75, 10]         --\n",
       "|    └─Conv2d: 2-1                       [-1, 256, 150, 20]        3,328\n",
       "|    └─MaxPool2d: 2-2                    [-1, 256, 75, 10]         --\n",
       "├─BAMBlock: 1-2                          [-1, 256, 75, 10]         --\n",
       "|    └─SpatialAttention: 2-3             [-1, 256, 75, 10]         --\n",
       "|    |    └─Sequential: 3-1              [-1, 1, 75, 10]           127,809\n",
       "|    └─ChannelAttention: 2-4             [-1, 256, 75, 10]         --\n",
       "|    |    └─AdaptiveAvgPool2d: 3-2       [-1, 256, 1, 1]           --\n",
       "|    |    └─Sequential: 3-3              [-1, 256]                 41,792\n",
       "|    └─Sigmoid: 2-5                      [-1, 256, 75, 10]         --\n",
       "├─Sequential: 1-3                        [-1, 128, 18, 2]          --\n",
       "|    └─Conv2d: 2-6                       [-1, 128, 37, 5]          131,200\n",
       "|    └─MaxPool2d: 2-7                    [-1, 128, 18, 2]          --\n",
       "├─BAMBlock: 1-4                          [-1, 128, 18, 2]          --\n",
       "|    └─SpatialAttention: 2-8             [-1, 128, 18, 2]          --\n",
       "|    |    └─Sequential: 3-4              [-1, 1, 18, 2]            32,161\n",
       "|    └─ChannelAttention: 2-9             [-1, 128, 18, 2]          --\n",
       "|    |    └─AdaptiveAvgPool2d: 3-5       [-1, 128, 1, 1]           --\n",
       "|    |    └─Sequential: 3-6              [-1, 128]                 10,656\n",
       "|    └─Sigmoid: 2-10                     [-1, 128, 18, 2]          --\n",
       "├─Sequential: 1-5                        [-1, 64, 17, 1]           --\n",
       "|    └─Conv2d: 2-11                      [-1, 64, 18, 2]           8,256\n",
       "|    └─AvgPool2d: 2-12                   [-1, 64, 17, 1]           --\n",
       "|    └─BatchNorm2d: 2-13                 [-1, 64, 17, 1]           128\n",
       "├─BAMBlock: 1-6                          [-1, 64, 17, 1]           --\n",
       "|    └─SpatialAttention: 2-14            [-1, 64, 17, 1]           --\n",
       "|    |    └─Sequential: 3-7              [-1, 1, 17, 1]            2,345\n",
       "|    └─ChannelAttention: 2-15            [-1, 64, 17, 1]           --\n",
       "|    |    └─AdaptiveAvgPool2d: 3-8       [-1, 64, 1, 1]            --\n",
       "|    |    └─Sequential: 3-9              [-1, 64]                  1,288\n",
       "|    └─Sigmoid: 2-16                     [-1, 64, 17, 1]           --\n",
       "├─Sequential: 1-7                        [-1, 8, 17, 1]            --\n",
       "|    └─Conv2d: 2-17                      [-1, 32, 17, 1]           2,080\n",
       "|    └─Conv2d: 2-18                      [-1, 16, 17, 1]           528\n",
       "|    └─Conv2d: 2-19                      [-1, 8, 17, 1]            136\n",
       "├─Sequential: 1-8                        [-1, 2]                   --\n",
       "|    └─Linear: 2-20                      [-1, 4]                   548\n",
       "|    └─Linear: 2-21                      [-1, 2]                   10\n",
       "├─Softmax: 1-9                           [-1, 2]                   --\n",
       "==========================================================================================\n",
       "Total params: 362,265\n",
       "Trainable params: 362,265\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.14\n",
       "Forward/backward pass size (MB): 9.10\n",
       "Params size (MB): 1.38\n",
       "Estimated Total Size (MB): 10.62\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torch-summary\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 300, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=torch.randn(50,256,7,7)\n",
    "# bam = BAMBlock(channel=256,reduction=16,dia_val=2)\n",
    "# output=bam(input)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(outputs, labels):\n",
    "#     _, preds = torch.max(outputs, dim=1)\n",
    "#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "# @torch.no_grad()\n",
    "# def evaluate(model, val_loader):\n",
    "#     model.eval()\n",
    "#     outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "# def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "#     history = []\n",
    "#     optimizer = opt_func(model.parameters(),lr)\n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         model.train()\n",
    "#         train_losses = []\n",
    "#         for batch in train_loader:\n",
    "#             loss = model.training_step(batch)\n",
    "#             train_losses.append(loss)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         result = evaluate(model, val_loader)\n",
    "#         result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "#         model.epoch_end(epoch, result)\n",
    "#         history.append(result)\n",
    "    \n",
    "#     return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a72cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CTX = torch.device('cuda')\n",
    "# # train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # # train_dl.train_labels.to(CTX)\n",
    "# # # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # # val_dl.train_labels.to(CTX)\n",
    "# num_epochs = 24\n",
    "# opt_func = torch.optim.Adam\n",
    "# lr = 0.007\n",
    "# #fitting the model on training data and record the result after each epoch\n",
    "# history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26874318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_accuracies(history):\n",
    "#     \"\"\" Plot the history of accuracies\"\"\"\n",
    "#     accuracies = [x['val_acc'] for x in history]\n",
    "#     plt.plot(accuracies, '-x')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.title('Accuracy vs. No. of epochs');\n",
    "    \n",
    "\n",
    "# plot_accuracies(history)\n",
    "\n",
    "# def plot_losses(history):\n",
    "#     \"\"\" Plot the losses in each epoch\"\"\"\n",
    "#     train_losses = [x.get('train_loss') for x in history]\n",
    "#     val_losses = [x['val_loss'] for x in history]\n",
    "#     plt.plot(train_losses, '-bx')\n",
    "#     plt.plot(val_losses, '-rx')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.legend(['Training', 'Validation'])\n",
    "#     plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "# plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60afc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([x['val_acc'] for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f3164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function; CrossEntropyLoss() fairly standard for multiclass problems \n",
    "def criterion(predictions, targets): \n",
    "    return nn.CrossEntropyLoss()(input=predictions, target=targets)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# define function to create a single step of the training phase\n",
    "def make_train_step(model, criterion, optimizer):\n",
    "    \n",
    "    # define the training step of the training phase\n",
    "    def train_step(X,Y):\n",
    "        \n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        \n",
    "        # compute loss on logits because nn.CrossEntropyLoss implements log softmax\n",
    "        loss = criterion(output_logits, Y) \n",
    "        \n",
    "        # compute gradients for the optimizer to use \n",
    "        loss.backward()\n",
    "        \n",
    "        # update network parameters based on gradient stored (by calling loss.backward())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero out gradients for next pass\n",
    "        # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step\n",
    "\n",
    "def make_validate_fnc(model,criterion):\n",
    "    def validate(X,Y):\n",
    "        \n",
    "        # don't want to update any network parameters on validation passes: don't need gradient\n",
    "        # wrap in torch.no_grad to save memory and compute in validation phase: \n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
    "            model.eval()\n",
    "      \n",
    "            # get the model's predictions on the validation set\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "\n",
    "            # calculate the mean accuracy over the entire validation set\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            \n",
    "            # compute error from logits (nn.crossentropy implements softmax)\n",
    "            loss = criterion(output_logits,Y)\n",
    "            \n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate\n",
    "\n",
    "def make_save_checkpoint(): \n",
    "    def save_checkpoint(optimizer, model, epoch, filename):\n",
    "        checkpoint_dict = {\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "    return save_checkpoint\n",
    "\n",
    "def load_checkpoint(optimizer, model, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    model.load_state_dict(checkpoint_dict['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad031387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda selected\n",
      "Number of trainable params:  362265\n"
     ]
    }
   ],
   "source": [
    "# get training set size to calculate # iterations and minibatch indices\n",
    "train_size = train_x.shape[0]\n",
    "\n",
    "# pick minibatch size (of 32... always)\n",
    "minibatch = 32\n",
    "\n",
    "# set device to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device} selected')\n",
    "\n",
    "# instantiate model and move to GPU for training\n",
    "model = BAM().to(device) \n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "\n",
    "# encountered bugs in google colab only, unless I explicitly defined optimizer in this cell...\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# instantiate the checkpoint save function\n",
    "save_checkpoint = make_save_checkpoint()\n",
    "\n",
    "# instantiate the training step function \n",
    "train_step = make_train_step(model, criterion, optimizer=optimizer)\n",
    "\n",
    "# instantiate the validation loop function\n",
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "# instantiate lists to hold scalar performance metrics to plot later\n",
    "train_losses=[]\n",
    "valid_losses = []\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "valid_acc = 0\n",
    "# create training loop for one complete epoch (entire training set)\n",
    "def train(optimizer, model, num_epochs, X_train, Y_train, X_valid, Y_valid):\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # set model to train phase\n",
    "        model.train()         \n",
    "        \n",
    "        # shuffle entire training set in each epoch to randomize minibatch order\n",
    "        train_indices = np.random.permutation(train_size) \n",
    "        \n",
    "        # shuffle the training set for each epoch:\n",
    "        X_train = X_train[train_indices,:,:,:] \n",
    "        Y_train = Y_train[train_indices]\n",
    "\n",
    "        # instantiate scalar values to keep track of progress after each epoch so we can stop training when appropriate \n",
    "        epoch_acc = 0 \n",
    "        epoch_loss = 0\n",
    "        num_iterations = int(train_size / minibatch)\n",
    "        \n",
    "        # create a loop for each minibatch of 32 samples:\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            # we have to track and update minibatch position for the current minibatch\n",
    "            # if we take a random batch position from a set, we almost certainly will skip some of the data in that set\n",
    "            # track minibatch position based on iteration number:\n",
    "            batch_start = i * minibatch \n",
    "            # ensure we don't go out of the bounds of our training set:\n",
    "            batch_end = min(batch_start + minibatch, train_size) \n",
    "            # ensure we don't have an index error\n",
    "            actual_batch_size = batch_end-batch_start \n",
    "            \n",
    "            # get training minibatch with all channnels and 2D feature dims\n",
    "            X = X_train[batch_start:batch_end,:,:,:] \n",
    "            # get training minibatch labels \n",
    "            Y = Y_train[batch_start:batch_end] \n",
    "\n",
    "            # instantiate training tensors\n",
    "            X_tensor = torch.tensor(X, device=device).float() \n",
    "            Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "            \n",
    "            # Pass input tensors thru 1 training step (fwd+backwards pass)\n",
    "            loss, acc = train_step(X_tensor,Y_tensor) \n",
    "            \n",
    "            # aggregate batch accuracy to measure progress of entire epoch\n",
    "            epoch_acc += acc * actual_batch_size / train_size\n",
    "            epoch_loss += loss * actual_batch_size / train_size\n",
    "            \n",
    "            # keep track of the iteration to see if the model's too slow\n",
    "            print('\\r'+f'Epoch {epoch}: iteration {i}/{num_iterations}',end='')\n",
    "        \n",
    "        # create tensors from validation set\n",
    "        X_valid_tensor = torch.tensor(X_valid,device=device).float()\n",
    "        Y_valid_tensor = torch.tensor(Y_valid,dtype=torch.long,device=device)\n",
    "        \n",
    "        # calculate validation metrics to keep track of progress; don't need predictions now\n",
    "        valid_loss, valid_acc, _ = validate(X_valid_tensor,Y_valid_tensor)\n",
    "        \n",
    "        # accumulate scalar performance metrics at each epoch to track and plot later\n",
    "        train_losses.append(epoch_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "                  \n",
    "           \n",
    "        if valid_acc > best_val_acc:\n",
    "            best_epoch = epoch\n",
    "            best_val_acc = valid_acc\n",
    "            print(\"Best one is\",epoch,best_val_acc)\n",
    "             # Save checkpoint of the model\n",
    "            checkpoint_filename = 'checkpoint/BAM-{:03d}.pkl'.format(epoch)\n",
    "            save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
    "        \n",
    "        # keep track of each epoch's progress\n",
    "        print(f'\\nEpoch {epoch} --- loss:{epoch_loss:.3f}, Epoch accuracy:{epoch_acc:.2f}%, Validation loss:{valid_loss:.3f}, Validation accuracy:{valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213c6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp/ipykernel_7636/776434586.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, device=device).float()\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp/ipykernel_7636/776434586.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 3, 2, 2], expected input[32, 300, 40, 3] to have 3 channels, but got 300 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7636/557916903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train it!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7636/776434586.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(optimizer, model, num_epochs, X_train, Y_train, X_valid, Y_valid)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# Pass input tensors thru 1 training step (fwd+backwards pass)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# aggregate batch accuracy to measure progress of entire epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7636/3679156440.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutput_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_softmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_softmax\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7636/662978854.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;31m#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;31m#         print(x.size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLW_CNN1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLW_CNN2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 3, 2, 2], expected input[32, 300, 40, 3] to have 3 channels, but got 300 channels instead"
     ]
    }
   ],
   "source": [
    "# choose number of epochs higher than reasonable so we can manually stop training \n",
    "num_epochs = 50\n",
    "\n",
    "# train it!\n",
    "train(optimizer, model, num_epochs, train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss Curve for BAM Model')\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.plot(train_losses[:],'b')\n",
    "plt.plot(valid_losses[:],'r')\n",
    "plt.legend(['Training loss','Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627d394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# pick load folder  \n",
    "load_folder = 'checkpoint'  \n",
    "\n",
    "# pick the epoch to load\n",
    "epoch = '039'\n",
    "model_name = f'BAM-{epoch}.pkl'\n",
    "\n",
    "# make full load path\n",
    "load_path = os.path.join(load_folder, model_name)\n",
    "\n",
    "## instantiate empty model and populate with params from binary \n",
    "model = BAM()\n",
    "load_checkpoint(optimizer, model, load_path)\n",
    "\n",
    "print(f'Loaded model from {load_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "X_val = val_x\n",
    "y_val = val_y\n",
    "# Convert 4D test feature set array to tensor and move to GPU\n",
    "X_val_tensor = torch.tensor(X_val,device='cpu').float()\n",
    "# Convert 4D test label set array to tensor and move to GPU\n",
    "y_val_tensor = torch.tensor(y_val,dtype=torch.long,device='cpu')\n",
    "\n",
    "# Get the model's performance metrics using the validation function we defined\n",
    "_loss, _acc, predicted_emotions = validate(X_val_tensor,y_val_tensor)\n",
    "\n",
    "print(f'Validation accuracy is {_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ecf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "# because model tested on GPU, move prediction tensor to CPU then convert to array\n",
    "predicted_emotions = predicted_emotions\n",
    "# use labels from test set\n",
    "emotions_groundtruth = y_val\n",
    "\n",
    "# build confusion matrix and normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(emotions_groundtruth, predicted_emotions)\n",
    "conf_matrix_norm = confusion_matrix(emotions_groundtruth, predicted_emotions,normalize='true')\n",
    "\n",
    "# set labels for matrix axes from emotions\n",
    "emotion_names = ['cc', 'cd']\n",
    "\n",
    "# make a confusion matrix with labels using a DataFrame\n",
    "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_names, columns=emotion_names)\n",
    "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_names, columns=emotion_names)\n",
    "\n",
    "# plot confusion matrices\n",
    "plt.figure(figsize=(16,6))\n",
    "sn.set(font_scale=1.8) # emotion label and title size\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 18}) #annot_kws is value font\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df_norm, annot=True, annot_kws={\"size\": 13}) #annot_kws is value font\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622c039",
   "metadata": {},
   "source": [
    "ToDo: Make X_Test and Y_Test, Confusion Matrix, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'adress_test_3_channles.pkl'\n",
    "\n",
    "X_test,Y_test,speaker,segment = load_data(data_path)\n",
    "# converting training images into torch format\n",
    "X_test  = torch.from_numpy(X_test)\n",
    "\n",
    "# converting the target into torch format\n",
    "\n",
    "Y_test = Y_test.reshape(Y_test.shape[0]).astype(float);\n",
    "y_test = torch.from_numpy(Y_test)\n",
    "\n",
    "# reinitialize validation function with model from chosen checkpoint\n",
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "# Convert 4D test feature set array to tensor and move to GPU\n",
    "X_test_tensor = torch.tensor(X_test,device='cpu').float()\n",
    "# Convert 4D test label set array to tensor and move to GPU\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.long,device='cpu')\n",
    "\n",
    "# Get the model's performance metrics using the validation function we defined\n",
    "test_loss, test_acc, predicted_emotions = validate(X_test_tensor,y_test_tensor)\n",
    "\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "# because model tested on GPU, move prediction tensor to CPU then convert to array\n",
    "predicted_emotions = predicted_emotions\n",
    "# use labels from test set\n",
    "emotions_groundtruth = y_test\n",
    "\n",
    "# build confusion matrix and normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(emotions_groundtruth, predicted_emotions)\n",
    "conf_matrix_norm = confusion_matrix(emotions_groundtruth, predicted_emotions,normalize='true')\n",
    "\n",
    "# set labels for matrix axes from emotions\n",
    "emotion_names = ['cc', 'cd']\n",
    "\n",
    "# make a confusion matrix with labels using a DataFrame\n",
    "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_names, columns=emotion_names)\n",
    "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_names, columns=emotion_names)\n",
    "\n",
    "# plot confusion matrices\n",
    "plt.figure(figsize=(16,6))\n",
    "sn.set(font_scale=1.8) # emotion label and title size\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 18}) #annot_kws is value font\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df_norm, annot=True, annot_kws={\"size\": 13}) #annot_kws is value font\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_dict = {}\n",
    "for spk,seg,emo, y in zip(speaker,segment,predicted_emotions,y_test):\n",
    "    print(spk,seg,emo.cpu(), y.cpu())\n",
    "    speaker_dict[spk] = y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33236da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "\n",
    "for spk,seg,emo, y in zip(speaker,segment,predicted_emotions,y_test):\n",
    "    print(spk,seg,emo.cpu(), y.cpu())\n",
    "    output_dict[spk] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0474f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spk,seg,emo, y in zip(speaker,segment,predicted_emotions,y_test):\n",
    "    print(spk,seg,emo.cpu(), y.cpu())\n",
    "    output_dict[spk].append(emo.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0eae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1,0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for spk in output_dict:\n",
    "    summition = sum(output_dict[spk])\n",
    "    length = len(output_dict[spk])\n",
    "    ad = 0\n",
    "    if(summition/length<0.3):\n",
    "        ad = 0.0\n",
    "    else:\n",
    "        ad = 1.0\n",
    "    results_dict[spk] = ad\n",
    "#     print(summition/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "wrong_prediction = 0\n",
    "for spk in output_dict:\n",
    "   \n",
    "    res = results_dict[spk]\n",
    "    test = speaker_dict[spk]\n",
    "    print(spk,res,test)\n",
    "    if(res != test):\n",
    "        wrong_prediction += 1\n",
    "wrong_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b666d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cae38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrong_prediction/len(results_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12b4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
