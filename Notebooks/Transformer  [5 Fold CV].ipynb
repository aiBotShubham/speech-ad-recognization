{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,valid_data,valid_label = pickle.load(f)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "# data_path = 'adress_512.pkl'\n",
    "data_path = 'adress_transformer.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,valid_data,valid_label = load_data(data_path)\n",
    "\n",
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(train_y.shape[0]).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(CTX),train_y.to(CTX)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09537d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(val_y.shape[0]).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4701d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "# val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d492176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "from sklearn.metrics import recall_score as recall\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out, _softmax= self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out, _softmax = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(_softmax, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        \n",
    "        valid_conf = confusion(np.argmax(_softmax.cpu(),1),labels.cpu())\n",
    "#         valid_conf = 8008\n",
    "#         print(np.argmax(out.cpu(),1))\n",
    "#         print(labels.cpu())\n",
    "        print ('Valid Confusion Matrix:[\"cc\",\"cd\"]')\n",
    "        print(valid_conf)\n",
    "        print ('Recall')\n",
    "        print(recall(np.argmax(_softmax.cpu(),1),labels.cpu(),average='macro'))\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc }\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.8f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "#         print (result['valid_conf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change nn.sequential to take dict to make more readable \n",
    "\n",
    "class parallel_all_you_want(ImageClassificationBase):\n",
    "    # Define all layers present in the network\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__() \n",
    "        \n",
    "        ################ TRANSFORMER BLOCK #############################\n",
    "        # maxpool the input feature map/tensor to the transformer \n",
    "        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )\n",
    "        \n",
    "        # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper\n",
    "        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
    "        \n",
    "        ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock1 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock2 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        ################# FINAL LINEAR BLOCK ####################\n",
    "        # Linear softmax layer to take final concatenated embedding tensor \n",
    "        #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
    "        # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
    "        # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
    "        # 512*2+40 == 1064 input features --> 8 output emotions \n",
    "        self.fc1_linear = nn.Linear(512*2+40,num_emotions) \n",
    "        \n",
    "        ### Softmax layer for the 8 output logits from final FC linear layer \n",
    "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
    "        \n",
    "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
    "    def forward(self,x):\n",
    "        \n",
    "        ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
    "        # create final feature embedding from 1st convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
    "        \n",
    "        ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
    "        # create final feature embedding from 2nd convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
    "        \n",
    "         \n",
    "        ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
    "        # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
    "        x_maxpool = self.transformer_maxpool(x)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x = x_maxpool_reduced.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        \n",
    "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
    "        \n",
    "        ############# concatenate freq embeddings from convolutional and transformer blocks ######\n",
    "        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks\n",
    "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
    "\n",
    "        ######### final FC linear layer, need logits for loss #########################\n",
    "        output_logits = self.fc1_linear(complete_embedding)  \n",
    "        \n",
    "        ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
    "        output_softmax = self.softmax_out(output_logits)\n",
    "        \n",
    "        # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
    "        return output_logits, output_softmax  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f81fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel_all_you_want(\n",
      "  (transformer_maxpool): MaxPool2d(kernel_size=[1, 4], stride=[1, 4], padding=0, dilation=1, ceil_mode=False)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=40, out_features=40, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=40, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
      "        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=40, out_features=40, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=40, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
      "        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=40, out_features=40, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=40, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
      "        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=40, out_features=40, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=40, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
      "        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2Dblock1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "    (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "    (10): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (conv2Dblock2): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "    (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "    (10): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc1_linear): Linear(in_features=1064, out_features=2, bias=True)\n",
      "  (softmax_out): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = parallel_all_you_want(num_emotions = 2)\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ed4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "num_epochs=16\n",
    "batch_size=32\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "\n",
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(images)\n",
    "        loss = loss_fn(output,labels.type(torch.LongTensor).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        output, _ = model(images)\n",
    "        loss=loss_fn(output,labels.type(torch.LongTensor).to(device))\n",
    "        valid_loss+=loss.item()*images.size(0)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7773bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/16 AVG Training Loss:2.143 AVG Test Loss:0.662 AVG Training Acc 55.39 % AVG Test Acc 66.91 %\n",
      "Epoch:2/16 AVG Training Loss:0.988 AVG Test Loss:1.107 AVG Training Acc 57.92 % AVG Test Acc 55.88 %\n",
      "Epoch:3/16 AVG Training Loss:1.212 AVG Test Loss:0.895 AVG Training Acc 58.82 % AVG Test Acc 66.18 %\n",
      "Epoch:4/16 AVG Training Loss:0.992 AVG Test Loss:0.615 AVG Training Acc 61.97 % AVG Test Acc 67.65 %\n",
      "Epoch:5/16 AVG Training Loss:0.766 AVG Test Loss:0.601 AVG Training Acc 64.97 % AVG Test Acc 66.18 %\n",
      "Epoch:6/16 AVG Training Loss:0.768 AVG Test Loss:0.593 AVG Training Acc 65.40 % AVG Test Acc 67.65 %\n",
      "Epoch:7/16 AVG Training Loss:0.725 AVG Test Loss:0.779 AVG Training Acc 66.95 % AVG Test Acc 56.62 %\n",
      "Epoch:8/16 AVG Training Loss:0.745 AVG Test Loss:0.638 AVG Training Acc 65.90 % AVG Test Acc 69.85 %\n",
      "Epoch:9/16 AVG Training Loss:0.738 AVG Test Loss:0.584 AVG Training Acc 69.79 % AVG Test Acc 69.85 %\n",
      "Epoch:10/16 AVG Training Loss:0.719 AVG Test Loss:0.679 AVG Training Acc 69.60 % AVG Test Acc 71.32 %\n",
      "Epoch:11/16 AVG Training Loss:0.743 AVG Test Loss:0.569 AVG Training Acc 66.95 % AVG Test Acc 70.59 %\n",
      "Epoch:12/16 AVG Training Loss:0.783 AVG Test Loss:0.571 AVG Training Acc 68.24 % AVG Test Acc 67.65 %\n",
      "Epoch:13/16 AVG Training Loss:0.708 AVG Test Loss:0.574 AVG Training Acc 69.37 % AVG Test Acc 72.79 %\n",
      "Epoch:14/16 AVG Training Loss:0.764 AVG Test Loss:0.602 AVG Training Acc 69.79 % AVG Test Acc 72.79 %\n",
      "Epoch:15/16 AVG Training Loss:0.732 AVG Test Loss:0.693 AVG Training Acc 70.81 % AVG Test Acc 69.12 %\n",
      "Epoch:16/16 AVG Training Loss:0.601 AVG Test Loss:0.611 AVG Training Acc 74.19 % AVG Test Acc 72.06 %\n",
      "Fold 2\n",
      "Epoch:1/16 AVG Training Loss:2.157 AVG Test Loss:0.787 AVG Training Acc 53.29 % AVG Test Acc 52.94 %\n",
      "Epoch:2/16 AVG Training Loss:1.083 AVG Test Loss:0.762 AVG Training Acc 56.13 % AVG Test Acc 47.06 %\n",
      "Epoch:3/16 AVG Training Loss:0.937 AVG Test Loss:0.812 AVG Training Acc 57.80 % AVG Test Acc 42.65 %\n",
      "Epoch:4/16 AVG Training Loss:0.949 AVG Test Loss:0.716 AVG Training Acc 59.44 % AVG Test Acc 57.35 %\n",
      "Epoch:5/16 AVG Training Loss:0.839 AVG Test Loss:0.628 AVG Training Acc 62.98 % AVG Test Acc 66.91 %\n",
      "Epoch:6/16 AVG Training Loss:0.810 AVG Test Loss:0.639 AVG Training Acc 64.27 % AVG Test Acc 63.24 %\n",
      "Epoch:7/16 AVG Training Loss:0.866 AVG Test Loss:0.648 AVG Training Acc 64.69 % AVG Test Acc 65.44 %\n",
      "Epoch:8/16 AVG Training Loss:0.978 AVG Test Loss:0.757 AVG Training Acc 64.58 % AVG Test Acc 70.59 %\n",
      "Epoch:9/16 AVG Training Loss:0.732 AVG Test Loss:0.592 AVG Training Acc 68.70 % AVG Test Acc 64.71 %\n",
      "Epoch:10/16 AVG Training Loss:0.797 AVG Test Loss:0.570 AVG Training Acc 65.63 % AVG Test Acc 68.38 %\n",
      "Epoch:11/16 AVG Training Loss:0.722 AVG Test Loss:0.679 AVG Training Acc 68.82 % AVG Test Acc 66.18 %\n",
      "Epoch:12/16 AVG Training Loss:0.799 AVG Test Loss:0.609 AVG Training Acc 66.64 % AVG Test Acc 69.85 %\n",
      "Epoch:13/16 AVG Training Loss:0.831 AVG Test Loss:0.564 AVG Training Acc 68.59 % AVG Test Acc 70.59 %\n",
      "Epoch:14/16 AVG Training Loss:0.651 AVG Test Loss:0.580 AVG Training Acc 71.20 % AVG Test Acc 68.38 %\n",
      "Epoch:15/16 AVG Training Loss:0.767 AVG Test Loss:0.943 AVG Training Acc 69.64 % AVG Test Acc 56.62 %\n",
      "Epoch:16/16 AVG Training Loss:0.964 AVG Test Loss:0.556 AVG Training Acc 64.50 % AVG Test Acc 76.47 %\n",
      "Fold 3\n",
      "Epoch:1/16 AVG Training Loss:1.931 AVG Test Loss:0.801 AVG Training Acc 52.47 % AVG Test Acc 56.62 %\n",
      "Epoch:2/16 AVG Training Loss:0.902 AVG Test Loss:0.671 AVG Training Acc 59.79 % AVG Test Acc 63.24 %\n",
      "Epoch:3/16 AVG Training Loss:0.927 AVG Test Loss:0.694 AVG Training Acc 58.51 % AVG Test Acc 61.76 %\n",
      "Epoch:4/16 AVG Training Loss:0.918 AVG Test Loss:0.768 AVG Training Acc 60.33 % AVG Test Acc 59.56 %\n",
      "Epoch:5/16 AVG Training Loss:0.962 AVG Test Loss:0.645 AVG Training Acc 60.02 % AVG Test Acc 65.44 %\n",
      "Epoch:6/16 AVG Training Loss:0.843 AVG Test Loss:1.138 AVG Training Acc 62.24 % AVG Test Acc 55.88 %\n",
      "Epoch:7/16 AVG Training Loss:0.894 AVG Test Loss:0.649 AVG Training Acc 62.24 % AVG Test Acc 65.44 %\n",
      "Epoch:8/16 AVG Training Loss:0.828 AVG Test Loss:0.701 AVG Training Acc 65.04 % AVG Test Acc 63.24 %\n",
      "Epoch:9/16 AVG Training Loss:0.734 AVG Test Loss:0.638 AVG Training Acc 67.57 % AVG Test Acc 66.91 %\n",
      "Epoch:10/16 AVG Training Loss:0.785 AVG Test Loss:0.751 AVG Training Acc 66.37 % AVG Test Acc 63.24 %\n",
      "Epoch:11/16 AVG Training Loss:0.850 AVG Test Loss:0.620 AVG Training Acc 63.53 % AVG Test Acc 69.12 %\n",
      "Epoch:12/16 AVG Training Loss:0.773 AVG Test Loss:0.651 AVG Training Acc 66.84 % AVG Test Acc 69.12 %\n",
      "Epoch:13/16 AVG Training Loss:0.670 AVG Test Loss:0.580 AVG Training Acc 71.16 % AVG Test Acc 72.79 %\n",
      "Epoch:14/16 AVG Training Loss:0.772 AVG Test Loss:0.763 AVG Training Acc 69.02 % AVG Test Acc 68.38 %\n",
      "Epoch:15/16 AVG Training Loss:0.668 AVG Test Loss:0.646 AVG Training Acc 71.43 % AVG Test Acc 72.06 %\n",
      "Epoch:16/16 AVG Training Loss:0.642 AVG Test Loss:0.584 AVG Training Acc 72.32 % AVG Test Acc 75.00 %\n",
      "Fold 4\n",
      "Epoch:1/16 AVG Training Loss:2.028 AVG Test Loss:0.899 AVG Training Acc 53.52 % AVG Test Acc 55.88 %\n",
      "Epoch:2/16 AVG Training Loss:1.032 AVG Test Loss:0.790 AVG Training Acc 57.45 % AVG Test Acc 58.82 %\n",
      "Epoch:3/16 AVG Training Loss:1.114 AVG Test Loss:0.669 AVG Training Acc 60.37 % AVG Test Acc 63.24 %\n",
      "Epoch:4/16 AVG Training Loss:0.855 AVG Test Loss:0.765 AVG Training Acc 63.60 % AVG Test Acc 66.18 %\n",
      "Epoch:5/16 AVG Training Loss:1.298 AVG Test Loss:0.610 AVG Training Acc 59.09 % AVG Test Acc 64.71 %\n",
      "Epoch:6/16 AVG Training Loss:0.800 AVG Test Loss:0.632 AVG Training Acc 64.81 % AVG Test Acc 66.91 %\n",
      "Epoch:7/16 AVG Training Loss:0.782 AVG Test Loss:0.599 AVG Training Acc 66.84 % AVG Test Acc 64.71 %\n",
      "Epoch:8/16 AVG Training Loss:0.771 AVG Test Loss:0.728 AVG Training Acc 66.56 % AVG Test Acc 64.71 %\n",
      "Epoch:9/16 AVG Training Loss:0.769 AVG Test Loss:0.633 AVG Training Acc 66.80 % AVG Test Acc 66.91 %\n",
      "Epoch:10/16 AVG Training Loss:0.800 AVG Test Loss:0.621 AVG Training Acc 68.78 % AVG Test Acc 66.91 %\n",
      "Epoch:11/16 AVG Training Loss:0.721 AVG Test Loss:0.696 AVG Training Acc 70.84 % AVG Test Acc 66.91 %\n",
      "Epoch:12/16 AVG Training Loss:0.758 AVG Test Loss:0.566 AVG Training Acc 68.47 % AVG Test Acc 72.06 %\n",
      "Epoch:13/16 AVG Training Loss:0.642 AVG Test Loss:0.687 AVG Training Acc 71.35 % AVG Test Acc 70.59 %\n",
      "Epoch:14/16 AVG Training Loss:0.683 AVG Test Loss:0.882 AVG Training Acc 72.75 % AVG Test Acc 67.65 %\n",
      "Epoch:15/16 AVG Training Loss:0.843 AVG Test Loss:0.576 AVG Training Acc 67.73 % AVG Test Acc 72.06 %\n",
      "Epoch:16/16 AVG Training Loss:0.612 AVG Test Loss:0.620 AVG Training Acc 74.00 % AVG Test Acc 73.53 %\n",
      "Fold 5\n",
      "Epoch:1/16 AVG Training Loss:1.343 AVG Test Loss:0.688 AVG Training Acc 54.38 % AVG Test Acc 55.88 %\n",
      "Epoch:2/16 AVG Training Loss:0.991 AVG Test Loss:0.728 AVG Training Acc 58.74 % AVG Test Acc 63.24 %\n",
      "Epoch:3/16 AVG Training Loss:1.130 AVG Test Loss:0.646 AVG Training Acc 58.51 % AVG Test Acc 66.91 %\n",
      "Epoch:4/16 AVG Training Loss:0.866 AVG Test Loss:0.788 AVG Training Acc 62.55 % AVG Test Acc 57.35 %\n",
      "Epoch:5/16 AVG Training Loss:0.882 AVG Test Loss:0.788 AVG Training Acc 58.74 % AVG Test Acc 53.68 %\n",
      "Epoch:6/16 AVG Training Loss:1.041 AVG Test Loss:0.867 AVG Training Acc 62.16 % AVG Test Acc 64.71 %\n",
      "Epoch:7/16 AVG Training Loss:0.930 AVG Test Loss:0.595 AVG Training Acc 62.90 % AVG Test Acc 66.91 %\n",
      "Epoch:8/16 AVG Training Loss:0.903 AVG Test Loss:1.136 AVG Training Acc 64.73 % AVG Test Acc 44.12 %\n",
      "Epoch:9/16 AVG Training Loss:0.985 AVG Test Loss:0.637 AVG Training Acc 62.44 % AVG Test Acc 71.32 %\n",
      "Epoch:10/16 AVG Training Loss:0.796 AVG Test Loss:0.647 AVG Training Acc 67.38 % AVG Test Acc 63.24 %\n",
      "Epoch:11/16 AVG Training Loss:0.795 AVG Test Loss:0.548 AVG Training Acc 66.95 % AVG Test Acc 72.79 %\n",
      "Epoch:12/16 AVG Training Loss:0.688 AVG Test Loss:0.581 AVG Training Acc 69.72 % AVG Test Acc 74.26 %\n",
      "Epoch:13/16 AVG Training Loss:0.750 AVG Test Loss:0.775 AVG Training Acc 70.38 % AVG Test Acc 63.24 %\n",
      "Epoch:14/16 AVG Training Loss:0.726 AVG Test Loss:0.752 AVG Training Acc 71.16 % AVG Test Acc 64.71 %\n",
      "Epoch:15/16 AVG Training Loss:0.809 AVG Test Loss:0.550 AVG Training Acc 69.95 % AVG Test Acc 75.74 %\n",
      "Epoch:16/16 AVG Training Loss:0.776 AVG Test Loss:0.536 AVG Training Acc 70.46 % AVG Test Acc 75.00 %\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_x) + len(val_x)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = train_dl\n",
    "    test_loader = val_dl\n",
    "#     train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "#     val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     model = ConvNet()\n",
    "    model = parallel_all_you_want(num_emotions=2).to(device) \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "        test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             test_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             test_acc))\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "    foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "torch.save(model,'k_cross_CNN.pt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f6535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2705"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x) + len(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4739b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 5 fold cross validation\n",
      "Average Training Loss: 0.900 \t Average Test Loss: 0.689 \t Average Training Acc: 64.95 \t Average Test Acc: 65.41\n"
     ]
    }
   ],
   "source": [
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=5\n",
    "for f in range(1,k+1):\n",
    "\n",
    "     tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "     testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "\n",
    "     ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "     testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ceae23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
