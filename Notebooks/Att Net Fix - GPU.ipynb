{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d,  Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid\n",
    "\n",
    "data_path = 'adress.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = load_data(data_path)\n",
    "\n",
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(2379).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(CTX),train_y.to(CTX)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09537d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(297).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4701d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d492176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "#         print(x.shape())\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    \n",
    "class Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(128 * 20 * 1, 256),\n",
    "            Linear(256, 64),\n",
    "            Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.attention = CBAM(gate_channels=128)\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f81fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(300, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=2560, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention): CBAM(\n",
      "    (ChannelGate): ChannelGate(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten()\n",
      "        (1): Linear(in_features=128, out_features=8, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=8, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (SpatialGate): SpatialGate(\n",
      "      (compress): ChannelPool()\n",
      "      (spatial): BasicConv(\n",
      "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477fd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 256, 40, 3]         691,456\n",
      "       BatchNorm2d-2           [-1, 256, 40, 3]             512\n",
      "              ReLU-3           [-1, 256, 40, 3]               0\n",
      "         MaxPool2d-4           [-1, 256, 20, 1]               0\n",
      "            Conv2d-5           [-1, 128, 20, 1]         295,040\n",
      "       BatchNorm2d-6           [-1, 128, 20, 1]             256\n",
      "              ReLU-7           [-1, 128, 20, 1]               0\n",
      "         MaxPool2d-8           [-1, 128, 20, 1]               0\n",
      "           Flatten-9                  [-1, 128]               0\n",
      "           Linear-10                    [-1, 8]           1,032\n",
      "             ReLU-11                    [-1, 8]               0\n",
      "           Linear-12                  [-1, 128]           1,152\n",
      "          Flatten-13                  [-1, 128]               0\n",
      "           Linear-14                    [-1, 8]           1,032\n",
      "             ReLU-15                    [-1, 8]               0\n",
      "           Linear-16                  [-1, 128]           1,152\n",
      "      ChannelGate-17           [-1, 128, 20, 1]               0\n",
      "      ChannelPool-18             [-1, 2, 20, 1]               0\n",
      "           Conv2d-19             [-1, 1, 20, 1]              98\n",
      "      BatchNorm2d-20             [-1, 1, 20, 1]               2\n",
      "        BasicConv-21             [-1, 1, 20, 1]               0\n",
      "      SpatialGate-22           [-1, 128, 20, 1]               0\n",
      "             CBAM-23           [-1, 128, 20, 1]               0\n",
      "           Linear-24                  [-1, 256]         655,616\n",
      "           Linear-25                   [-1, 64]          16,448\n",
      "           Linear-26                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 1,663,926\n",
      "Trainable params: 1,663,926\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.89\n",
      "Params size (MB): 6.35\n",
      "Estimated Total Size (MB): 7.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (300, 40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a72cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6944, val_loss: 0.7034, val_acc: 0.4950\n",
      "Epoch [1], train_loss: 0.6811, val_loss: 0.7207, val_acc: 0.5421\n",
      "Epoch [2], train_loss: 0.6864, val_loss: 0.6944, val_acc: 0.5295\n",
      "Epoch [3], train_loss: 0.6829, val_loss: 0.6823, val_acc: 0.5392\n",
      "Epoch [4], train_loss: 0.6777, val_loss: 0.6992, val_acc: 0.5265\n",
      "Epoch [5], train_loss: 0.6782, val_loss: 0.6779, val_acc: 0.5727\n",
      "Epoch [6], train_loss: 0.6732, val_loss: 0.7247, val_acc: 0.5295\n",
      "Epoch [7], train_loss: 0.6794, val_loss: 0.6878, val_acc: 0.5344\n",
      "Epoch [8], train_loss: 0.6747, val_loss: 0.6854, val_acc: 0.5487\n",
      "Epoch [9], train_loss: 0.6682, val_loss: 0.6883, val_acc: 0.5864\n",
      "Epoch [10], train_loss: 0.6599, val_loss: 0.6651, val_acc: 0.5715\n",
      "Epoch [11], train_loss: 0.6613, val_loss: 0.6824, val_acc: 0.5730\n",
      "Epoch [12], train_loss: 0.6599, val_loss: 0.7119, val_acc: 0.5187\n",
      "Epoch [13], train_loss: 0.6568, val_loss: 0.6537, val_acc: 0.5887\n",
      "Epoch [14], train_loss: 0.6418, val_loss: 0.6602, val_acc: 0.5903\n",
      "Epoch [15], train_loss: 0.6398, val_loss: 0.6903, val_acc: 0.5506\n",
      "Epoch [16], train_loss: 0.6379, val_loss: 0.6604, val_acc: 0.5744\n",
      "Epoch [17], train_loss: 0.6229, val_loss: 0.6948, val_acc: 0.5239\n",
      "Epoch [18], train_loss: 0.6228, val_loss: 0.7180, val_acc: 0.5614\n",
      "Epoch [19], train_loss: 0.6288, val_loss: 0.6664, val_acc: 0.5643\n",
      "Epoch [20], train_loss: 0.6164, val_loss: 0.6756, val_acc: 0.5704\n",
      "Epoch [21], train_loss: 0.6107, val_loss: 0.6607, val_acc: 0.5773\n",
      "Epoch [22], train_loss: 0.6027, val_loss: 0.6605, val_acc: 0.5965\n",
      "Epoch [23], train_loss: 0.5902, val_loss: 0.6745, val_acc: 0.6069\n",
      "Epoch [24], train_loss: 0.5803, val_loss: 0.6886, val_acc: 0.6167\n",
      "Epoch [25], train_loss: 0.5677, val_loss: 0.6925, val_acc: 0.6320\n",
      "Epoch [26], train_loss: 0.5605, val_loss: 0.7107, val_acc: 0.5760\n",
      "Epoch [27], train_loss: 0.5530, val_loss: 0.7099, val_acc: 0.6092\n",
      "Epoch [28], train_loss: 0.5396, val_loss: 0.7162, val_acc: 0.5405\n",
      "Epoch [29], train_loss: 0.5466, val_loss: 0.7594, val_acc: 0.5809\n",
      "Epoch [30], train_loss: 0.5253, val_loss: 0.7778, val_acc: 0.5467\n",
      "Epoch [31], train_loss: 0.5016, val_loss: 0.7277, val_acc: 0.6307\n",
      "Epoch [32], train_loss: 0.4940, val_loss: 0.7457, val_acc: 0.6235\n",
      "Epoch [33], train_loss: 0.4983, val_loss: 0.7783, val_acc: 0.5445\n",
      "Epoch [34], train_loss: 0.4792, val_loss: 0.7590, val_acc: 0.5981\n",
      "Epoch [35], train_loss: 0.4661, val_loss: 0.7694, val_acc: 0.5877\n",
      "Epoch [36], train_loss: 0.4544, val_loss: 0.7214, val_acc: 0.5910\n",
      "Epoch [37], train_loss: 0.4390, val_loss: 1.1073, val_acc: 0.4839\n",
      "Epoch [38], train_loss: 0.4210, val_loss: 0.9433, val_acc: 0.5776\n",
      "Epoch [39], train_loss: 0.4232, val_loss: 1.0229, val_acc: 0.5926\n",
      "Epoch [40], train_loss: 0.3949, val_loss: 0.9692, val_acc: 0.5848\n",
      "Epoch [41], train_loss: 0.3817, val_loss: 1.0508, val_acc: 0.5565\n",
      "Epoch [42], train_loss: 0.3705, val_loss: 0.9672, val_acc: 0.5646\n",
      "Epoch [43], train_loss: 0.3361, val_loss: 1.1417, val_acc: 0.5659\n",
      "Epoch [44], train_loss: 0.3080, val_loss: 1.0487, val_acc: 0.6092\n",
      "Epoch [45], train_loss: 0.3011, val_loss: 1.3119, val_acc: 0.5126\n",
      "Epoch [46], train_loss: 0.2894, val_loss: 1.4617, val_acc: 0.5633\n",
      "Epoch [47], train_loss: 0.2738, val_loss: 1.4034, val_acc: 0.5431\n",
      "Epoch [48], train_loss: 0.2692, val_loss: 1.2033, val_acc: 0.5578\n",
      "Epoch [49], train_loss: 0.2720, val_loss: 1.1732, val_acc: 0.5555\n",
      "Epoch [50], train_loss: 0.2455, val_loss: 1.8488, val_acc: 0.5181\n",
      "Epoch [51], train_loss: 0.2402, val_loss: 1.4673, val_acc: 0.5799\n",
      "Epoch [52], train_loss: 0.2381, val_loss: 1.4589, val_acc: 0.5334\n",
      "Epoch [53], train_loss: 0.2189, val_loss: 1.3951, val_acc: 0.5575\n",
      "Epoch [54], train_loss: 0.1802, val_loss: 2.0236, val_acc: 0.5393\n",
      "Epoch [55], train_loss: 0.2256, val_loss: 1.5919, val_acc: 0.5662\n",
      "Epoch [56], train_loss: 0.2044, val_loss: 1.8402, val_acc: 0.5575\n",
      "Epoch [57], train_loss: 0.1882, val_loss: 1.4306, val_acc: 0.5711\n",
      "Epoch [58], train_loss: 0.2035, val_loss: 1.4591, val_acc: 0.5591\n",
      "Epoch [59], train_loss: 0.1328, val_loss: 1.6107, val_acc: 0.5848\n",
      "Epoch [60], train_loss: 0.1172, val_loss: 1.7057, val_acc: 0.5816\n",
      "Epoch [61], train_loss: 0.1060, val_loss: 1.8986, val_acc: 0.5630\n",
      "Epoch [62], train_loss: 0.0940, val_loss: 2.4520, val_acc: 0.5767\n",
      "Epoch [63], train_loss: 0.0833, val_loss: 2.1069, val_acc: 0.6248\n",
      "Epoch [64], train_loss: 0.0950, val_loss: 2.2502, val_acc: 0.5604\n",
      "Epoch [65], train_loss: 0.0563, val_loss: 3.1564, val_acc: 0.5506\n",
      "Epoch [66], train_loss: 0.0796, val_loss: 4.0256, val_acc: 0.5471\n",
      "Epoch [67], train_loss: 0.1908, val_loss: 1.8972, val_acc: 0.5607\n",
      "Epoch [68], train_loss: 0.0939, val_loss: 1.9409, val_acc: 0.5138\n",
      "Epoch [69], train_loss: 0.0378, val_loss: 2.8422, val_acc: 0.5441\n",
      "Epoch [70], train_loss: 0.0280, val_loss: 3.0230, val_acc: 0.5399\n",
      "Epoch [71], train_loss: 0.0775, val_loss: 2.3693, val_acc: 0.5689\n",
      "Epoch [72], train_loss: 0.0424, val_loss: 2.5895, val_acc: 0.5929\n",
      "Epoch [73], train_loss: 0.0204, val_loss: 3.8357, val_acc: 0.5692\n",
      "Epoch [74], train_loss: 0.0272, val_loss: 4.0126, val_acc: 0.5363\n",
      "Epoch [75], train_loss: 0.0255, val_loss: 3.6701, val_acc: 0.5685\n",
      "Epoch [76], train_loss: 0.0188, val_loss: 4.0980, val_acc: 0.5692\n",
      "Epoch [77], train_loss: 0.0642, val_loss: 2.6812, val_acc: 0.5718\n",
      "Epoch [78], train_loss: 0.0497, val_loss: 3.0576, val_acc: 0.5591\n",
      "Epoch [79], train_loss: 0.1051, val_loss: 1.9999, val_acc: 0.5584\n",
      "Epoch [80], train_loss: 0.0385, val_loss: 3.0243, val_acc: 0.5584\n",
      "Epoch [81], train_loss: 0.0214, val_loss: 3.3949, val_acc: 0.5419\n",
      "Epoch [82], train_loss: 0.0252, val_loss: 3.5013, val_acc: 0.5494\n",
      "Epoch [83], train_loss: 0.0573, val_loss: 2.8247, val_acc: 0.5802\n",
      "Epoch [84], train_loss: 0.0485, val_loss: 2.8240, val_acc: 0.5523\n",
      "Epoch [85], train_loss: 0.0163, val_loss: 3.3857, val_acc: 0.5711\n",
      "Epoch [86], train_loss: 0.0022, val_loss: 3.8176, val_acc: 0.5708\n",
      "Epoch [87], train_loss: 0.0035, val_loss: 4.1235, val_acc: 0.5604\n",
      "Epoch [88], train_loss: 0.0231, val_loss: 3.7182, val_acc: 0.5692\n",
      "Epoch [89], train_loss: 0.0262, val_loss: 4.6133, val_acc: 0.5529\n",
      "Epoch [90], train_loss: 0.0604, val_loss: 2.9837, val_acc: 0.5399\n",
      "Epoch [91], train_loss: 0.0337, val_loss: 2.9969, val_acc: 0.5763\n",
      "Epoch [92], train_loss: 0.0304, val_loss: 2.9684, val_acc: 0.5744\n",
      "Epoch [93], train_loss: 0.0114, val_loss: 3.6004, val_acc: 0.5662\n",
      "Epoch [94], train_loss: 0.0359, val_loss: 2.6438, val_acc: 0.6127\n",
      "Epoch [95], train_loss: 0.0254, val_loss: 3.3495, val_acc: 0.5643\n",
      "Epoch [96], train_loss: 0.0129, val_loss: 4.2719, val_acc: 0.5666\n",
      "Epoch [97], train_loss: 0.0388, val_loss: 3.3978, val_acc: 0.5721\n",
      "Epoch [98], train_loss: 0.0566, val_loss: 3.1097, val_acc: 0.5191\n",
      "Epoch [99], train_loss: 0.0222, val_loss: 3.1220, val_acc: 0.5614\n",
      "Epoch [100], train_loss: 0.0180, val_loss: 4.0463, val_acc: 0.5477\n",
      "Epoch [101], train_loss: 0.0048, val_loss: 4.2632, val_acc: 0.5900\n",
      "Epoch [102], train_loss: 0.0057, val_loss: 4.6913, val_acc: 0.5581\n",
      "Epoch [103], train_loss: 0.0124, val_loss: 4.2867, val_acc: 0.5857\n",
      "Epoch [104], train_loss: 0.1434, val_loss: 1.9966, val_acc: 0.5584\n",
      "Epoch [105], train_loss: 0.0225, val_loss: 3.8388, val_acc: 0.5474\n",
      "Epoch [106], train_loss: 0.0156, val_loss: 3.3639, val_acc: 0.5575\n",
      "Epoch [107], train_loss: 0.0208, val_loss: 3.0460, val_acc: 0.5737\n",
      "Epoch [108], train_loss: 0.0121, val_loss: 4.1611, val_acc: 0.5659\n",
      "Epoch [109], train_loss: 0.0110, val_loss: 3.5305, val_acc: 0.5480\n",
      "Epoch [110], train_loss: 0.0287, val_loss: 3.4591, val_acc: 0.5474\n",
      "Epoch [111], train_loss: 0.1198, val_loss: 2.8616, val_acc: 0.5497\n",
      "Epoch [112], train_loss: 0.0330, val_loss: 2.8962, val_acc: 0.5689\n",
      "Epoch [113], train_loss: 0.0105, val_loss: 4.1983, val_acc: 0.5636\n",
      "Epoch [114], train_loss: 0.0388, val_loss: 4.1410, val_acc: 0.5425\n",
      "Epoch [115], train_loss: 0.0301, val_loss: 3.0545, val_acc: 0.5796\n",
      "Epoch [116], train_loss: 0.0374, val_loss: 4.3347, val_acc: 0.5503\n",
      "Epoch [117], train_loss: 0.0418, val_loss: 2.6687, val_acc: 0.5767\n",
      "Epoch [118], train_loss: 0.0233, val_loss: 4.4341, val_acc: 0.5454\n",
      "Epoch [119], train_loss: 0.0457, val_loss: 2.7272, val_acc: 0.5747\n",
      "Epoch [120], train_loss: 0.0092, val_loss: 3.6970, val_acc: 0.5314\n",
      "Epoch [121], train_loss: 0.0032, val_loss: 4.4182, val_acc: 0.5692\n",
      "Epoch [122], train_loss: 0.0081, val_loss: 4.2875, val_acc: 0.5503\n",
      "Epoch [123], train_loss: 0.0390, val_loss: 2.4068, val_acc: 0.5685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 0.0284, val_loss: 3.0330, val_acc: 0.5344\n",
      "Epoch [125], train_loss: 0.0262, val_loss: 3.0936, val_acc: 0.5614\n",
      "Epoch [126], train_loss: 0.0088, val_loss: 3.5573, val_acc: 0.5633\n",
      "Epoch [127], train_loss: 0.0062, val_loss: 4.4686, val_acc: 0.5796\n",
      "Epoch [128], train_loss: 0.0183, val_loss: 4.6899, val_acc: 0.5399\n",
      "Epoch [129], train_loss: 0.1027, val_loss: 1.6899, val_acc: 0.5640\n",
      "Epoch [130], train_loss: 0.0556, val_loss: 2.6822, val_acc: 0.5561\n",
      "Epoch [131], train_loss: 0.0132, val_loss: 3.3560, val_acc: 0.5640\n",
      "Epoch [132], train_loss: 0.0029, val_loss: 4.1498, val_acc: 0.5555\n",
      "Epoch [133], train_loss: 0.0081, val_loss: 4.1225, val_acc: 0.5370\n",
      "Epoch [134], train_loss: 0.0108, val_loss: 3.7394, val_acc: 0.5662\n",
      "Epoch [135], train_loss: 0.0095, val_loss: 3.4981, val_acc: 0.5477\n",
      "Epoch [136], train_loss: 0.0024, val_loss: 4.0888, val_acc: 0.5477\n",
      "Epoch [137], train_loss: 0.0019, val_loss: 4.6648, val_acc: 0.5363\n",
      "Epoch [138], train_loss: 0.0006, val_loss: 4.8629, val_acc: 0.5555\n",
      "Epoch [139], train_loss: 0.0003, val_loss: 5.0281, val_acc: 0.5636\n",
      "Epoch [140], train_loss: 0.0008, val_loss: 5.0507, val_acc: 0.5581\n",
      "Epoch [141], train_loss: 0.0009, val_loss: 4.7280, val_acc: 0.5532\n",
      "Epoch [142], train_loss: 0.0017, val_loss: 4.9433, val_acc: 0.5529\n",
      "Epoch [143], train_loss: 0.0188, val_loss: 6.4729, val_acc: 0.5685\n",
      "Epoch [144], train_loss: 0.0648, val_loss: 2.9364, val_acc: 0.5828\n",
      "Epoch [145], train_loss: 0.0147, val_loss: 3.4949, val_acc: 0.5744\n",
      "Epoch [146], train_loss: 0.0043, val_loss: 3.8652, val_acc: 0.5311\n",
      "Epoch [147], train_loss: 0.0022, val_loss: 4.2660, val_acc: 0.5737\n",
      "Epoch [148], train_loss: 0.0098, val_loss: 3.9985, val_acc: 0.5552\n",
      "Epoch [149], train_loss: 0.0066, val_loss: 4.0527, val_acc: 0.5763\n",
      "Epoch [150], train_loss: 0.0151, val_loss: 3.8535, val_acc: 0.5415\n",
      "Epoch [151], train_loss: 0.0057, val_loss: 4.0715, val_acc: 0.5767\n",
      "Epoch [152], train_loss: 0.0017, val_loss: 4.5155, val_acc: 0.5393\n",
      "Epoch [153], train_loss: 0.0019, val_loss: 5.5456, val_acc: 0.5578\n",
      "Epoch [154], train_loss: 0.0052, val_loss: 4.2758, val_acc: 0.5474\n",
      "Epoch [155], train_loss: 0.0360, val_loss: 3.1941, val_acc: 0.5633\n",
      "Epoch [156], train_loss: 0.0135, val_loss: 3.2987, val_acc: 0.5718\n",
      "Epoch [157], train_loss: 0.0047, val_loss: 4.5523, val_acc: 0.5796\n",
      "Epoch [158], train_loss: 0.0248, val_loss: 5.1257, val_acc: 0.5386\n",
      "Epoch [159], train_loss: 0.0505, val_loss: 3.1411, val_acc: 0.5900\n",
      "Epoch [160], train_loss: 0.0413, val_loss: 2.8116, val_acc: 0.5311\n",
      "Epoch [161], train_loss: 0.0687, val_loss: 2.8264, val_acc: 0.5718\n",
      "Epoch [162], train_loss: 0.0223, val_loss: 3.1410, val_acc: 0.5689\n",
      "Epoch [163], train_loss: 0.0085, val_loss: 3.7062, val_acc: 0.5659\n",
      "Epoch [164], train_loss: 0.0029, val_loss: 4.1551, val_acc: 0.5610\n",
      "Epoch [165], train_loss: 0.0005, val_loss: 4.7425, val_acc: 0.5871\n",
      "Epoch [166], train_loss: 0.0002, val_loss: 4.7015, val_acc: 0.5711\n",
      "Epoch [167], train_loss: 0.0001, val_loss: 4.7883, val_acc: 0.5737\n",
      "Epoch [168], train_loss: 0.0013, val_loss: 4.8601, val_acc: 0.5659\n",
      "Epoch [169], train_loss: 0.0050, val_loss: 5.0945, val_acc: 0.5822\n",
      "Epoch [170], train_loss: 0.0123, val_loss: 5.0889, val_acc: 0.5604\n",
      "Epoch [171], train_loss: 0.0108, val_loss: 4.3091, val_acc: 0.5689\n",
      "Epoch [172], train_loss: 0.0188, val_loss: 3.7701, val_acc: 0.5581\n",
      "Epoch [173], train_loss: 0.0146, val_loss: 4.4525, val_acc: 0.5737\n",
      "Epoch [174], train_loss: 0.0165, val_loss: 4.5620, val_acc: 0.5630\n",
      "Epoch [175], train_loss: 0.0068, val_loss: 3.9102, val_acc: 0.5500\n",
      "Epoch [176], train_loss: 0.0021, val_loss: 4.6409, val_acc: 0.5549\n",
      "Epoch [177], train_loss: 0.0068, val_loss: 4.7249, val_acc: 0.5604\n",
      "Epoch [178], train_loss: 0.0079, val_loss: 4.4014, val_acc: 0.5627\n",
      "Epoch [179], train_loss: 0.0051, val_loss: 4.6219, val_acc: 0.6007\n",
      "Epoch [180], train_loss: 0.0499, val_loss: 3.1231, val_acc: 0.5988\n",
      "Epoch [181], train_loss: 0.0265, val_loss: 3.4329, val_acc: 0.5854\n",
      "Epoch [182], train_loss: 0.0076, val_loss: 4.7757, val_acc: 0.5689\n",
      "Epoch [183], train_loss: 0.0013, val_loss: 5.1976, val_acc: 0.5874\n",
      "Epoch [184], train_loss: 0.0052, val_loss: 4.7806, val_acc: 0.5744\n",
      "Epoch [185], train_loss: 0.0207, val_loss: 6.1416, val_acc: 0.5861\n",
      "Epoch [186], train_loss: 0.0740, val_loss: 3.6358, val_acc: 0.5903\n",
      "Epoch [187], train_loss: 0.0314, val_loss: 3.1381, val_acc: 0.5575\n",
      "Epoch [188], train_loss: 0.0245, val_loss: 3.2130, val_acc: 0.5578\n",
      "Epoch [189], train_loss: 0.0399, val_loss: 2.5901, val_acc: 0.5894\n",
      "Epoch [190], train_loss: 0.0239, val_loss: 3.2902, val_acc: 0.5897\n",
      "Epoch [191], train_loss: 0.0475, val_loss: 2.4612, val_acc: 0.5662\n",
      "Epoch [192], train_loss: 0.0193, val_loss: 3.4194, val_acc: 0.5630\n",
      "Epoch [193], train_loss: 0.0132, val_loss: 3.9430, val_acc: 0.6193\n",
      "Epoch [194], train_loss: 0.0172, val_loss: 3.9731, val_acc: 0.5767\n",
      "Epoch [195], train_loss: 0.0118, val_loss: 4.3391, val_acc: 0.5903\n",
      "Epoch [196], train_loss: 0.0155, val_loss: 3.4771, val_acc: 0.6063\n",
      "Epoch [197], train_loss: 0.0238, val_loss: 3.1444, val_acc: 0.5767\n",
      "Epoch [198], train_loss: 0.0283, val_loss: 3.2237, val_acc: 0.6085\n",
      "Epoch [199], train_loss: 0.0098, val_loss: 3.6188, val_acc: 0.5763\n",
      "Epoch [200], train_loss: 0.0094, val_loss: 3.8078, val_acc: 0.5871\n",
      "Epoch [201], train_loss: 0.0158, val_loss: 3.6465, val_acc: 0.5708\n",
      "Epoch [202], train_loss: 0.0057, val_loss: 3.7935, val_acc: 0.5656\n",
      "Epoch [203], train_loss: 0.0034, val_loss: 4.5774, val_acc: 0.5656\n",
      "Epoch [204], train_loss: 0.0123, val_loss: 5.4650, val_acc: 0.5393\n",
      "Epoch [205], train_loss: 0.0254, val_loss: 3.6184, val_acc: 0.5445\n",
      "Epoch [206], train_loss: 0.0469, val_loss: 4.2856, val_acc: 0.5578\n",
      "Epoch [207], train_loss: 0.0669, val_loss: 1.9896, val_acc: 0.5822\n",
      "Epoch [208], train_loss: 0.0159, val_loss: 3.2218, val_acc: 0.5877\n",
      "Epoch [209], train_loss: 0.0034, val_loss: 4.0334, val_acc: 0.5932\n",
      "Epoch [210], train_loss: 0.0078, val_loss: 4.5204, val_acc: 0.5526\n",
      "Epoch [211], train_loss: 0.0017, val_loss: 4.5877, val_acc: 0.5685\n",
      "Epoch [212], train_loss: 0.0005, val_loss: 4.9379, val_acc: 0.5685\n",
      "Epoch [213], train_loss: 0.0015, val_loss: 5.3817, val_acc: 0.5793\n",
      "Epoch [214], train_loss: 0.0103, val_loss: 4.6029, val_acc: 0.5480\n",
      "Epoch [215], train_loss: 0.0594, val_loss: 2.3461, val_acc: 0.5799\n",
      "Epoch [216], train_loss: 0.0269, val_loss: 3.5653, val_acc: 0.5682\n",
      "Epoch [217], train_loss: 0.0095, val_loss: 4.3601, val_acc: 0.5685\n",
      "Epoch [218], train_loss: 0.0118, val_loss: 4.4896, val_acc: 0.5848\n",
      "Epoch [219], train_loss: 0.0032, val_loss: 4.7823, val_acc: 0.5445\n",
      "Epoch [220], train_loss: 0.0037, val_loss: 5.0242, val_acc: 0.5393\n",
      "Epoch [221], train_loss: 0.0011, val_loss: 5.2710, val_acc: 0.5415\n",
      "Epoch [222], train_loss: 0.0004, val_loss: 5.8198, val_acc: 0.5389\n",
      "Epoch [223], train_loss: 0.0002, val_loss: 5.9356, val_acc: 0.5311\n",
      "Epoch [224], train_loss: 0.0002, val_loss: 5.4092, val_acc: 0.5419\n",
      "Epoch [225], train_loss: 0.0002, val_loss: 5.8160, val_acc: 0.5415\n",
      "Epoch [226], train_loss: 0.0002, val_loss: 5.8335, val_acc: 0.5415\n",
      "Epoch [227], train_loss: 0.0012, val_loss: 5.4722, val_acc: 0.5471\n",
      "Epoch [228], train_loss: 0.0012, val_loss: 5.7760, val_acc: 0.5689\n",
      "Epoch [229], train_loss: 0.0026, val_loss: 6.5234, val_acc: 0.5445\n",
      "Epoch [230], train_loss: 0.0121, val_loss: 4.5878, val_acc: 0.5607\n",
      "Epoch [231], train_loss: 0.0395, val_loss: 2.9497, val_acc: 0.5123\n",
      "Epoch [232], train_loss: 0.0129, val_loss: 3.6066, val_acc: 0.5311\n",
      "Epoch [233], train_loss: 0.0098, val_loss: 3.8818, val_acc: 0.5572\n",
      "Epoch [234], train_loss: 0.0054, val_loss: 4.6571, val_acc: 0.5708\n",
      "Epoch [235], train_loss: 0.0013, val_loss: 4.8251, val_acc: 0.5415\n",
      "Epoch [236], train_loss: 0.0006, val_loss: 5.3588, val_acc: 0.5552\n",
      "Epoch [237], train_loss: 0.0030, val_loss: 5.9530, val_acc: 0.5581\n",
      "Epoch [238], train_loss: 0.0531, val_loss: 2.2325, val_acc: 0.5913\n",
      "Epoch [239], train_loss: 0.0511, val_loss: 3.0184, val_acc: 0.5614\n",
      "Epoch [240], train_loss: 0.0172, val_loss: 3.2787, val_acc: 0.5643\n",
      "Epoch [241], train_loss: 0.0193, val_loss: 3.5897, val_acc: 0.5604\n",
      "Epoch [242], train_loss: 0.0380, val_loss: 2.8442, val_acc: 0.5848\n",
      "Epoch [243], train_loss: 0.0392, val_loss: 2.3977, val_acc: 0.5926\n",
      "Epoch [244], train_loss: 0.0363, val_loss: 4.2382, val_acc: 0.5529\n",
      "Epoch [245], train_loss: 0.0347, val_loss: 3.0395, val_acc: 0.5503\n",
      "Epoch [246], train_loss: 0.0219, val_loss: 3.6586, val_acc: 0.5685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247], train_loss: 0.0494, val_loss: 3.0806, val_acc: 0.5552\n",
      "Epoch [248], train_loss: 0.0171, val_loss: 3.4722, val_acc: 0.5659\n",
      "Epoch [249], train_loss: 0.0283, val_loss: 3.3559, val_acc: 0.5584\n",
      "Epoch [250], train_loss: 0.0083, val_loss: 4.0756, val_acc: 0.5737\n",
      "Epoch [251], train_loss: 0.0029, val_loss: 4.2670, val_acc: 0.5584\n",
      "Epoch [252], train_loss: 0.0028, val_loss: 5.5073, val_acc: 0.5474\n",
      "Epoch [253], train_loss: 0.0048, val_loss: 5.4105, val_acc: 0.5578\n",
      "Epoch [254], train_loss: 0.0174, val_loss: 4.9483, val_acc: 0.5555\n",
      "Epoch [255], train_loss: 0.0647, val_loss: 3.1113, val_acc: 0.5610\n",
      "Epoch [256], train_loss: 0.0073, val_loss: 4.2376, val_acc: 0.5347\n",
      "Epoch [257], train_loss: 0.0125, val_loss: 3.7730, val_acc: 0.5419\n",
      "Epoch [258], train_loss: 0.0027, val_loss: 3.9793, val_acc: 0.5366\n",
      "Epoch [259], train_loss: 0.0004, val_loss: 4.2098, val_acc: 0.5363\n",
      "Epoch [260], train_loss: 0.0002, val_loss: 4.3379, val_acc: 0.5633\n",
      "Epoch [261], train_loss: 0.0002, val_loss: 4.4821, val_acc: 0.5715\n",
      "Epoch [262], train_loss: 0.0001, val_loss: 4.5040, val_acc: 0.5555\n",
      "Epoch [263], train_loss: 0.0018, val_loss: 4.6359, val_acc: 0.5340\n",
      "Epoch [264], train_loss: 0.0193, val_loss: 3.5195, val_acc: 0.5422\n",
      "Epoch [265], train_loss: 0.0204, val_loss: 3.7958, val_acc: 0.5607\n",
      "Epoch [266], train_loss: 0.0081, val_loss: 4.0195, val_acc: 0.5715\n",
      "Epoch [267], train_loss: 0.0055, val_loss: 4.2241, val_acc: 0.5819\n",
      "Epoch [268], train_loss: 0.0042, val_loss: 4.9959, val_acc: 0.5532\n",
      "Epoch [269], train_loss: 0.0047, val_loss: 4.4109, val_acc: 0.5666\n",
      "Epoch [270], train_loss: 0.0042, val_loss: 4.7915, val_acc: 0.5763\n",
      "Epoch [271], train_loss: 0.0040, val_loss: 4.8845, val_acc: 0.5581\n",
      "Epoch [272], train_loss: 0.0003, val_loss: 5.2129, val_acc: 0.5474\n",
      "Epoch [273], train_loss: 0.0002, val_loss: 5.5042, val_acc: 0.5581\n",
      "Epoch [274], train_loss: 0.0001, val_loss: 5.2816, val_acc: 0.5474\n",
      "Epoch [275], train_loss: 0.0001, val_loss: 5.4170, val_acc: 0.5448\n",
      "Epoch [276], train_loss: 0.0000, val_loss: 5.1843, val_acc: 0.5659\n",
      "Epoch [277], train_loss: 0.0002, val_loss: 5.5572, val_acc: 0.5529\n",
      "Epoch [278], train_loss: 0.0000, val_loss: 5.3628, val_acc: 0.5610\n",
      "Epoch [279], train_loss: 0.0001, val_loss: 5.4258, val_acc: 0.5662\n",
      "Epoch [280], train_loss: 0.0000, val_loss: 5.3768, val_acc: 0.5581\n",
      "Epoch [281], train_loss: 0.0000, val_loss: 5.5415, val_acc: 0.5636\n",
      "Epoch [282], train_loss: 0.0000, val_loss: 5.5529, val_acc: 0.5610\n",
      "Epoch [283], train_loss: 0.0001, val_loss: 5.4455, val_acc: 0.5503\n",
      "Epoch [284], train_loss: 0.0000, val_loss: 5.6453, val_acc: 0.5558\n",
      "Epoch [285], train_loss: 0.0000, val_loss: 5.7361, val_acc: 0.5584\n",
      "Epoch [286], train_loss: 0.0000, val_loss: 5.7753, val_acc: 0.5532\n",
      "Epoch [287], train_loss: 0.0000, val_loss: 5.6392, val_acc: 0.5532\n",
      "Epoch [288], train_loss: 0.0000, val_loss: 5.7462, val_acc: 0.5558\n",
      "Epoch [289], train_loss: 0.0000, val_loss: 5.7403, val_acc: 0.5610\n",
      "Epoch [290], train_loss: 0.0000, val_loss: 5.6710, val_acc: 0.5741\n",
      "Epoch [291], train_loss: 0.0000, val_loss: 5.6917, val_acc: 0.5555\n",
      "Epoch [292], train_loss: 0.0028, val_loss: 6.3990, val_acc: 0.5441\n",
      "Epoch [293], train_loss: 0.0268, val_loss: 3.1203, val_acc: 0.5718\n",
      "Epoch [294], train_loss: 0.0298, val_loss: 3.5304, val_acc: 0.5767\n",
      "Epoch [295], train_loss: 0.0688, val_loss: 2.5586, val_acc: 0.5767\n",
      "Epoch [296], train_loss: 0.0354, val_loss: 2.9230, val_acc: 0.5549\n",
      "Epoch [297], train_loss: 0.0317, val_loss: 3.8894, val_acc: 0.5093\n",
      "Epoch [298], train_loss: 0.0326, val_loss: 2.8780, val_acc: 0.5607\n",
      "Epoch [299], train_loss: 0.0146, val_loss: 3.6799, val_acc: 0.5532\n",
      "Epoch [300], train_loss: 0.0147, val_loss: 4.0359, val_acc: 0.5715\n",
      "Epoch [301], train_loss: 0.0720, val_loss: 3.6062, val_acc: 0.5259\n",
      "Epoch [302], train_loss: 0.0121, val_loss: 4.4206, val_acc: 0.5321\n",
      "Epoch [303], train_loss: 0.0023, val_loss: 4.9667, val_acc: 0.5503\n",
      "Epoch [304], train_loss: 0.0006, val_loss: 5.6211, val_acc: 0.5474\n",
      "Epoch [305], train_loss: 0.0003, val_loss: 6.2466, val_acc: 0.5422\n",
      "Epoch [306], train_loss: 0.0003, val_loss: 6.1867, val_acc: 0.5396\n",
      "Epoch [307], train_loss: 0.0001, val_loss: 6.3707, val_acc: 0.5532\n",
      "Epoch [308], train_loss: 0.0001, val_loss: 6.5553, val_acc: 0.5344\n",
      "Epoch [309], train_loss: 0.0001, val_loss: 6.4955, val_acc: 0.5581\n",
      "Epoch [310], train_loss: 0.0000, val_loss: 6.1617, val_acc: 0.5529\n",
      "Epoch [311], train_loss: 0.0000, val_loss: 6.2284, val_acc: 0.5529\n",
      "Epoch [312], train_loss: 0.0000, val_loss: 6.3076, val_acc: 0.5581\n",
      "Epoch [313], train_loss: 0.0000, val_loss: 6.3405, val_acc: 0.5555\n",
      "Epoch [314], train_loss: 0.0000, val_loss: 6.5248, val_acc: 0.5555\n",
      "Epoch [315], train_loss: 0.0000, val_loss: 6.6493, val_acc: 0.5581\n",
      "Epoch [316], train_loss: 0.0000, val_loss: 6.5787, val_acc: 0.5581\n",
      "Epoch [317], train_loss: 0.0000, val_loss: 6.4500, val_acc: 0.5555\n",
      "Epoch [318], train_loss: 0.0000, val_loss: 6.5959, val_acc: 0.5581\n",
      "Epoch [319], train_loss: 0.0000, val_loss: 6.8252, val_acc: 0.5715\n",
      "Epoch [320], train_loss: 0.0039, val_loss: 6.4901, val_acc: 0.5692\n",
      "Epoch [321], train_loss: 0.1370, val_loss: 2.9602, val_acc: 0.5506\n",
      "Epoch [322], train_loss: 0.0942, val_loss: 2.5077, val_acc: 0.5662\n",
      "Epoch [323], train_loss: 0.0483, val_loss: 2.6543, val_acc: 0.5656\n",
      "Epoch [324], train_loss: 0.0153, val_loss: 3.2760, val_acc: 0.5441\n",
      "Epoch [325], train_loss: 0.0271, val_loss: 2.5309, val_acc: 0.5526\n",
      "Epoch [326], train_loss: 0.0060, val_loss: 3.2626, val_acc: 0.5767\n",
      "Epoch [327], train_loss: 0.0013, val_loss: 3.6166, val_acc: 0.5737\n",
      "Epoch [328], train_loss: 0.0026, val_loss: 3.9767, val_acc: 0.5630\n",
      "Epoch [329], train_loss: 0.0176, val_loss: 3.4226, val_acc: 0.5773\n",
      "Epoch [330], train_loss: 0.0049, val_loss: 4.3795, val_acc: 0.5770\n",
      "Epoch [331], train_loss: 0.0055, val_loss: 4.1487, val_acc: 0.5929\n",
      "Epoch [332], train_loss: 0.0007, val_loss: 4.3255, val_acc: 0.6033\n",
      "Epoch [333], train_loss: 0.0013, val_loss: 4.2469, val_acc: 0.5985\n",
      "Epoch [334], train_loss: 0.0003, val_loss: 4.5613, val_acc: 0.6063\n",
      "Epoch [335], train_loss: 0.0002, val_loss: 4.7962, val_acc: 0.6037\n",
      "Epoch [336], train_loss: 0.0001, val_loss: 4.8639, val_acc: 0.6037\n",
      "Epoch [337], train_loss: 0.0001, val_loss: 4.8904, val_acc: 0.5981\n",
      "Epoch [338], train_loss: 0.0000, val_loss: 5.0677, val_acc: 0.5978\n",
      "Epoch [339], train_loss: 0.0001, val_loss: 4.9084, val_acc: 0.6007\n",
      "Epoch [340], train_loss: 0.0000, val_loss: 5.0098, val_acc: 0.5981\n",
      "Epoch [341], train_loss: 0.0001, val_loss: 5.1122, val_acc: 0.5900\n",
      "Epoch [342], train_loss: 0.0001, val_loss: 5.2165, val_acc: 0.5926\n",
      "Epoch [343], train_loss: 0.0000, val_loss: 5.2820, val_acc: 0.5926\n",
      "Epoch [344], train_loss: 0.0000, val_loss: 5.2672, val_acc: 0.6033\n",
      "Epoch [345], train_loss: 0.0000, val_loss: 5.4000, val_acc: 0.5874\n",
      "Epoch [346], train_loss: 0.0001, val_loss: 5.5088, val_acc: 0.5845\n",
      "Epoch [347], train_loss: 0.0000, val_loss: 5.4444, val_acc: 0.5900\n",
      "Epoch [348], train_loss: 0.0000, val_loss: 5.5126, val_acc: 0.5952\n",
      "Epoch [349], train_loss: 0.0000, val_loss: 5.4066, val_acc: 0.5897\n",
      "Epoch [350], train_loss: 0.0000, val_loss: 5.5678, val_acc: 0.5926\n",
      "Epoch [351], train_loss: 0.0000, val_loss: 5.4903, val_acc: 0.5845\n",
      "Epoch [352], train_loss: 0.0000, val_loss: 5.5955, val_acc: 0.5871\n",
      "Epoch [353], train_loss: 0.0000, val_loss: 5.6173, val_acc: 0.6059\n",
      "Epoch [354], train_loss: 0.0000, val_loss: 5.5990, val_acc: 0.5978\n",
      "Epoch [355], train_loss: 0.0000, val_loss: 5.6442, val_acc: 0.5978\n",
      "Epoch [356], train_loss: 0.0000, val_loss: 5.6434, val_acc: 0.5952\n",
      "Epoch [357], train_loss: 0.0000, val_loss: 5.6855, val_acc: 0.5897\n",
      "Epoch [358], train_loss: 0.0000, val_loss: 5.7198, val_acc: 0.5952\n",
      "Epoch [359], train_loss: 0.0000, val_loss: 5.7831, val_acc: 0.5897\n",
      "Epoch [360], train_loss: 0.0000, val_loss: 5.7198, val_acc: 0.6085\n",
      "Epoch [361], train_loss: 0.0000, val_loss: 5.8091, val_acc: 0.5952\n",
      "Epoch [362], train_loss: 0.0000, val_loss: 5.8208, val_acc: 0.6033\n",
      "Epoch [363], train_loss: 0.0000, val_loss: 5.8360, val_acc: 0.6033\n",
      "Epoch [364], train_loss: 0.0000, val_loss: 5.9172, val_acc: 0.5871\n",
      "Epoch [365], train_loss: 0.0000, val_loss: 5.9462, val_acc: 0.5952\n",
      "Epoch [366], train_loss: 0.0012, val_loss: 5.9266, val_acc: 0.5767\n",
      "Epoch [367], train_loss: 0.1144, val_loss: 2.8604, val_acc: 0.5610\n",
      "Epoch [368], train_loss: 0.2448, val_loss: 1.6446, val_acc: 0.5470\n",
      "Epoch [369], train_loss: 0.0739, val_loss: 2.7443, val_acc: 0.5318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370], train_loss: 0.0172, val_loss: 3.0739, val_acc: 0.5500\n",
      "Epoch [371], train_loss: 0.0111, val_loss: 3.6457, val_acc: 0.5790\n",
      "Epoch [372], train_loss: 0.0018, val_loss: 4.4170, val_acc: 0.5578\n",
      "Epoch [373], train_loss: 0.0009, val_loss: 4.8000, val_acc: 0.5393\n",
      "Epoch [374], train_loss: 0.0003, val_loss: 4.9871, val_acc: 0.5578\n",
      "Epoch [375], train_loss: 0.0001, val_loss: 5.0455, val_acc: 0.5575\n",
      "Epoch [376], train_loss: 0.0001, val_loss: 5.1242, val_acc: 0.5578\n",
      "Epoch [377], train_loss: 0.0003, val_loss: 5.2525, val_acc: 0.5422\n",
      "Epoch [378], train_loss: 0.0001, val_loss: 5.4738, val_acc: 0.5474\n",
      "Epoch [379], train_loss: 0.0000, val_loss: 5.3980, val_acc: 0.5581\n",
      "Epoch [380], train_loss: 0.0066, val_loss: 5.0386, val_acc: 0.5552\n",
      "Epoch [381], train_loss: 0.0095, val_loss: 4.4456, val_acc: 0.5370\n",
      "Epoch [382], train_loss: 0.0068, val_loss: 5.1140, val_acc: 0.5715\n",
      "Epoch [383], train_loss: 0.0438, val_loss: 3.5911, val_acc: 0.5500\n",
      "Epoch [384], train_loss: 0.0386, val_loss: 3.6924, val_acc: 0.5123\n",
      "Epoch [385], train_loss: 0.0194, val_loss: 4.1933, val_acc: 0.5337\n",
      "Epoch [386], train_loss: 0.0192, val_loss: 4.7965, val_acc: 0.5610\n",
      "Epoch [387], train_loss: 0.1115, val_loss: 2.7750, val_acc: 0.5656\n",
      "Epoch [388], train_loss: 0.0137, val_loss: 4.2141, val_acc: 0.5230\n",
      "Epoch [389], train_loss: 0.0081, val_loss: 4.1464, val_acc: 0.5370\n",
      "Epoch [390], train_loss: 0.0209, val_loss: 3.5879, val_acc: 0.5633\n",
      "Epoch [391], train_loss: 0.0525, val_loss: 2.3165, val_acc: 0.5669\n",
      "Epoch [392], train_loss: 0.0098, val_loss: 3.8079, val_acc: 0.5419\n",
      "Epoch [393], train_loss: 0.0043, val_loss: 3.9259, val_acc: 0.5370\n",
      "Epoch [394], train_loss: 0.0090, val_loss: 4.2802, val_acc: 0.5497\n",
      "Epoch [395], train_loss: 0.0386, val_loss: 3.4988, val_acc: 0.5366\n",
      "Epoch [396], train_loss: 0.0103, val_loss: 3.2118, val_acc: 0.5340\n",
      "Epoch [397], train_loss: 0.0057, val_loss: 3.6101, val_acc: 0.5288\n",
      "Epoch [398], train_loss: 0.0012, val_loss: 4.0374, val_acc: 0.5396\n",
      "Epoch [399], train_loss: 0.0009, val_loss: 4.3581, val_acc: 0.5314\n",
      "Epoch [400], train_loss: 0.0003, val_loss: 4.3868, val_acc: 0.5262\n",
      "Epoch [401], train_loss: 0.0006, val_loss: 4.7361, val_acc: 0.5233\n",
      "Epoch [402], train_loss: 0.0005, val_loss: 4.6164, val_acc: 0.5314\n",
      "Epoch [403], train_loss: 0.0002, val_loss: 4.8213, val_acc: 0.5314\n",
      "Epoch [404], train_loss: 0.0001, val_loss: 4.8206, val_acc: 0.5366\n",
      "Epoch [405], train_loss: 0.0001, val_loss: 4.8921, val_acc: 0.5340\n",
      "Epoch [406], train_loss: 0.0001, val_loss: 5.0632, val_acc: 0.5340\n",
      "Epoch [407], train_loss: 0.0000, val_loss: 5.0196, val_acc: 0.5422\n",
      "Epoch [408], train_loss: 0.0000, val_loss: 5.2348, val_acc: 0.5340\n",
      "Epoch [409], train_loss: 0.0000, val_loss: 5.1653, val_acc: 0.5340\n",
      "Epoch [410], train_loss: 0.0001, val_loss: 5.0508, val_acc: 0.5207\n",
      "Epoch [411], train_loss: 0.0000, val_loss: 5.1527, val_acc: 0.5393\n",
      "Epoch [412], train_loss: 0.0000, val_loss: 5.1815, val_acc: 0.5340\n",
      "Epoch [413], train_loss: 0.0000, val_loss: 5.2544, val_acc: 0.5366\n",
      "Epoch [414], train_loss: 0.0000, val_loss: 5.2295, val_acc: 0.5393\n",
      "Epoch [415], train_loss: 0.0000, val_loss: 5.3001, val_acc: 0.5393\n",
      "Epoch [416], train_loss: 0.0000, val_loss: 5.2679, val_acc: 0.5366\n",
      "Epoch [417], train_loss: 0.0000, val_loss: 5.3187, val_acc: 0.5419\n",
      "Epoch [418], train_loss: 0.0000, val_loss: 5.3846, val_acc: 0.5393\n",
      "Epoch [419], train_loss: 0.0000, val_loss: 5.3790, val_acc: 0.5366\n",
      "Epoch [420], train_loss: 0.0000, val_loss: 5.4736, val_acc: 0.5259\n",
      "Epoch [421], train_loss: 0.0000, val_loss: 5.5081, val_acc: 0.5366\n",
      "Epoch [422], train_loss: 0.0000, val_loss: 5.4882, val_acc: 0.5311\n",
      "Epoch [423], train_loss: 0.0000, val_loss: 5.5042, val_acc: 0.5340\n",
      "Epoch [424], train_loss: 0.0000, val_loss: 5.5060, val_acc: 0.5233\n",
      "Epoch [425], train_loss: 0.0011, val_loss: 5.5585, val_acc: 0.5318\n",
      "Epoch [426], train_loss: 0.0559, val_loss: 2.7120, val_acc: 0.5584\n",
      "Epoch [427], train_loss: 0.0247, val_loss: 3.2190, val_acc: 0.5848\n",
      "Epoch [428], train_loss: 0.0175, val_loss: 3.9560, val_acc: 0.5529\n",
      "Epoch [429], train_loss: 0.0103, val_loss: 3.4557, val_acc: 0.5523\n",
      "Epoch [430], train_loss: 0.0161, val_loss: 3.5192, val_acc: 0.5607\n",
      "Epoch [431], train_loss: 0.0403, val_loss: 2.6992, val_acc: 0.5718\n",
      "Epoch [432], train_loss: 0.0223, val_loss: 3.5330, val_acc: 0.5396\n",
      "Epoch [433], train_loss: 0.0069, val_loss: 3.5228, val_acc: 0.5741\n",
      "Epoch [434], train_loss: 0.0028, val_loss: 4.1391, val_acc: 0.5607\n",
      "Epoch [435], train_loss: 0.0031, val_loss: 4.5365, val_acc: 0.5607\n",
      "Epoch [436], train_loss: 0.0154, val_loss: 4.5014, val_acc: 0.5903\n",
      "Epoch [437], train_loss: 0.0115, val_loss: 4.3689, val_acc: 0.5581\n",
      "Epoch [438], train_loss: 0.0248, val_loss: 2.9538, val_acc: 0.5662\n",
      "Epoch [439], train_loss: 0.0125, val_loss: 3.9289, val_acc: 0.5207\n",
      "Epoch [440], train_loss: 0.0012, val_loss: 4.3007, val_acc: 0.5259\n",
      "Epoch [441], train_loss: 0.0002, val_loss: 4.4800, val_acc: 0.5152\n",
      "Epoch [442], train_loss: 0.0003, val_loss: 4.5423, val_acc: 0.5233\n",
      "Epoch [443], train_loss: 0.0001, val_loss: 4.7442, val_acc: 0.5152\n",
      "Epoch [444], train_loss: 0.0001, val_loss: 4.7552, val_acc: 0.5233\n",
      "Epoch [445], train_loss: 0.0001, val_loss: 4.8645, val_acc: 0.5366\n",
      "Epoch [446], train_loss: 0.0000, val_loss: 4.9435, val_acc: 0.5311\n",
      "Epoch [447], train_loss: 0.0000, val_loss: 5.0577, val_acc: 0.5259\n",
      "Epoch [448], train_loss: 0.0000, val_loss: 4.9933, val_acc: 0.5337\n",
      "Epoch [449], train_loss: 0.0000, val_loss: 4.9682, val_acc: 0.5126\n",
      "Epoch [450], train_loss: 0.0000, val_loss: 4.9887, val_acc: 0.5233\n",
      "Epoch [451], train_loss: 0.0000, val_loss: 5.1504, val_acc: 0.5259\n",
      "Epoch [452], train_loss: 0.0001, val_loss: 5.1930, val_acc: 0.5340\n",
      "Epoch [453], train_loss: 0.0000, val_loss: 5.2911, val_acc: 0.5393\n",
      "Epoch [454], train_loss: 0.0001, val_loss: 5.2957, val_acc: 0.5366\n",
      "Epoch [455], train_loss: 0.0000, val_loss: 5.3910, val_acc: 0.5288\n",
      "Epoch [456], train_loss: 0.0000, val_loss: 5.4254, val_acc: 0.5314\n",
      "Epoch [457], train_loss: 0.0000, val_loss: 5.4255, val_acc: 0.5262\n",
      "Epoch [458], train_loss: 0.0000, val_loss: 5.4625, val_acc: 0.5340\n",
      "Epoch [459], train_loss: 0.0000, val_loss: 5.4367, val_acc: 0.5314\n",
      "Epoch [460], train_loss: 0.0000, val_loss: 5.5010, val_acc: 0.5314\n",
      "Epoch [461], train_loss: 0.0000, val_loss: 5.4840, val_acc: 0.5262\n",
      "Epoch [462], train_loss: 0.0000, val_loss: 5.5503, val_acc: 0.5259\n",
      "Epoch [463], train_loss: 0.0001, val_loss: 5.5285, val_acc: 0.5152\n",
      "Epoch [464], train_loss: 0.0001, val_loss: 5.0091, val_acc: 0.5097\n",
      "Epoch [465], train_loss: 0.0000, val_loss: 5.2660, val_acc: 0.5256\n",
      "Epoch [466], train_loss: 0.0000, val_loss: 5.2666, val_acc: 0.5178\n",
      "Epoch [467], train_loss: 0.0001, val_loss: 5.2944, val_acc: 0.5178\n",
      "Epoch [468], train_loss: 0.0000, val_loss: 5.3873, val_acc: 0.5204\n",
      "Epoch [469], train_loss: 0.0000, val_loss: 5.2085, val_acc: 0.5288\n",
      "Epoch [470], train_loss: 0.0000, val_loss: 5.4452, val_acc: 0.5178\n",
      "Epoch [471], train_loss: 0.0000, val_loss: 5.5255, val_acc: 0.5178\n",
      "Epoch [472], train_loss: 0.0000, val_loss: 5.4798, val_acc: 0.5152\n",
      "Epoch [473], train_loss: 0.0000, val_loss: 5.5787, val_acc: 0.5233\n",
      "Epoch [474], train_loss: 0.0000, val_loss: 5.5139, val_acc: 0.5285\n",
      "Epoch [475], train_loss: 0.0000, val_loss: 5.5361, val_acc: 0.5204\n",
      "Epoch [476], train_loss: 0.0000, val_loss: 5.5954, val_acc: 0.5285\n",
      "Epoch [477], train_loss: 0.0000, val_loss: 5.7031, val_acc: 0.5126\n",
      "Epoch [478], train_loss: 0.0000, val_loss: 5.7405, val_acc: 0.5204\n",
      "Epoch [479], train_loss: 0.0000, val_loss: 5.7328, val_acc: 0.5178\n",
      "Epoch [480], train_loss: 0.0000, val_loss: 5.7528, val_acc: 0.5285\n",
      "Epoch [481], train_loss: 0.0000, val_loss: 5.6368, val_acc: 0.5204\n",
      "Epoch [482], train_loss: 0.0000, val_loss: 5.8188, val_acc: 0.5207\n",
      "Epoch [483], train_loss: 0.0000, val_loss: 5.8247, val_acc: 0.5204\n",
      "Epoch [484], train_loss: 0.0000, val_loss: 5.7515, val_acc: 0.5285\n",
      "Epoch [485], train_loss: 0.0000, val_loss: 5.9120, val_acc: 0.5233\n",
      "Epoch [486], train_loss: 0.0000, val_loss: 5.7988, val_acc: 0.5259\n",
      "Epoch [487], train_loss: 0.0000, val_loss: 5.9282, val_acc: 0.5259\n",
      "Epoch [488], train_loss: 0.0000, val_loss: 5.8598, val_acc: 0.5259\n",
      "Epoch [489], train_loss: 0.0000, val_loss: 5.8964, val_acc: 0.5285\n",
      "Epoch [490], train_loss: 0.0000, val_loss: 5.9555, val_acc: 0.5285\n",
      "Epoch [491], train_loss: 0.0000, val_loss: 6.0126, val_acc: 0.5178\n",
      "Epoch [492], train_loss: 0.0000, val_loss: 5.9663, val_acc: 0.5126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [493], train_loss: 0.0000, val_loss: 5.9678, val_acc: 0.5178\n",
      "Epoch [494], train_loss: 0.0000, val_loss: 6.0489, val_acc: 0.5126\n",
      "Epoch [495], train_loss: 0.0000, val_loss: 6.0548, val_acc: 0.5233\n",
      "Epoch [496], train_loss: 0.0000, val_loss: 6.0513, val_acc: 0.5152\n",
      "Epoch [497], train_loss: 0.0000, val_loss: 6.0953, val_acc: 0.5178\n",
      "Epoch [498], train_loss: 0.0000, val_loss: 6.1112, val_acc: 0.5152\n",
      "Epoch [499], train_loss: 0.0000, val_loss: 6.0676, val_acc: 0.5259\n",
      "Epoch [500], train_loss: 0.0000, val_loss: 6.1364, val_acc: 0.5285\n",
      "Epoch [501], train_loss: 0.0000, val_loss: 6.0506, val_acc: 0.5311\n",
      "Epoch [502], train_loss: 0.0001, val_loss: 6.6680, val_acc: 0.5149\n",
      "Epoch [503], train_loss: 0.0000, val_loss: 6.3959, val_acc: 0.5311\n",
      "Epoch [504], train_loss: 0.0000, val_loss: 6.3513, val_acc: 0.5178\n",
      "Epoch [505], train_loss: 0.0000, val_loss: 6.4338, val_acc: 0.5233\n",
      "Epoch [506], train_loss: 0.0000, val_loss: 6.4280, val_acc: 0.5259\n",
      "Epoch [507], train_loss: 0.0000, val_loss: 6.5313, val_acc: 0.5262\n",
      "Epoch [508], train_loss: 0.0001, val_loss: 6.6004, val_acc: 0.5314\n",
      "Epoch [509], train_loss: 0.0588, val_loss: 2.9813, val_acc: 0.5542\n",
      "Epoch [510], train_loss: 0.1111, val_loss: 1.9190, val_acc: 0.5402\n",
      "Epoch [511], train_loss: 0.0256, val_loss: 3.0510, val_acc: 0.5767\n",
      "Epoch [512], train_loss: 0.0116, val_loss: 4.0502, val_acc: 0.5454\n",
      "Epoch [513], train_loss: 0.0254, val_loss: 4.1948, val_acc: 0.5480\n",
      "Epoch [514], train_loss: 0.0265, val_loss: 2.7561, val_acc: 0.5370\n",
      "Epoch [515], train_loss: 0.0104, val_loss: 3.4761, val_acc: 0.5701\n",
      "Epoch [516], train_loss: 0.0345, val_loss: 2.9153, val_acc: 0.5509\n",
      "Epoch [517], train_loss: 0.0471, val_loss: 1.7928, val_acc: 0.5558\n",
      "Epoch [518], train_loss: 0.0259, val_loss: 3.4932, val_acc: 0.5477\n",
      "Epoch [519], train_loss: 0.0230, val_loss: 2.9214, val_acc: 0.5558\n",
      "Epoch [520], train_loss: 0.0045, val_loss: 3.7233, val_acc: 0.5532\n",
      "Epoch [521], train_loss: 0.0015, val_loss: 3.8509, val_acc: 0.5773\n",
      "Epoch [522], train_loss: 0.0007, val_loss: 3.9558, val_acc: 0.5747\n",
      "Epoch [523], train_loss: 0.0005, val_loss: 3.9409, val_acc: 0.5773\n",
      "Epoch [524], train_loss: 0.0003, val_loss: 4.2311, val_acc: 0.5799\n",
      "Epoch [525], train_loss: 0.0001, val_loss: 4.2673, val_acc: 0.5884\n",
      "Epoch [526], train_loss: 0.0001, val_loss: 4.2306, val_acc: 0.5617\n",
      "Epoch [527], train_loss: 0.0001, val_loss: 4.4314, val_acc: 0.5721\n",
      "Epoch [528], train_loss: 0.0000, val_loss: 4.4879, val_acc: 0.5669\n",
      "Epoch [529], train_loss: 0.0000, val_loss: 4.6674, val_acc: 0.5721\n",
      "Epoch [530], train_loss: 0.0001, val_loss: 4.5303, val_acc: 0.5669\n",
      "Epoch [531], train_loss: 0.0000, val_loss: 4.6485, val_acc: 0.5721\n",
      "Epoch [532], train_loss: 0.0000, val_loss: 4.7007, val_acc: 0.5721\n",
      "Epoch [533], train_loss: 0.0000, val_loss: 4.7220, val_acc: 0.5776\n",
      "Epoch [534], train_loss: 0.0000, val_loss: 4.7445, val_acc: 0.5747\n",
      "Epoch [535], train_loss: 0.0000, val_loss: 4.6732, val_acc: 0.5695\n",
      "Epoch [536], train_loss: 0.0000, val_loss: 4.7743, val_acc: 0.5669\n",
      "Epoch [537], train_loss: 0.0000, val_loss: 4.8750, val_acc: 0.5695\n",
      "Epoch [538], train_loss: 0.0000, val_loss: 4.9352, val_acc: 0.5747\n",
      "Epoch [539], train_loss: 0.0000, val_loss: 4.9444, val_acc: 0.5588\n",
      "Epoch [540], train_loss: 0.0001, val_loss: 4.9866, val_acc: 0.5773\n",
      "Epoch [541], train_loss: 0.0000, val_loss: 5.0663, val_acc: 0.5773\n",
      "Epoch [542], train_loss: 0.0000, val_loss: 5.1251, val_acc: 0.5773\n",
      "Epoch [543], train_loss: 0.0000, val_loss: 5.0361, val_acc: 0.5828\n",
      "Epoch [544], train_loss: 0.0000, val_loss: 5.0736, val_acc: 0.5721\n",
      "Epoch [545], train_loss: 0.0000, val_loss: 5.1456, val_acc: 0.5799\n",
      "Epoch [546], train_loss: 0.0000, val_loss: 5.1445, val_acc: 0.5721\n",
      "Epoch [547], train_loss: 0.0000, val_loss: 5.1766, val_acc: 0.5828\n",
      "Epoch [548], train_loss: 0.0000, val_loss: 5.1900, val_acc: 0.5828\n",
      "Epoch [549], train_loss: 0.0000, val_loss: 5.2162, val_acc: 0.5851\n",
      "Epoch [550], train_loss: 0.0000, val_loss: 5.1957, val_acc: 0.5854\n",
      "Epoch [551], train_loss: 0.0000, val_loss: 5.3049, val_acc: 0.5773\n",
      "Epoch [552], train_loss: 0.0000, val_loss: 5.3010, val_acc: 0.5802\n",
      "Epoch [553], train_loss: 0.0000, val_loss: 5.2502, val_acc: 0.5692\n",
      "Epoch [554], train_loss: 0.0000, val_loss: 5.4198, val_acc: 0.5747\n",
      "Epoch [555], train_loss: 0.0000, val_loss: 5.3967, val_acc: 0.5854\n",
      "Epoch [556], train_loss: 0.0000, val_loss: 5.2885, val_acc: 0.5773\n",
      "Epoch [557], train_loss: 0.0000, val_loss: 5.4737, val_acc: 0.5773\n",
      "Epoch [558], train_loss: 0.0000, val_loss: 5.5100, val_acc: 0.5825\n",
      "Epoch [559], train_loss: 0.0000, val_loss: 5.4278, val_acc: 0.5614\n",
      "Epoch [560], train_loss: 0.0000, val_loss: 5.5654, val_acc: 0.5825\n",
      "Epoch [561], train_loss: 0.0000, val_loss: 5.4343, val_acc: 0.5721\n",
      "Epoch [562], train_loss: 0.0000, val_loss: 5.5101, val_acc: 0.5880\n",
      "Epoch [563], train_loss: 0.0000, val_loss: 5.5290, val_acc: 0.5854\n",
      "Epoch [564], train_loss: 0.0000, val_loss: 5.5670, val_acc: 0.5828\n",
      "Epoch [565], train_loss: 0.0000, val_loss: 5.6013, val_acc: 0.5825\n",
      "Epoch [566], train_loss: 0.0000, val_loss: 5.4207, val_acc: 0.5828\n",
      "Epoch [567], train_loss: 0.0000, val_loss: 5.5813, val_acc: 0.5880\n",
      "Epoch [568], train_loss: 0.0000, val_loss: 5.5315, val_acc: 0.5880\n",
      "Epoch [569], train_loss: 0.0000, val_loss: 5.6103, val_acc: 0.5828\n",
      "Epoch [570], train_loss: 0.0000, val_loss: 5.6520, val_acc: 0.5773\n",
      "Epoch [571], train_loss: 0.0000, val_loss: 5.9346, val_acc: 0.5880\n",
      "Epoch [572], train_loss: 0.0000, val_loss: 5.8205, val_acc: 0.5747\n",
      "Epoch [573], train_loss: 0.0000, val_loss: 5.9075, val_acc: 0.5721\n",
      "Epoch [574], train_loss: 0.0000, val_loss: 5.8385, val_acc: 0.5776\n",
      "Epoch [575], train_loss: 0.0000, val_loss: 5.8621, val_acc: 0.5776\n",
      "Epoch [576], train_loss: 0.0000, val_loss: 5.9205, val_acc: 0.5802\n",
      "Epoch [577], train_loss: 0.0000, val_loss: 5.9715, val_acc: 0.5750\n",
      "Epoch [578], train_loss: 0.0000, val_loss: 5.9154, val_acc: 0.5750\n",
      "Epoch [579], train_loss: 0.0000, val_loss: 6.0013, val_acc: 0.5802\n",
      "Epoch [580], train_loss: 0.0000, val_loss: 5.9382, val_acc: 0.5776\n",
      "Epoch [581], train_loss: 0.0000, val_loss: 5.8856, val_acc: 0.5802\n",
      "Epoch [582], train_loss: 0.0000, val_loss: 5.7626, val_acc: 0.5750\n",
      "Epoch [583], train_loss: 0.0000, val_loss: 5.9745, val_acc: 0.5776\n",
      "Epoch [584], train_loss: 0.0000, val_loss: 6.0645, val_acc: 0.5776\n",
      "Epoch [585], train_loss: 0.0000, val_loss: 5.9558, val_acc: 0.5854\n",
      "Epoch [586], train_loss: 0.0000, val_loss: 6.0714, val_acc: 0.5750\n",
      "Epoch [587], train_loss: 0.0000, val_loss: 5.9527, val_acc: 0.5802\n",
      "Epoch [588], train_loss: 0.0910, val_loss: 2.1253, val_acc: 0.5662\n",
      "Epoch [589], train_loss: 0.1000, val_loss: 2.3655, val_acc: 0.5610\n",
      "Epoch [590], train_loss: 0.0511, val_loss: 2.7359, val_acc: 0.5666\n",
      "Epoch [591], train_loss: 0.0113, val_loss: 4.3166, val_acc: 0.5718\n",
      "Epoch [592], train_loss: 0.0084, val_loss: 3.7771, val_acc: 0.5926\n",
      "Epoch [593], train_loss: 0.0113, val_loss: 3.7621, val_acc: 0.5741\n",
      "Epoch [594], train_loss: 0.0276, val_loss: 3.0257, val_acc: 0.5692\n",
      "Epoch [595], train_loss: 0.0138, val_loss: 3.4372, val_acc: 0.5799\n",
      "Epoch [596], train_loss: 0.0027, val_loss: 3.9856, val_acc: 0.6063\n",
      "Epoch [597], train_loss: 0.0012, val_loss: 4.0619, val_acc: 0.6014\n",
      "Epoch [598], train_loss: 0.0008, val_loss: 4.3485, val_acc: 0.6011\n",
      "Epoch [599], train_loss: 0.0003, val_loss: 4.4580, val_acc: 0.6059\n",
      "Epoch [600], train_loss: 0.0002, val_loss: 4.6981, val_acc: 0.6011\n",
      "Epoch [601], train_loss: 0.0001, val_loss: 4.7037, val_acc: 0.5929\n",
      "Epoch [602], train_loss: 0.0001, val_loss: 4.9053, val_acc: 0.6037\n",
      "Epoch [603], train_loss: 0.0001, val_loss: 5.1273, val_acc: 0.5955\n",
      "Epoch [604], train_loss: 0.0001, val_loss: 5.1954, val_acc: 0.5955\n",
      "Epoch [605], train_loss: 0.0000, val_loss: 5.2552, val_acc: 0.5929\n",
      "Epoch [606], train_loss: 0.0000, val_loss: 5.3547, val_acc: 0.6011\n",
      "Epoch [607], train_loss: 0.0000, val_loss: 5.3200, val_acc: 0.5903\n",
      "Epoch [608], train_loss: 0.0000, val_loss: 5.3129, val_acc: 0.5955\n",
      "Epoch [609], train_loss: 0.0000, val_loss: 5.4105, val_acc: 0.5985\n",
      "Epoch [610], train_loss: 0.0001, val_loss: 5.4203, val_acc: 0.5906\n",
      "Epoch [611], train_loss: 0.0000, val_loss: 5.5622, val_acc: 0.5958\n",
      "Epoch [612], train_loss: 0.0001, val_loss: 5.5490, val_acc: 0.5851\n",
      "Epoch [613], train_loss: 0.0000, val_loss: 5.6491, val_acc: 0.5906\n",
      "Epoch [614], train_loss: 0.0000, val_loss: 5.7112, val_acc: 0.5773\n",
      "Epoch [615], train_loss: 0.0000, val_loss: 5.7434, val_acc: 0.5825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [616], train_loss: 0.0001, val_loss: 5.9082, val_acc: 0.5958\n",
      "Epoch [617], train_loss: 0.0000, val_loss: 5.8201, val_acc: 0.5799\n",
      "Epoch [618], train_loss: 0.0000, val_loss: 5.6731, val_acc: 0.5718\n",
      "Epoch [619], train_loss: 0.0000, val_loss: 5.8155, val_acc: 0.5851\n",
      "Epoch [620], train_loss: 0.0000, val_loss: 5.6504, val_acc: 0.5825\n",
      "Epoch [621], train_loss: 0.0000, val_loss: 5.8009, val_acc: 0.5825\n",
      "Epoch [622], train_loss: 0.0000, val_loss: 5.9787, val_acc: 0.5799\n",
      "Epoch [623], train_loss: 0.0000, val_loss: 5.9037, val_acc: 0.5776\n",
      "Epoch [624], train_loss: 0.0000, val_loss: 5.9833, val_acc: 0.5724\n",
      "Epoch [625], train_loss: 0.0000, val_loss: 5.9191, val_acc: 0.5910\n",
      "Epoch [626], train_loss: 0.0000, val_loss: 5.9898, val_acc: 0.5932\n",
      "Epoch [627], train_loss: 0.0000, val_loss: 6.0337, val_acc: 0.5796\n",
      "Epoch [628], train_loss: 0.0000, val_loss: 6.0610, val_acc: 0.5796\n",
      "Epoch [629], train_loss: 0.0000, val_loss: 6.0551, val_acc: 0.5796\n",
      "Epoch [630], train_loss: 0.0000, val_loss: 6.0636, val_acc: 0.5770\n",
      "Epoch [631], train_loss: 0.0000, val_loss: 6.0956, val_acc: 0.5770\n",
      "Epoch [632], train_loss: 0.0000, val_loss: 6.1332, val_acc: 0.5796\n",
      "Epoch [633], train_loss: 0.0000, val_loss: 6.1035, val_acc: 0.5770\n",
      "Epoch [634], train_loss: 0.0000, val_loss: 6.1769, val_acc: 0.5770\n",
      "Epoch [635], train_loss: 0.0000, val_loss: 6.2496, val_acc: 0.5770\n",
      "Epoch [636], train_loss: 0.0000, val_loss: 6.2300, val_acc: 0.5744\n",
      "Epoch [637], train_loss: 0.0000, val_loss: 6.2195, val_acc: 0.5744\n",
      "Epoch [638], train_loss: 0.0000, val_loss: 6.2092, val_acc: 0.5796\n",
      "Epoch [639], train_loss: 0.0000, val_loss: 6.1834, val_acc: 0.5770\n",
      "Epoch [640], train_loss: 0.0000, val_loss: 6.2052, val_acc: 0.5770\n",
      "Epoch [641], train_loss: 0.0000, val_loss: 6.1520, val_acc: 0.5903\n",
      "Epoch [642], train_loss: 0.0000, val_loss: 6.2478, val_acc: 0.5770\n",
      "Epoch [643], train_loss: 0.0000, val_loss: 6.2501, val_acc: 0.5744\n",
      "Epoch [644], train_loss: 0.0000, val_loss: 6.2397, val_acc: 0.5851\n",
      "Epoch [645], train_loss: 0.0000, val_loss: 6.0222, val_acc: 0.5770\n",
      "Epoch [646], train_loss: 0.0000, val_loss: 6.2565, val_acc: 0.5851\n",
      "Epoch [647], train_loss: 0.0000, val_loss: 6.2070, val_acc: 0.5796\n",
      "Epoch [648], train_loss: 0.0000, val_loss: 6.2806, val_acc: 0.5796\n",
      "Epoch [649], train_loss: 0.0000, val_loss: 6.2215, val_acc: 0.5903\n",
      "Epoch [650], train_loss: 0.0000, val_loss: 6.2981, val_acc: 0.5744\n",
      "Epoch [651], train_loss: 0.0000, val_loss: 6.3464, val_acc: 0.5744\n",
      "Epoch [652], train_loss: 0.0000, val_loss: 6.3815, val_acc: 0.5770\n",
      "Epoch [653], train_loss: 0.0000, val_loss: 6.2070, val_acc: 0.5770\n",
      "Epoch [654], train_loss: 0.0000, val_loss: 6.2250, val_acc: 0.5770\n",
      "Epoch [655], train_loss: 0.0000, val_loss: 6.3762, val_acc: 0.5770\n",
      "Epoch [656], train_loss: 0.0000, val_loss: 6.3360, val_acc: 0.5822\n",
      "Epoch [657], train_loss: 0.0000, val_loss: 6.3156, val_acc: 0.5796\n",
      "Epoch [658], train_loss: 0.0000, val_loss: 6.4550, val_acc: 0.5796\n",
      "Epoch [659], train_loss: 0.0000, val_loss: 6.3209, val_acc: 0.5770\n",
      "Epoch [660], train_loss: 0.0000, val_loss: 6.2826, val_acc: 0.5822\n",
      "Epoch [661], train_loss: 0.0000, val_loss: 6.4047, val_acc: 0.5877\n",
      "Epoch [662], train_loss: 0.0000, val_loss: 6.5462, val_acc: 0.5799\n",
      "Epoch [663], train_loss: 0.0000, val_loss: 6.2866, val_acc: 0.5822\n",
      "Epoch [664], train_loss: 0.0000, val_loss: 6.4277, val_acc: 0.5822\n",
      "Epoch [665], train_loss: 0.0000, val_loss: 6.4598, val_acc: 0.5744\n",
      "Epoch [666], train_loss: 0.0023, val_loss: 6.9437, val_acc: 0.5877\n",
      "Epoch [667], train_loss: 0.1479, val_loss: 1.9494, val_acc: 0.5867\n",
      "Epoch [668], train_loss: 0.0773, val_loss: 2.6211, val_acc: 0.6079\n",
      "Epoch [669], train_loss: 0.0343, val_loss: 2.9873, val_acc: 0.5854\n",
      "Epoch [670], train_loss: 0.0244, val_loss: 2.1648, val_acc: 0.6179\n",
      "Epoch [671], train_loss: 0.0123, val_loss: 3.0619, val_acc: 0.6121\n",
      "Epoch [672], train_loss: 0.0017, val_loss: 4.0946, val_acc: 0.6202\n",
      "Epoch [673], train_loss: 0.0007, val_loss: 4.1986, val_acc: 0.6037\n",
      "Epoch [674], train_loss: 0.0003, val_loss: 4.3924, val_acc: 0.6092\n",
      "Epoch [675], train_loss: 0.0001, val_loss: 4.6348, val_acc: 0.5903\n",
      "Epoch [676], train_loss: 0.0001, val_loss: 4.6697, val_acc: 0.5903\n",
      "Epoch [677], train_loss: 0.0001, val_loss: 4.7992, val_acc: 0.5825\n",
      "Epoch [678], train_loss: 0.0001, val_loss: 4.8189, val_acc: 0.5903\n",
      "Epoch [679], train_loss: 0.0001, val_loss: 4.8054, val_acc: 0.5796\n",
      "Epoch [680], train_loss: 0.0001, val_loss: 4.8195, val_acc: 0.5851\n",
      "Epoch [681], train_loss: 0.0001, val_loss: 4.9247, val_acc: 0.5903\n",
      "Epoch [682], train_loss: 0.0001, val_loss: 4.8868, val_acc: 0.5877\n",
      "Epoch [683], train_loss: 0.0000, val_loss: 4.9702, val_acc: 0.5851\n",
      "Epoch [684], train_loss: 0.0000, val_loss: 5.0239, val_acc: 0.5796\n",
      "Epoch [685], train_loss: 0.0000, val_loss: 5.0951, val_acc: 0.5877\n",
      "Epoch [686], train_loss: 0.0000, val_loss: 5.1433, val_acc: 0.5903\n",
      "Epoch [687], train_loss: 0.0000, val_loss: 5.1866, val_acc: 0.5877\n",
      "Epoch [688], train_loss: 0.0000, val_loss: 5.1105, val_acc: 0.5822\n",
      "Epoch [689], train_loss: 0.0000, val_loss: 5.2428, val_acc: 0.5903\n",
      "Epoch [690], train_loss: 0.0001, val_loss: 5.2129, val_acc: 0.5877\n",
      "Epoch [691], train_loss: 0.0000, val_loss: 4.9992, val_acc: 0.5958\n",
      "Epoch [692], train_loss: 0.0001, val_loss: 5.0492, val_acc: 0.5906\n",
      "Epoch [693], train_loss: 0.0458, val_loss: 4.0348, val_acc: 0.5962\n",
      "Epoch [694], train_loss: 0.0753, val_loss: 2.1628, val_acc: 0.5900\n",
      "Epoch [695], train_loss: 0.0190, val_loss: 2.6695, val_acc: 0.5981\n",
      "Epoch [696], train_loss: 0.0172, val_loss: 3.3531, val_acc: 0.5981\n",
      "Epoch [697], train_loss: 0.0315, val_loss: 2.6935, val_acc: 0.5848\n",
      "Epoch [698], train_loss: 0.0063, val_loss: 3.0082, val_acc: 0.5988\n",
      "Epoch [699], train_loss: 0.0047, val_loss: 3.5241, val_acc: 0.5952\n",
      "Epoch [700], train_loss: 0.0013, val_loss: 4.2903, val_acc: 0.6001\n",
      "Epoch [701], train_loss: 0.0004, val_loss: 4.4353, val_acc: 0.5763\n",
      "Epoch [702], train_loss: 0.0005, val_loss: 4.3337, val_acc: 0.5763\n",
      "Epoch [703], train_loss: 0.0001, val_loss: 4.5371, val_acc: 0.5842\n",
      "Epoch [704], train_loss: 0.0001, val_loss: 4.7058, val_acc: 0.5845\n",
      "Epoch [705], train_loss: 0.0000, val_loss: 4.7098, val_acc: 0.5845\n",
      "Epoch [706], train_loss: 0.0000, val_loss: 4.7412, val_acc: 0.5763\n",
      "Epoch [707], train_loss: 0.0001, val_loss: 5.0246, val_acc: 0.5897\n",
      "Epoch [708], train_loss: 0.0000, val_loss: 4.8709, val_acc: 0.5790\n",
      "Epoch [709], train_loss: 0.0001, val_loss: 4.9913, val_acc: 0.5737\n",
      "Epoch [710], train_loss: 0.0000, val_loss: 4.9527, val_acc: 0.5685\n",
      "Epoch [711], train_loss: 0.0000, val_loss: 5.0006, val_acc: 0.5737\n",
      "Epoch [712], train_loss: 0.0000, val_loss: 5.2029, val_acc: 0.5763\n",
      "Epoch [713], train_loss: 0.0000, val_loss: 5.0617, val_acc: 0.5711\n",
      "Epoch [714], train_loss: 0.0000, val_loss: 5.1320, val_acc: 0.5763\n",
      "Epoch [715], train_loss: 0.0000, val_loss: 5.2437, val_acc: 0.5737\n",
      "Epoch [716], train_loss: 0.0000, val_loss: 5.2552, val_acc: 0.5897\n",
      "Epoch [717], train_loss: 0.0000, val_loss: 5.3235, val_acc: 0.5819\n",
      "Epoch [718], train_loss: 0.0000, val_loss: 5.2935, val_acc: 0.5819\n",
      "Epoch [719], train_loss: 0.0000, val_loss: 5.3657, val_acc: 0.5952\n",
      "Epoch [720], train_loss: 0.0000, val_loss: 5.3723, val_acc: 0.5900\n",
      "Epoch [721], train_loss: 0.0000, val_loss: 5.3747, val_acc: 0.5819\n",
      "Epoch [722], train_loss: 0.0000, val_loss: 5.4938, val_acc: 0.5845\n",
      "Epoch [723], train_loss: 0.0000, val_loss: 5.3300, val_acc: 0.5845\n",
      "Epoch [724], train_loss: 0.0000, val_loss: 5.5558, val_acc: 0.5871\n",
      "Epoch [725], train_loss: 0.0000, val_loss: 5.5711, val_acc: 0.5871\n",
      "Epoch [726], train_loss: 0.0000, val_loss: 5.5814, val_acc: 0.5763\n",
      "Epoch [727], train_loss: 0.0000, val_loss: 5.6196, val_acc: 0.5819\n",
      "Epoch [728], train_loss: 0.0000, val_loss: 5.6954, val_acc: 0.5871\n",
      "Epoch [729], train_loss: 0.0000, val_loss: 5.5983, val_acc: 0.5845\n",
      "Epoch [730], train_loss: 0.0000, val_loss: 5.6708, val_acc: 0.5819\n",
      "Epoch [731], train_loss: 0.0000, val_loss: 5.6220, val_acc: 0.5845\n",
      "Epoch [732], train_loss: 0.0000, val_loss: 5.6276, val_acc: 0.5900\n",
      "Epoch [733], train_loss: 0.0000, val_loss: 5.6291, val_acc: 0.5952\n",
      "Epoch [734], train_loss: 0.0000, val_loss: 5.8518, val_acc: 0.5763\n",
      "Epoch [735], train_loss: 0.0000, val_loss: 5.7143, val_acc: 0.5845\n",
      "Epoch [736], train_loss: 0.0000, val_loss: 5.8335, val_acc: 0.5737\n",
      "Epoch [737], train_loss: 0.0000, val_loss: 5.6568, val_acc: 0.5737\n",
      "Epoch [738], train_loss: 0.0000, val_loss: 5.7883, val_acc: 0.5790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [739], train_loss: 0.0000, val_loss: 6.0287, val_acc: 0.5682\n",
      "Epoch [740], train_loss: 0.0000, val_loss: 5.9170, val_acc: 0.5790\n",
      "Epoch [741], train_loss: 0.0000, val_loss: 6.0280, val_acc: 0.5819\n",
      "Epoch [742], train_loss: 0.0000, val_loss: 5.9830, val_acc: 0.5871\n",
      "Epoch [743], train_loss: 0.0000, val_loss: 5.9915, val_acc: 0.5737\n",
      "Epoch [744], train_loss: 0.0000, val_loss: 6.0449, val_acc: 0.5682\n",
      "Epoch [745], train_loss: 0.0000, val_loss: 6.0116, val_acc: 0.5763\n",
      "Epoch [746], train_loss: 0.0000, val_loss: 6.0407, val_acc: 0.5763\n",
      "Epoch [747], train_loss: 0.0000, val_loss: 5.9093, val_acc: 0.5819\n",
      "Epoch [748], train_loss: 0.0000, val_loss: 6.1191, val_acc: 0.5763\n",
      "Epoch [749], train_loss: 0.0000, val_loss: 6.0442, val_acc: 0.5737\n",
      "Epoch [750], train_loss: 0.0000, val_loss: 6.0801, val_acc: 0.5763\n",
      "Epoch [751], train_loss: 0.0000, val_loss: 6.2174, val_acc: 0.5763\n",
      "Epoch [752], train_loss: 0.0000, val_loss: 6.0175, val_acc: 0.5874\n",
      "Epoch [753], train_loss: 0.0000, val_loss: 6.0193, val_acc: 0.5819\n",
      "Epoch [754], train_loss: 0.0000, val_loss: 5.9259, val_acc: 0.5848\n",
      "Epoch [755], train_loss: 0.0000, val_loss: 6.1267, val_acc: 0.5793\n",
      "Epoch [756], train_loss: 0.0000, val_loss: 6.1047, val_acc: 0.5763\n",
      "Epoch [757], train_loss: 0.0001, val_loss: 5.8187, val_acc: 0.5819\n",
      "Epoch [758], train_loss: 0.0876, val_loss: 2.0839, val_acc: 0.5779\n",
      "Epoch [759], train_loss: 0.0718, val_loss: 2.4029, val_acc: 0.5805\n",
      "Epoch [760], train_loss: 0.0287, val_loss: 2.6308, val_acc: 0.6033\n",
      "Epoch [761], train_loss: 0.0220, val_loss: 3.3441, val_acc: 0.5903\n",
      "Epoch [762], train_loss: 0.0123, val_loss: 4.4220, val_acc: 0.5419\n",
      "Epoch [763], train_loss: 0.0040, val_loss: 4.0410, val_acc: 0.5477\n",
      "Epoch [764], train_loss: 0.0021, val_loss: 4.9231, val_acc: 0.5770\n",
      "Epoch [765], train_loss: 0.0007, val_loss: 4.7788, val_acc: 0.5662\n",
      "Epoch [766], train_loss: 0.0001, val_loss: 4.6942, val_acc: 0.5715\n",
      "Epoch [767], train_loss: 0.0001, val_loss: 4.8933, val_acc: 0.5767\n",
      "Epoch [768], train_loss: 0.0001, val_loss: 4.9969, val_acc: 0.5767\n",
      "Epoch [769], train_loss: 0.0001, val_loss: 5.3235, val_acc: 0.5741\n",
      "Epoch [770], train_loss: 0.0000, val_loss: 5.1816, val_acc: 0.5711\n",
      "Epoch [771], train_loss: 0.0019, val_loss: 5.2417, val_acc: 0.5685\n",
      "Epoch [772], train_loss: 0.0405, val_loss: 2.8189, val_acc: 0.5750\n",
      "Epoch [773], train_loss: 0.0301, val_loss: 3.5051, val_acc: 0.5802\n",
      "Epoch [774], train_loss: 0.0358, val_loss: 3.8231, val_acc: 0.5874\n",
      "Epoch [775], train_loss: 0.0157, val_loss: 3.6369, val_acc: 0.5692\n",
      "Epoch [776], train_loss: 0.0037, val_loss: 4.3258, val_acc: 0.5913\n",
      "Epoch [777], train_loss: 0.0038, val_loss: 4.3849, val_acc: 0.5750\n",
      "Epoch [778], train_loss: 0.0011, val_loss: 5.0356, val_acc: 0.5669\n",
      "Epoch [779], train_loss: 0.0003, val_loss: 5.4088, val_acc: 0.5802\n",
      "Epoch [780], train_loss: 0.0004, val_loss: 5.7156, val_acc: 0.5776\n",
      "Epoch [781], train_loss: 0.0003, val_loss: 5.8558, val_acc: 0.5828\n",
      "Epoch [782], train_loss: 0.0002, val_loss: 6.0382, val_acc: 0.5591\n",
      "Epoch [783], train_loss: 0.0001, val_loss: 6.0109, val_acc: 0.5750\n",
      "Epoch [784], train_loss: 0.0000, val_loss: 6.1472, val_acc: 0.5695\n",
      "Epoch [785], train_loss: 0.0000, val_loss: 6.1405, val_acc: 0.5776\n",
      "Epoch [786], train_loss: 0.0000, val_loss: 6.1329, val_acc: 0.5776\n",
      "Epoch [787], train_loss: 0.0000, val_loss: 6.2196, val_acc: 0.5802\n",
      "Epoch [788], train_loss: 0.0001, val_loss: 6.2764, val_acc: 0.5776\n",
      "Epoch [789], train_loss: 0.0001, val_loss: 6.1149, val_acc: 0.5643\n",
      "Epoch [790], train_loss: 0.0000, val_loss: 6.5441, val_acc: 0.5695\n",
      "Epoch [791], train_loss: 0.0000, val_loss: 6.4652, val_acc: 0.5695\n",
      "Epoch [792], train_loss: 0.0000, val_loss: 6.6294, val_acc: 0.5747\n",
      "Epoch [793], train_loss: 0.0000, val_loss: 6.6009, val_acc: 0.5721\n",
      "Epoch [794], train_loss: 0.0000, val_loss: 6.6793, val_acc: 0.5747\n",
      "Epoch [795], train_loss: 0.0000, val_loss: 6.6903, val_acc: 0.5695\n",
      "Epoch [796], train_loss: 0.0000, val_loss: 6.5975, val_acc: 0.5773\n",
      "Epoch [797], train_loss: 0.0000, val_loss: 6.7102, val_acc: 0.5747\n",
      "Epoch [798], train_loss: 0.0000, val_loss: 6.7935, val_acc: 0.5695\n",
      "Epoch [799], train_loss: 0.0000, val_loss: 6.6030, val_acc: 0.5747\n",
      "Epoch [800], train_loss: 0.0000, val_loss: 6.8268, val_acc: 0.5695\n",
      "Epoch [801], train_loss: 0.0000, val_loss: 6.6369, val_acc: 0.5721\n",
      "Epoch [802], train_loss: 0.0003, val_loss: 6.5775, val_acc: 0.5558\n",
      "Epoch [803], train_loss: 0.0019, val_loss: 6.0136, val_acc: 0.5610\n",
      "Epoch [804], train_loss: 0.0065, val_loss: 4.9692, val_acc: 0.5610\n",
      "Epoch [805], train_loss: 0.0382, val_loss: 2.2050, val_acc: 0.5633\n",
      "Epoch [806], train_loss: 0.0227, val_loss: 4.2991, val_acc: 0.5558\n",
      "Epoch [807], train_loss: 0.0124, val_loss: 3.2789, val_acc: 0.6011\n",
      "Epoch [808], train_loss: 0.0029, val_loss: 4.2307, val_acc: 0.6037\n",
      "Epoch [809], train_loss: 0.0003, val_loss: 4.7568, val_acc: 0.5985\n",
      "Epoch [810], train_loss: 0.0002, val_loss: 4.8605, val_acc: 0.6011\n",
      "Epoch [811], train_loss: 0.0001, val_loss: 5.0631, val_acc: 0.5903\n",
      "Epoch [812], train_loss: 0.0001, val_loss: 5.1366, val_acc: 0.5929\n",
      "Epoch [813], train_loss: 0.0000, val_loss: 5.2811, val_acc: 0.5877\n",
      "Epoch [814], train_loss: 0.0000, val_loss: 5.1902, val_acc: 0.5877\n",
      "Epoch [815], train_loss: 0.0000, val_loss: 5.3652, val_acc: 0.5825\n",
      "Epoch [816], train_loss: 0.0000, val_loss: 5.4156, val_acc: 0.5822\n",
      "Epoch [817], train_loss: 0.0000, val_loss: 5.4075, val_acc: 0.5851\n",
      "Epoch [818], train_loss: 0.0000, val_loss: 5.4400, val_acc: 0.5822\n",
      "Epoch [819], train_loss: 0.0000, val_loss: 5.4120, val_acc: 0.5822\n",
      "Epoch [820], train_loss: 0.0000, val_loss: 5.5872, val_acc: 0.5796\n",
      "Epoch [821], train_loss: 0.0000, val_loss: 5.5273, val_acc: 0.5848\n",
      "Epoch [822], train_loss: 0.0000, val_loss: 5.5644, val_acc: 0.5770\n",
      "Epoch [823], train_loss: 0.0000, val_loss: 5.7194, val_acc: 0.5929\n",
      "Epoch [824], train_loss: 0.0000, val_loss: 5.6213, val_acc: 0.5796\n",
      "Epoch [825], train_loss: 0.0000, val_loss: 5.7596, val_acc: 0.5796\n",
      "Epoch [826], train_loss: 0.0000, val_loss: 5.7755, val_acc: 0.5796\n",
      "Epoch [827], train_loss: 0.0000, val_loss: 5.7900, val_acc: 0.5822\n",
      "Epoch [828], train_loss: 0.0000, val_loss: 5.7755, val_acc: 0.5796\n",
      "Epoch [829], train_loss: 0.0000, val_loss: 5.8822, val_acc: 0.5770\n",
      "Epoch [830], train_loss: 0.0000, val_loss: 5.9021, val_acc: 0.5796\n",
      "Epoch [831], train_loss: 0.0000, val_loss: 5.9187, val_acc: 0.5770\n",
      "Epoch [832], train_loss: 0.0000, val_loss: 5.9146, val_acc: 0.5796\n",
      "Epoch [833], train_loss: 0.0000, val_loss: 5.9325, val_acc: 0.5903\n",
      "Epoch [834], train_loss: 0.0000, val_loss: 5.9778, val_acc: 0.5796\n",
      "Epoch [835], train_loss: 0.0000, val_loss: 5.9360, val_acc: 0.5903\n",
      "Epoch [836], train_loss: 0.0000, val_loss: 6.1315, val_acc: 0.5796\n",
      "Epoch [837], train_loss: 0.0000, val_loss: 6.1112, val_acc: 0.5796\n",
      "Epoch [838], train_loss: 0.0000, val_loss: 6.0015, val_acc: 0.5848\n",
      "Epoch [839], train_loss: 0.0000, val_loss: 6.0971, val_acc: 0.5770\n",
      "Epoch [840], train_loss: 0.0000, val_loss: 6.1142, val_acc: 0.5796\n",
      "Epoch [841], train_loss: 0.0000, val_loss: 6.1207, val_acc: 0.5718\n",
      "Epoch [842], train_loss: 0.0000, val_loss: 6.1536, val_acc: 0.5796\n",
      "Epoch [843], train_loss: 0.0000, val_loss: 6.4048, val_acc: 0.5744\n",
      "Epoch [844], train_loss: 0.0007, val_loss: 6.9149, val_acc: 0.5747\n",
      "Epoch [845], train_loss: 0.0499, val_loss: 2.7871, val_acc: 0.6238\n",
      "Epoch [846], train_loss: 0.0182, val_loss: 3.9616, val_acc: 0.5614\n",
      "Epoch [847], train_loss: 0.0152, val_loss: 5.4471, val_acc: 0.5614\n",
      "Epoch [848], train_loss: 0.0123, val_loss: 4.2576, val_acc: 0.5558\n",
      "Epoch [849], train_loss: 0.0096, val_loss: 4.4060, val_acc: 0.5831\n",
      "Epoch [850], train_loss: 0.0027, val_loss: 5.2089, val_acc: 0.5845\n",
      "Epoch [851], train_loss: 0.0005, val_loss: 6.0257, val_acc: 0.5799\n",
      "Epoch [852], train_loss: 0.0001, val_loss: 6.0535, val_acc: 0.5906\n",
      "Epoch [853], train_loss: 0.0000, val_loss: 6.1546, val_acc: 0.5880\n",
      "Epoch [854], train_loss: 0.0000, val_loss: 6.2417, val_acc: 0.5692\n",
      "Epoch [855], train_loss: 0.0001, val_loss: 6.3281, val_acc: 0.5666\n",
      "Epoch [856], train_loss: 0.0000, val_loss: 6.3216, val_acc: 0.5584\n",
      "Epoch [857], train_loss: 0.0000, val_loss: 6.5632, val_acc: 0.5692\n",
      "Epoch [858], train_loss: 0.0000, val_loss: 6.4658, val_acc: 0.5610\n",
      "Epoch [859], train_loss: 0.0000, val_loss: 6.5694, val_acc: 0.5666\n",
      "Epoch [860], train_loss: 0.0000, val_loss: 6.4861, val_acc: 0.5610\n",
      "Epoch [861], train_loss: 0.0000, val_loss: 6.4578, val_acc: 0.5822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [862], train_loss: 0.0000, val_loss: 6.6879, val_acc: 0.5692\n",
      "Epoch [863], train_loss: 0.0000, val_loss: 6.6246, val_acc: 0.5636\n",
      "Epoch [864], train_loss: 0.0016, val_loss: 6.2491, val_acc: 0.5718\n",
      "Epoch [865], train_loss: 0.0234, val_loss: 3.7448, val_acc: 0.5584\n",
      "Epoch [866], train_loss: 0.0071, val_loss: 4.0023, val_acc: 0.5767\n",
      "Epoch [867], train_loss: 0.0185, val_loss: 2.7352, val_acc: 0.5741\n",
      "Epoch [868], train_loss: 0.0277, val_loss: 3.0124, val_acc: 0.6414\n",
      "Epoch [869], train_loss: 0.0102, val_loss: 4.3315, val_acc: 0.5877\n",
      "Epoch [870], train_loss: 0.0048, val_loss: 4.4665, val_acc: 0.6121\n",
      "Epoch [871], train_loss: 0.0108, val_loss: 6.8469, val_acc: 0.5643\n",
      "Epoch [872], train_loss: 0.0331, val_loss: 2.8734, val_acc: 0.5939\n",
      "Epoch [873], train_loss: 0.0024, val_loss: 4.2724, val_acc: 0.5695\n",
      "Epoch [874], train_loss: 0.0014, val_loss: 4.5017, val_acc: 0.5636\n",
      "Epoch [875], train_loss: 0.0003, val_loss: 4.7049, val_acc: 0.5662\n",
      "Epoch [876], train_loss: 0.0001, val_loss: 4.9130, val_acc: 0.5662\n",
      "Epoch [877], train_loss: 0.0001, val_loss: 4.9541, val_acc: 0.5662\n",
      "Epoch [878], train_loss: 0.0000, val_loss: 5.1552, val_acc: 0.5718\n",
      "Epoch [879], train_loss: 0.0000, val_loss: 5.0707, val_acc: 0.5718\n",
      "Epoch [880], train_loss: 0.0000, val_loss: 5.0761, val_acc: 0.5799\n",
      "Epoch [881], train_loss: 0.0000, val_loss: 5.3100, val_acc: 0.5851\n",
      "Epoch [882], train_loss: 0.0000, val_loss: 5.5054, val_acc: 0.5825\n",
      "Epoch [883], train_loss: 0.0000, val_loss: 5.3055, val_acc: 0.5689\n",
      "Epoch [884], train_loss: 0.0000, val_loss: 5.6747, val_acc: 0.5799\n",
      "Epoch [885], train_loss: 0.0000, val_loss: 5.5028, val_acc: 0.5770\n",
      "Epoch [886], train_loss: 0.0000, val_loss: 5.4571, val_acc: 0.5770\n",
      "Epoch [887], train_loss: 0.0000, val_loss: 5.4251, val_acc: 0.5770\n",
      "Epoch [888], train_loss: 0.0000, val_loss: 5.5494, val_acc: 0.5770\n",
      "Epoch [889], train_loss: 0.0000, val_loss: 5.5576, val_acc: 0.5715\n",
      "Epoch [890], train_loss: 0.0000, val_loss: 5.4093, val_acc: 0.5689\n",
      "Epoch [891], train_loss: 0.0000, val_loss: 5.6927, val_acc: 0.5744\n",
      "Epoch [892], train_loss: 0.0000, val_loss: 5.6296, val_acc: 0.5744\n",
      "Epoch [893], train_loss: 0.0000, val_loss: 5.8441, val_acc: 0.5744\n",
      "Epoch [894], train_loss: 0.0000, val_loss: 5.9249, val_acc: 0.5636\n",
      "Epoch [895], train_loss: 0.0000, val_loss: 5.8955, val_acc: 0.5555\n",
      "Epoch [896], train_loss: 0.0000, val_loss: 5.8706, val_acc: 0.5796\n",
      "Epoch [897], train_loss: 0.0000, val_loss: 5.7756, val_acc: 0.5715\n",
      "Epoch [898], train_loss: 0.0000, val_loss: 5.8471, val_acc: 0.5770\n",
      "Epoch [899], train_loss: 0.0000, val_loss: 5.9301, val_acc: 0.5796\n",
      "Epoch [900], train_loss: 0.0000, val_loss: 5.8313, val_acc: 0.5636\n",
      "Epoch [901], train_loss: 0.0000, val_loss: 5.7855, val_acc: 0.5715\n",
      "Epoch [902], train_loss: 0.0000, val_loss: 6.0297, val_acc: 0.5529\n",
      "Epoch [903], train_loss: 0.0000, val_loss: 5.9249, val_acc: 0.5715\n",
      "Epoch [904], train_loss: 0.0000, val_loss: 5.8632, val_acc: 0.5636\n",
      "Epoch [905], train_loss: 0.0000, val_loss: 5.8117, val_acc: 0.5770\n",
      "Epoch [906], train_loss: 0.0000, val_loss: 6.0717, val_acc: 0.5689\n",
      "Epoch [907], train_loss: 0.0000, val_loss: 6.0480, val_acc: 0.5770\n",
      "Epoch [908], train_loss: 0.0000, val_loss: 6.0108, val_acc: 0.5796\n",
      "Epoch [909], train_loss: 0.0000, val_loss: 6.1865, val_acc: 0.5718\n",
      "Epoch [910], train_loss: 0.0000, val_loss: 5.9072, val_acc: 0.5636\n",
      "Epoch [911], train_loss: 0.0000, val_loss: 6.2563, val_acc: 0.5584\n",
      "Epoch [912], train_loss: 0.0000, val_loss: 6.2075, val_acc: 0.5610\n",
      "Epoch [913], train_loss: 0.0000, val_loss: 6.2915, val_acc: 0.5558\n",
      "Epoch [914], train_loss: 0.0000, val_loss: 6.2355, val_acc: 0.5666\n",
      "Epoch [915], train_loss: 0.0000, val_loss: 6.2292, val_acc: 0.5610\n",
      "Epoch [916], train_loss: 0.0000, val_loss: 6.1569, val_acc: 0.5692\n",
      "Epoch [917], train_loss: 0.0000, val_loss: 6.2159, val_acc: 0.5692\n",
      "Epoch [918], train_loss: 0.0000, val_loss: 6.2648, val_acc: 0.5692\n",
      "Epoch [919], train_loss: 0.0000, val_loss: 6.1609, val_acc: 0.5584\n",
      "Epoch [920], train_loss: 0.0000, val_loss: 6.2148, val_acc: 0.5558\n",
      "Epoch [921], train_loss: 0.0000, val_loss: 6.2576, val_acc: 0.5610\n",
      "Epoch [922], train_loss: 0.0000, val_loss: 6.3683, val_acc: 0.5692\n",
      "Epoch [923], train_loss: 0.0000, val_loss: 6.3407, val_acc: 0.5692\n",
      "Epoch [924], train_loss: 0.0000, val_loss: 6.4049, val_acc: 0.5692\n",
      "Epoch [925], train_loss: 0.0000, val_loss: 6.3347, val_acc: 0.5692\n",
      "Epoch [926], train_loss: 0.0000, val_loss: 6.4731, val_acc: 0.5558\n",
      "Epoch [927], train_loss: 0.0000, val_loss: 6.4375, val_acc: 0.5640\n",
      "Epoch [928], train_loss: 0.0000, val_loss: 6.4356, val_acc: 0.5666\n",
      "Epoch [929], train_loss: 0.0000, val_loss: 6.4092, val_acc: 0.5666\n",
      "Epoch [930], train_loss: 0.0000, val_loss: 6.6831, val_acc: 0.5532\n",
      "Epoch [931], train_loss: 0.0003, val_loss: 6.5725, val_acc: 0.5718\n",
      "Epoch [932], train_loss: 0.0447, val_loss: 3.2372, val_acc: 0.5730\n",
      "Epoch [933], train_loss: 0.0644, val_loss: 3.5976, val_acc: 0.5588\n",
      "Epoch [934], train_loss: 0.0134, val_loss: 4.8368, val_acc: 0.5828\n",
      "Epoch [935], train_loss: 0.0103, val_loss: 3.6062, val_acc: 0.5880\n",
      "Epoch [936], train_loss: 0.0074, val_loss: 4.3449, val_acc: 0.5724\n",
      "Epoch [937], train_loss: 0.0019, val_loss: 4.8655, val_acc: 0.5744\n",
      "Epoch [938], train_loss: 0.0013, val_loss: 5.6029, val_acc: 0.5799\n",
      "Epoch [939], train_loss: 0.0006, val_loss: 5.9646, val_acc: 0.5669\n",
      "Epoch [940], train_loss: 0.0011, val_loss: 6.3684, val_acc: 0.5561\n",
      "Epoch [941], train_loss: 0.0001, val_loss: 6.5903, val_acc: 0.5561\n",
      "Epoch [942], train_loss: 0.0000, val_loss: 6.5752, val_acc: 0.5588\n",
      "Epoch [943], train_loss: 0.0000, val_loss: 6.7016, val_acc: 0.5724\n",
      "Epoch [944], train_loss: 0.0000, val_loss: 6.4751, val_acc: 0.5695\n",
      "Epoch [945], train_loss: 0.0000, val_loss: 6.6005, val_acc: 0.5721\n",
      "Epoch [946], train_loss: 0.0000, val_loss: 6.5732, val_acc: 0.5669\n",
      "Epoch [947], train_loss: 0.0000, val_loss: 6.6674, val_acc: 0.5776\n",
      "Epoch [948], train_loss: 0.0000, val_loss: 6.6639, val_acc: 0.5750\n",
      "Epoch [949], train_loss: 0.0000, val_loss: 6.5820, val_acc: 0.5724\n",
      "Epoch [950], train_loss: 0.0000, val_loss: 6.7404, val_acc: 0.5802\n",
      "Epoch [951], train_loss: 0.0000, val_loss: 6.6179, val_acc: 0.5721\n",
      "Epoch [952], train_loss: 0.0000, val_loss: 6.7975, val_acc: 0.5802\n",
      "Epoch [953], train_loss: 0.0000, val_loss: 6.6270, val_acc: 0.5747\n",
      "Epoch [954], train_loss: 0.0000, val_loss: 6.8386, val_acc: 0.5721\n",
      "Epoch [955], train_loss: 0.0000, val_loss: 6.5625, val_acc: 0.5747\n",
      "Epoch [956], train_loss: 0.0000, val_loss: 6.8069, val_acc: 0.5721\n",
      "Epoch [957], train_loss: 0.0000, val_loss: 6.7127, val_acc: 0.5802\n",
      "Epoch [958], train_loss: 0.0000, val_loss: 6.8057, val_acc: 0.5695\n",
      "Epoch [959], train_loss: 0.0000, val_loss: 6.9152, val_acc: 0.5802\n",
      "Epoch [960], train_loss: 0.0000, val_loss: 6.8546, val_acc: 0.5721\n",
      "Epoch [961], train_loss: 0.0000, val_loss: 6.9829, val_acc: 0.5721\n",
      "Epoch [962], train_loss: 0.0000, val_loss: 6.9258, val_acc: 0.5721\n",
      "Epoch [963], train_loss: 0.0000, val_loss: 6.9882, val_acc: 0.5721\n",
      "Epoch [964], train_loss: 0.0000, val_loss: 6.8531, val_acc: 0.5802\n",
      "Epoch [965], train_loss: 0.0000, val_loss: 6.8859, val_acc: 0.5721\n",
      "Epoch [966], train_loss: 0.0000, val_loss: 7.0500, val_acc: 0.5747\n",
      "Epoch [967], train_loss: 0.0000, val_loss: 7.1215, val_acc: 0.5721\n",
      "Epoch [968], train_loss: 0.0000, val_loss: 7.0895, val_acc: 0.5695\n",
      "Epoch [969], train_loss: 0.0000, val_loss: 7.1273, val_acc: 0.5721\n",
      "Epoch [970], train_loss: 0.0000, val_loss: 7.2487, val_acc: 0.5721\n",
      "Epoch [971], train_loss: 0.0000, val_loss: 7.1327, val_acc: 0.5695\n",
      "Epoch [972], train_loss: 0.0000, val_loss: 6.9927, val_acc: 0.5721\n",
      "Epoch [973], train_loss: 0.0000, val_loss: 7.2025, val_acc: 0.5721\n",
      "Epoch [974], train_loss: 0.0000, val_loss: 7.0994, val_acc: 0.5695\n",
      "Epoch [975], train_loss: 0.0000, val_loss: 7.0130, val_acc: 0.5695\n",
      "Epoch [976], train_loss: 0.0000, val_loss: 7.0526, val_acc: 0.5640\n",
      "Epoch [977], train_loss: 0.0000, val_loss: 7.1796, val_acc: 0.5695\n",
      "Epoch [978], train_loss: 0.0000, val_loss: 7.2164, val_acc: 0.5695\n",
      "Epoch [979], train_loss: 0.0000, val_loss: 7.1495, val_acc: 0.5669\n",
      "Epoch [980], train_loss: 0.0000, val_loss: 7.2181, val_acc: 0.5669\n",
      "Epoch [981], train_loss: 0.0000, val_loss: 7.4814, val_acc: 0.5695\n",
      "Epoch [982], train_loss: 0.0000, val_loss: 7.2340, val_acc: 0.5640\n",
      "Epoch [983], train_loss: 0.0000, val_loss: 7.2393, val_acc: 0.5640\n",
      "Epoch [984], train_loss: 0.0000, val_loss: 7.3360, val_acc: 0.5614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [985], train_loss: 0.0000, val_loss: 7.5315, val_acc: 0.5721\n",
      "Epoch [986], train_loss: 0.0000, val_loss: 7.3468, val_acc: 0.5614\n",
      "Epoch [987], train_loss: 0.0000, val_loss: 7.3919, val_acc: 0.5721\n",
      "Epoch [988], train_loss: 0.0000, val_loss: 7.4170, val_acc: 0.5747\n",
      "Epoch [989], train_loss: 0.0000, val_loss: 7.2409, val_acc: 0.5640\n",
      "Epoch [990], train_loss: 0.0000, val_loss: 7.4757, val_acc: 0.5614\n",
      "Epoch [991], train_loss: 0.0000, val_loss: 7.4827, val_acc: 0.5614\n",
      "Epoch [992], train_loss: 0.0000, val_loss: 7.4904, val_acc: 0.5721\n",
      "Epoch [993], train_loss: 0.0090, val_loss: 7.1388, val_acc: 0.5506\n",
      "Epoch [994], train_loss: 0.0969, val_loss: 1.6974, val_acc: 0.6092\n",
      "Epoch [995], train_loss: 0.0423, val_loss: 2.5834, val_acc: 0.5776\n",
      "Epoch [996], train_loss: 0.0218, val_loss: 3.0171, val_acc: 0.6063\n",
      "Epoch [997], train_loss: 0.0098, val_loss: 3.0883, val_acc: 0.6040\n",
      "Epoch [998], train_loss: 0.0068, val_loss: 3.9612, val_acc: 0.6092\n",
      "Epoch [999], train_loss: 0.0008, val_loss: 4.4010, val_acc: 0.6063\n",
      "Epoch [1000], train_loss: 0.0002, val_loss: 4.4940, val_acc: 0.6199\n",
      "Epoch [1001], train_loss: 0.0001, val_loss: 4.7141, val_acc: 0.6066\n",
      "Epoch [1002], train_loss: 0.0001, val_loss: 4.7086, val_acc: 0.6092\n",
      "Epoch [1003], train_loss: 0.0002, val_loss: 4.7281, val_acc: 0.5721\n",
      "Epoch [1004], train_loss: 0.0001, val_loss: 4.9701, val_acc: 0.5877\n",
      "Epoch [1005], train_loss: 0.0000, val_loss: 5.0470, val_acc: 0.6092\n",
      "Epoch [1006], train_loss: 0.0000, val_loss: 5.0579, val_acc: 0.6066\n",
      "Epoch [1007], train_loss: 0.0000, val_loss: 5.1974, val_acc: 0.6014\n",
      "Epoch [1008], train_loss: 0.0000, val_loss: 5.2355, val_acc: 0.6118\n",
      "Epoch [1009], train_loss: 0.0000, val_loss: 5.3047, val_acc: 0.6121\n",
      "Epoch [1010], train_loss: 0.0000, val_loss: 5.3131, val_acc: 0.6040\n",
      "Epoch [1011], train_loss: 0.0000, val_loss: 5.3006, val_acc: 0.6040\n",
      "Epoch [1012], train_loss: 0.0000, val_loss: 5.3470, val_acc: 0.6066\n",
      "Epoch [1013], train_loss: 0.0001, val_loss: 5.3610, val_acc: 0.6095\n",
      "Epoch [1014], train_loss: 0.0000, val_loss: 5.5233, val_acc: 0.6066\n",
      "Epoch [1015], train_loss: 0.0000, val_loss: 5.6424, val_acc: 0.6121\n",
      "Epoch [1016], train_loss: 0.0000, val_loss: 5.4856, val_acc: 0.6066\n",
      "Epoch [1017], train_loss: 0.0000, val_loss: 5.5298, val_acc: 0.6147\n",
      "Epoch [1018], train_loss: 0.0000, val_loss: 5.5352, val_acc: 0.6199\n",
      "Epoch [1019], train_loss: 0.0000, val_loss: 5.6488, val_acc: 0.6147\n",
      "Epoch [1020], train_loss: 0.0000, val_loss: 5.8055, val_acc: 0.6147\n",
      "Epoch [1021], train_loss: 0.0000, val_loss: 5.6470, val_acc: 0.6147\n",
      "Epoch [1022], train_loss: 0.0000, val_loss: 5.6839, val_acc: 0.6147\n",
      "Epoch [1023], train_loss: 0.0000, val_loss: 5.8243, val_acc: 0.6066\n",
      "Epoch [1024], train_loss: 0.0000, val_loss: 5.6742, val_acc: 0.6121\n",
      "Epoch [1025], train_loss: 0.0000, val_loss: 5.7706, val_acc: 0.6173\n",
      "Epoch [1026], train_loss: 0.0000, val_loss: 5.8830, val_acc: 0.6066\n",
      "Epoch [1027], train_loss: 0.0000, val_loss: 5.7674, val_acc: 0.6147\n",
      "Epoch [1028], train_loss: 0.0011, val_loss: 5.7683, val_acc: 0.6089\n",
      "Epoch [1029], train_loss: 0.0323, val_loss: 2.8063, val_acc: 0.6121\n",
      "Epoch [1030], train_loss: 0.0142, val_loss: 3.4318, val_acc: 0.6095\n",
      "Epoch [1031], train_loss: 0.0038, val_loss: 4.4600, val_acc: 0.5877\n",
      "Epoch [1032], train_loss: 0.0023, val_loss: 4.5017, val_acc: 0.5770\n",
      "Epoch [1033], train_loss: 0.0248, val_loss: 4.6547, val_acc: 0.5689\n",
      "Epoch [1034], train_loss: 0.0259, val_loss: 2.2622, val_acc: 0.6037\n",
      "Epoch [1035], train_loss: 0.0077, val_loss: 3.4034, val_acc: 0.5903\n",
      "Epoch [1036], train_loss: 0.0013, val_loss: 4.0253, val_acc: 0.5926\n",
      "Epoch [1037], train_loss: 0.0003, val_loss: 4.2074, val_acc: 0.6300\n",
      "Epoch [1038], train_loss: 0.0001, val_loss: 4.3015, val_acc: 0.6196\n",
      "Epoch [1039], train_loss: 0.0001, val_loss: 4.5045, val_acc: 0.5981\n",
      "Epoch [1040], train_loss: 0.0000, val_loss: 4.6467, val_acc: 0.5874\n",
      "Epoch [1041], train_loss: 0.0001, val_loss: 4.6135, val_acc: 0.5929\n",
      "Epoch [1042], train_loss: 0.0000, val_loss: 4.8211, val_acc: 0.5848\n",
      "Epoch [1043], train_loss: 0.0000, val_loss: 4.8913, val_acc: 0.5955\n",
      "Epoch [1044], train_loss: 0.0006, val_loss: 4.9359, val_acc: 0.5874\n",
      "Epoch [1045], train_loss: 0.0005, val_loss: 6.1176, val_acc: 0.6063\n",
      "Epoch [1046], train_loss: 0.0001, val_loss: 6.1907, val_acc: 0.5822\n",
      "Epoch [1047], train_loss: 0.0000, val_loss: 6.2680, val_acc: 0.5822\n",
      "Epoch [1048], train_loss: 0.0000, val_loss: 6.1887, val_acc: 0.5796\n",
      "Epoch [1049], train_loss: 0.0000, val_loss: 6.2018, val_acc: 0.5796\n",
      "Epoch [1050], train_loss: 0.0000, val_loss: 6.4768, val_acc: 0.5822\n",
      "Epoch [1051], train_loss: 0.0000, val_loss: 6.3681, val_acc: 0.5822\n",
      "Epoch [1052], train_loss: 0.0146, val_loss: 6.2480, val_acc: 0.6037\n",
      "Epoch [1053], train_loss: 0.0373, val_loss: 2.6324, val_acc: 0.5539\n",
      "Epoch [1054], train_loss: 0.0120, val_loss: 3.3824, val_acc: 0.5910\n",
      "Epoch [1055], train_loss: 0.0012, val_loss: 3.8907, val_acc: 0.5828\n",
      "Epoch [1056], train_loss: 0.0005, val_loss: 4.3240, val_acc: 0.5884\n",
      "Epoch [1057], train_loss: 0.0001, val_loss: 4.3752, val_acc: 0.5828\n",
      "Epoch [1058], train_loss: 0.0001, val_loss: 4.4433, val_acc: 0.5828\n",
      "Epoch [1059], train_loss: 0.0001, val_loss: 4.4777, val_acc: 0.5884\n",
      "Epoch [1060], train_loss: 0.0000, val_loss: 4.6369, val_acc: 0.5857\n",
      "Epoch [1061], train_loss: 0.0000, val_loss: 4.5038, val_acc: 0.5828\n",
      "Epoch [1062], train_loss: 0.0180, val_loss: 4.7722, val_acc: 0.5770\n",
      "Epoch [1063], train_loss: 0.0272, val_loss: 2.8558, val_acc: 0.5991\n",
      "Epoch [1064], train_loss: 0.0055, val_loss: 3.8105, val_acc: 0.6011\n",
      "Epoch [1065], train_loss: 0.0005, val_loss: 4.1161, val_acc: 0.5880\n",
      "Epoch [1066], train_loss: 0.0002, val_loss: 4.3090, val_acc: 0.5825\n",
      "Epoch [1067], train_loss: 0.0001, val_loss: 4.5411, val_acc: 0.5718\n",
      "Epoch [1068], train_loss: 0.0001, val_loss: 4.7252, val_acc: 0.5744\n",
      "Epoch [1069], train_loss: 0.0000, val_loss: 4.7937, val_acc: 0.5770\n",
      "Epoch [1070], train_loss: 0.0001, val_loss: 4.6554, val_acc: 0.5744\n",
      "Epoch [1071], train_loss: 0.0000, val_loss: 4.8298, val_acc: 0.5744\n",
      "Epoch [1072], train_loss: 0.0000, val_loss: 4.8004, val_acc: 0.5718\n",
      "Epoch [1073], train_loss: 0.0000, val_loss: 4.8796, val_acc: 0.5718\n",
      "Epoch [1074], train_loss: 0.0000, val_loss: 5.1158, val_acc: 0.5822\n",
      "Epoch [1075], train_loss: 0.0000, val_loss: 5.1158, val_acc: 0.5744\n",
      "Epoch [1076], train_loss: 0.0000, val_loss: 5.0496, val_acc: 0.5744\n",
      "Epoch [1077], train_loss: 0.0000, val_loss: 4.9872, val_acc: 0.5770\n",
      "Epoch [1078], train_loss: 0.0000, val_loss: 5.1864, val_acc: 0.5796\n",
      "Epoch [1079], train_loss: 0.0000, val_loss: 5.2185, val_acc: 0.5718\n",
      "Epoch [1080], train_loss: 0.0000, val_loss: 5.2938, val_acc: 0.5770\n",
      "Epoch [1081], train_loss: 0.0000, val_loss: 5.2558, val_acc: 0.5744\n",
      "Epoch [1082], train_loss: 0.0000, val_loss: 5.2878, val_acc: 0.5822\n",
      "Epoch [1083], train_loss: 0.0000, val_loss: 5.3440, val_acc: 0.5796\n",
      "Epoch [1084], train_loss: 0.0000, val_loss: 5.4734, val_acc: 0.5822\n",
      "Epoch [1085], train_loss: 0.0000, val_loss: 5.3708, val_acc: 0.5744\n",
      "Epoch [1086], train_loss: 0.0000, val_loss: 5.2841, val_acc: 0.5744\n",
      "Epoch [1087], train_loss: 0.0000, val_loss: 5.3717, val_acc: 0.5718\n",
      "Epoch [1088], train_loss: 0.0000, val_loss: 5.4375, val_acc: 0.5822\n",
      "Epoch [1089], train_loss: 0.0000, val_loss: 5.4758, val_acc: 0.5744\n",
      "Epoch [1090], train_loss: 0.0000, val_loss: 5.4738, val_acc: 0.5770\n",
      "Epoch [1091], train_loss: 0.0000, val_loss: 5.5030, val_acc: 0.5744\n",
      "Epoch [1092], train_loss: 0.0000, val_loss: 5.5013, val_acc: 0.5822\n",
      "Epoch [1093], train_loss: 0.0000, val_loss: 5.5146, val_acc: 0.5796\n",
      "Epoch [1094], train_loss: 0.0141, val_loss: 5.2229, val_acc: 0.5799\n",
      "Epoch [1095], train_loss: 0.0403, val_loss: 3.0283, val_acc: 0.5890\n",
      "Epoch [1096], train_loss: 0.0201, val_loss: 4.3472, val_acc: 0.6066\n",
      "Epoch [1097], train_loss: 0.0079, val_loss: 4.3118, val_acc: 0.5802\n",
      "Epoch [1098], train_loss: 0.0079, val_loss: 4.1564, val_acc: 0.5448\n",
      "Epoch [1099], train_loss: 0.0080, val_loss: 5.0029, val_acc: 0.5344\n",
      "Epoch [1100], train_loss: 0.0020, val_loss: 5.7465, val_acc: 0.5503\n",
      "Epoch [1101], train_loss: 0.0002, val_loss: 5.7643, val_acc: 0.5454\n",
      "Epoch [1102], train_loss: 0.0083, val_loss: 5.8340, val_acc: 0.5532\n",
      "Epoch [1103], train_loss: 0.0134, val_loss: 3.4387, val_acc: 0.5799\n",
      "Epoch [1104], train_loss: 0.0039, val_loss: 4.2387, val_acc: 0.5770\n",
      "Epoch [1105], train_loss: 0.0031, val_loss: 4.3666, val_acc: 0.5477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1106], train_loss: 0.0042, val_loss: 4.0971, val_acc: 0.5610\n",
      "Epoch [1107], train_loss: 0.0002, val_loss: 4.5113, val_acc: 0.5581\n",
      "Epoch [1108], train_loss: 0.0001, val_loss: 4.5752, val_acc: 0.5581\n",
      "Epoch [1109], train_loss: 0.0001, val_loss: 4.7067, val_acc: 0.5581\n",
      "Epoch [1110], train_loss: 0.0001, val_loss: 4.9366, val_acc: 0.5581\n",
      "Epoch [1111], train_loss: 0.0000, val_loss: 4.8753, val_acc: 0.5555\n",
      "Epoch [1112], train_loss: 0.0000, val_loss: 4.9319, val_acc: 0.5529\n",
      "Epoch [1113], train_loss: 0.0000, val_loss: 5.0511, val_acc: 0.5555\n",
      "Epoch [1114], train_loss: 0.0000, val_loss: 5.0351, val_acc: 0.5529\n",
      "Epoch [1115], train_loss: 0.0000, val_loss: 5.1253, val_acc: 0.5555\n",
      "Epoch [1116], train_loss: 0.0000, val_loss: 5.1243, val_acc: 0.5503\n",
      "Epoch [1117], train_loss: 0.0000, val_loss: 5.2055, val_acc: 0.5529\n",
      "Epoch [1118], train_loss: 0.0000, val_loss: 5.2998, val_acc: 0.5555\n",
      "Epoch [1119], train_loss: 0.0000, val_loss: 5.3215, val_acc: 0.5477\n",
      "Epoch [1120], train_loss: 0.0000, val_loss: 5.3010, val_acc: 0.5503\n",
      "Epoch [1121], train_loss: 0.0000, val_loss: 5.4299, val_acc: 0.5503\n",
      "Epoch [1122], train_loss: 0.0000, val_loss: 5.3955, val_acc: 0.5584\n",
      "Epoch [1123], train_loss: 0.0000, val_loss: 5.5212, val_acc: 0.5503\n",
      "Epoch [1124], train_loss: 0.0000, val_loss: 5.4610, val_acc: 0.5503\n",
      "Epoch [1125], train_loss: 0.0000, val_loss: 5.5110, val_acc: 0.5584\n",
      "Epoch [1126], train_loss: 0.0000, val_loss: 5.5502, val_acc: 0.5477\n",
      "Epoch [1127], train_loss: 0.0000, val_loss: 5.5332, val_acc: 0.5503\n",
      "Epoch [1128], train_loss: 0.0000, val_loss: 5.7330, val_acc: 0.5503\n",
      "Epoch [1129], train_loss: 0.0000, val_loss: 5.6258, val_acc: 0.5503\n",
      "Epoch [1130], train_loss: 0.0000, val_loss: 5.7048, val_acc: 0.5529\n",
      "Epoch [1131], train_loss: 0.0000, val_loss: 5.7397, val_acc: 0.5555\n",
      "Epoch [1132], train_loss: 0.0192, val_loss: 5.7887, val_acc: 0.5773\n",
      "Epoch [1133], train_loss: 0.0373, val_loss: 4.6205, val_acc: 0.5350\n",
      "Epoch [1134], train_loss: 0.0083, val_loss: 4.5627, val_acc: 0.5695\n",
      "Epoch [1135], train_loss: 0.0157, val_loss: 3.5235, val_acc: 0.5584\n",
      "Epoch [1136], train_loss: 0.0094, val_loss: 4.5052, val_acc: 0.5662\n",
      "Epoch [1137], train_loss: 0.0040, val_loss: 4.3275, val_acc: 0.5854\n",
      "Epoch [1138], train_loss: 0.0012, val_loss: 4.8489, val_acc: 0.5636\n",
      "Epoch [1139], train_loss: 0.0006, val_loss: 4.8053, val_acc: 0.5636\n",
      "Epoch [1140], train_loss: 0.0005, val_loss: 4.6942, val_acc: 0.5854\n",
      "Epoch [1141], train_loss: 0.0003, val_loss: 5.0252, val_acc: 0.5799\n",
      "Epoch [1142], train_loss: 0.0001, val_loss: 5.1263, val_acc: 0.5822\n",
      "Epoch [1143], train_loss: 0.0000, val_loss: 5.3012, val_acc: 0.5744\n",
      "Epoch [1144], train_loss: 0.0000, val_loss: 5.3180, val_acc: 0.5692\n",
      "Epoch [1145], train_loss: 0.0000, val_loss: 5.3772, val_acc: 0.5718\n",
      "Epoch [1146], train_loss: 0.0000, val_loss: 5.4097, val_acc: 0.5718\n",
      "Epoch [1147], train_loss: 0.0000, val_loss: 5.4773, val_acc: 0.5744\n",
      "Epoch [1148], train_loss: 0.0000, val_loss: 5.4856, val_acc: 0.5744\n",
      "Epoch [1149], train_loss: 0.0000, val_loss: 5.5271, val_acc: 0.5744\n",
      "Epoch [1150], train_loss: 0.0000, val_loss: 5.5559, val_acc: 0.5744\n",
      "Epoch [1151], train_loss: 0.0000, val_loss: 5.6062, val_acc: 0.5718\n",
      "Epoch [1152], train_loss: 0.0000, val_loss: 5.5928, val_acc: 0.5770\n",
      "Epoch [1153], train_loss: 0.0000, val_loss: 5.6538, val_acc: 0.5744\n",
      "Epoch [1154], train_loss: 0.0000, val_loss: 5.6012, val_acc: 0.5796\n",
      "Epoch [1155], train_loss: 0.0000, val_loss: 5.6512, val_acc: 0.5744\n",
      "Epoch [1156], train_loss: 0.0000, val_loss: 5.7204, val_acc: 0.5718\n",
      "Epoch [1157], train_loss: 0.0000, val_loss: 5.6715, val_acc: 0.5718\n",
      "Epoch [1158], train_loss: 0.0000, val_loss: 5.7423, val_acc: 0.5718\n",
      "Epoch [1159], train_loss: 0.0000, val_loss: 5.8627, val_acc: 0.5744\n",
      "Epoch [1160], train_loss: 0.0000, val_loss: 5.8095, val_acc: 0.5718\n",
      "Epoch [1161], train_loss: 0.0015, val_loss: 4.8249, val_acc: 0.5695\n",
      "Epoch [1162], train_loss: 0.0153, val_loss: 4.5483, val_acc: 0.5588\n",
      "Epoch [1163], train_loss: 0.0172, val_loss: 3.6908, val_acc: 0.5851\n",
      "Epoch [1164], train_loss: 0.0085, val_loss: 3.5452, val_acc: 0.5796\n",
      "Epoch [1165], train_loss: 0.0078, val_loss: 4.0915, val_acc: 0.5763\n",
      "Epoch [1166], train_loss: 0.0012, val_loss: 4.1741, val_acc: 0.5796\n",
      "Epoch [1167], train_loss: 0.0003, val_loss: 4.4020, val_acc: 0.5659\n",
      "Epoch [1168], train_loss: 0.0005, val_loss: 4.8197, val_acc: 0.5636\n",
      "Epoch [1169], train_loss: 0.0001, val_loss: 4.7864, val_acc: 0.5796\n",
      "Epoch [1170], train_loss: 0.0001, val_loss: 4.9575, val_acc: 0.5744\n",
      "Epoch [1171], train_loss: 0.0000, val_loss: 5.1489, val_acc: 0.5744\n",
      "Epoch [1172], train_loss: 0.0001, val_loss: 4.9909, val_acc: 0.5718\n",
      "Epoch [1173], train_loss: 0.0001, val_loss: 5.2684, val_acc: 0.5799\n",
      "Epoch [1174], train_loss: 0.0000, val_loss: 5.3683, val_acc: 0.5692\n",
      "Epoch [1175], train_loss: 0.0000, val_loss: 5.4284, val_acc: 0.5744\n",
      "Epoch [1176], train_loss: 0.0000, val_loss: 5.5176, val_acc: 0.5770\n",
      "Epoch [1177], train_loss: 0.0000, val_loss: 5.4525, val_acc: 0.5770\n",
      "Epoch [1178], train_loss: 0.0000, val_loss: 5.4476, val_acc: 0.5744\n",
      "Epoch [1179], train_loss: 0.0000, val_loss: 5.5166, val_acc: 0.5770\n",
      "Epoch [1180], train_loss: 0.0000, val_loss: 5.6896, val_acc: 0.5770\n",
      "Epoch [1181], train_loss: 0.0000, val_loss: 5.7036, val_acc: 0.5796\n",
      "Epoch [1182], train_loss: 0.0000, val_loss: 5.7764, val_acc: 0.5822\n",
      "Epoch [1183], train_loss: 0.0000, val_loss: 5.6941, val_acc: 0.5770\n",
      "Epoch [1184], train_loss: 0.0000, val_loss: 5.7633, val_acc: 0.5796\n",
      "Epoch [1185], train_loss: 0.0000, val_loss: 5.7560, val_acc: 0.5822\n",
      "Epoch [1186], train_loss: 0.0000, val_loss: 5.7974, val_acc: 0.5744\n",
      "Epoch [1187], train_loss: 0.0000, val_loss: 5.7261, val_acc: 0.5796\n",
      "Epoch [1188], train_loss: 0.0000, val_loss: 5.9110, val_acc: 0.5796\n",
      "Epoch [1189], train_loss: 0.0000, val_loss: 5.8045, val_acc: 0.5770\n",
      "Epoch [1190], train_loss: 0.0000, val_loss: 5.8547, val_acc: 0.5822\n",
      "Epoch [1191], train_loss: 0.0000, val_loss: 5.7569, val_acc: 0.5770\n",
      "Epoch [1192], train_loss: 0.0000, val_loss: 5.9601, val_acc: 0.5744\n",
      "Epoch [1193], train_loss: 0.0000, val_loss: 5.9378, val_acc: 0.5822\n",
      "Epoch [1194], train_loss: 0.0000, val_loss: 5.9659, val_acc: 0.5796\n",
      "Epoch [1195], train_loss: 0.0000, val_loss: 5.8912, val_acc: 0.5796\n",
      "Epoch [1196], train_loss: 0.0000, val_loss: 5.8936, val_acc: 0.5770\n",
      "Epoch [1197], train_loss: 0.0000, val_loss: 6.0832, val_acc: 0.5796\n",
      "Epoch [1198], train_loss: 0.0000, val_loss: 6.0489, val_acc: 0.5770\n",
      "Epoch [1199], train_loss: 0.0000, val_loss: 6.0371, val_acc: 0.5796\n",
      "Epoch [1200], train_loss: 0.0000, val_loss: 6.0758, val_acc: 0.5796\n",
      "Epoch [1201], train_loss: 0.0000, val_loss: 6.1241, val_acc: 0.5744\n",
      "Epoch [1202], train_loss: 0.0000, val_loss: 6.1652, val_acc: 0.5796\n",
      "Epoch [1203], train_loss: 0.0000, val_loss: 6.0948, val_acc: 0.5796\n",
      "Epoch [1204], train_loss: 0.0000, val_loss: 6.1663, val_acc: 0.5796\n",
      "Epoch [1205], train_loss: 0.0000, val_loss: 6.1463, val_acc: 0.5744\n",
      "Epoch [1206], train_loss: 0.0000, val_loss: 6.0641, val_acc: 0.5796\n",
      "Epoch [1207], train_loss: 0.0000, val_loss: 6.0908, val_acc: 0.5796\n",
      "Epoch [1208], train_loss: 0.0000, val_loss: 6.2841, val_acc: 0.5689\n",
      "Epoch [1209], train_loss: 0.0000, val_loss: 6.2346, val_acc: 0.5796\n",
      "Epoch [1210], train_loss: 0.0000, val_loss: 6.3526, val_acc: 0.5715\n",
      "Epoch [1211], train_loss: 0.0000, val_loss: 6.2655, val_acc: 0.5689\n",
      "Epoch [1212], train_loss: 0.0000, val_loss: 6.2911, val_acc: 0.5744\n",
      "Epoch [1213], train_loss: 0.0000, val_loss: 6.2654, val_acc: 0.5715\n",
      "Epoch [1214], train_loss: 0.0000, val_loss: 6.2159, val_acc: 0.5796\n",
      "Epoch [1215], train_loss: 0.0000, val_loss: 6.3758, val_acc: 0.5689\n",
      "Epoch [1216], train_loss: 0.0000, val_loss: 6.3154, val_acc: 0.5770\n",
      "Epoch [1217], train_loss: 0.0000, val_loss: 6.3794, val_acc: 0.5715\n",
      "Epoch [1218], train_loss: 0.0000, val_loss: 6.4461, val_acc: 0.5744\n",
      "Epoch [1219], train_loss: 0.0000, val_loss: 6.5064, val_acc: 0.5662\n",
      "Epoch [1220], train_loss: 0.0000, val_loss: 6.4379, val_acc: 0.5715\n",
      "Epoch [1221], train_loss: 0.0000, val_loss: 6.4614, val_acc: 0.5715\n",
      "Epoch [1222], train_loss: 0.0000, val_loss: 6.4970, val_acc: 0.5689\n",
      "Epoch [1223], train_loss: 0.0000, val_loss: 6.4151, val_acc: 0.5715\n",
      "Epoch [1224], train_loss: 0.0000, val_loss: 6.5338, val_acc: 0.5715\n",
      "Epoch [1225], train_loss: 0.0000, val_loss: 6.6130, val_acc: 0.5741\n",
      "Epoch [1226], train_loss: 0.0000, val_loss: 6.6014, val_acc: 0.5715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1227], train_loss: 0.0000, val_loss: 6.5672, val_acc: 0.5689\n",
      "Epoch [1228], train_loss: 0.0000, val_loss: 6.5013, val_acc: 0.5770\n",
      "Epoch [1229], train_loss: 0.0000, val_loss: 6.6117, val_acc: 0.5715\n",
      "Epoch [1230], train_loss: 0.0000, val_loss: 6.6369, val_acc: 0.5689\n",
      "Epoch [1231], train_loss: 0.0000, val_loss: 6.7525, val_acc: 0.5744\n",
      "Epoch [1232], train_loss: 0.0000, val_loss: 6.8203, val_acc: 0.5822\n",
      "Epoch [1233], train_loss: 0.0000, val_loss: 6.9040, val_acc: 0.5715\n",
      "Epoch [1234], train_loss: 0.0000, val_loss: 6.9116, val_acc: 0.5770\n",
      "Epoch [1235], train_loss: 0.0000, val_loss: 6.9233, val_acc: 0.5770\n",
      "Epoch [1236], train_loss: 0.0000, val_loss: 6.9456, val_acc: 0.5715\n",
      "Epoch [1237], train_loss: 0.0000, val_loss: 6.9862, val_acc: 0.5689\n",
      "Epoch [1238], train_loss: 0.0000, val_loss: 6.7472, val_acc: 0.5770\n",
      "Epoch [1239], train_loss: 0.0000, val_loss: 6.9706, val_acc: 0.5770\n",
      "Epoch [1240], train_loss: 0.0000, val_loss: 6.9105, val_acc: 0.5770\n",
      "Epoch [1241], train_loss: 0.0000, val_loss: 7.1192, val_acc: 0.5689\n",
      "Epoch [1242], train_loss: 0.0000, val_loss: 7.0298, val_acc: 0.5689\n",
      "Epoch [1243], train_loss: 0.0000, val_loss: 6.8857, val_acc: 0.5770\n",
      "Epoch [1244], train_loss: 0.0000, val_loss: 6.9862, val_acc: 0.5770\n",
      "Epoch [1245], train_loss: 0.0000, val_loss: 6.9919, val_acc: 0.5770\n",
      "Epoch [1246], train_loss: 0.0000, val_loss: 7.0794, val_acc: 0.5689\n",
      "Epoch [1247], train_loss: 0.0000, val_loss: 7.0613, val_acc: 0.5744\n",
      "Epoch [1248], train_loss: 0.0141, val_loss: 6.2908, val_acc: 0.5529\n",
      "Epoch [1249], train_loss: 0.0609, val_loss: 3.1171, val_acc: 0.5747\n",
      "Epoch [1250], train_loss: 0.0346, val_loss: 3.3052, val_acc: 0.5689\n",
      "Epoch [1251], train_loss: 0.0091, val_loss: 3.4976, val_acc: 0.5640\n",
      "Epoch [1252], train_loss: 0.0184, val_loss: 2.9567, val_acc: 0.6007\n",
      "Epoch [1253], train_loss: 0.0209, val_loss: 3.0186, val_acc: 0.6144\n",
      "Epoch [1254], train_loss: 0.0029, val_loss: 4.0038, val_acc: 0.5848\n",
      "Epoch [1255], train_loss: 0.0008, val_loss: 4.5224, val_acc: 0.5985\n",
      "Epoch [1256], train_loss: 0.0002, val_loss: 4.8190, val_acc: 0.5770\n",
      "Epoch [1257], train_loss: 0.0005, val_loss: 4.6109, val_acc: 0.5793\n",
      "Epoch [1258], train_loss: 0.0002, val_loss: 5.0519, val_acc: 0.5822\n",
      "Epoch [1259], train_loss: 0.0000, val_loss: 5.0531, val_acc: 0.5741\n",
      "Epoch [1260], train_loss: 0.0000, val_loss: 5.1282, val_acc: 0.5633\n",
      "Epoch [1261], train_loss: 0.0000, val_loss: 5.0377, val_acc: 0.5874\n",
      "Epoch [1262], train_loss: 0.0000, val_loss: 5.1901, val_acc: 0.5819\n",
      "Epoch [1263], train_loss: 0.0000, val_loss: 5.4276, val_acc: 0.5685\n",
      "Epoch [1264], train_loss: 0.0000, val_loss: 5.2454, val_acc: 0.5715\n",
      "Epoch [1265], train_loss: 0.0000, val_loss: 5.3190, val_acc: 0.5607\n",
      "Epoch [1266], train_loss: 0.0000, val_loss: 5.6104, val_acc: 0.5767\n",
      "Epoch [1267], train_loss: 0.0000, val_loss: 5.4687, val_acc: 0.5848\n",
      "Epoch [1268], train_loss: 0.0000, val_loss: 5.5797, val_acc: 0.5767\n",
      "Epoch [1269], train_loss: 0.0000, val_loss: 5.7873, val_acc: 0.5685\n",
      "Epoch [1270], train_loss: 0.0000, val_loss: 5.4544, val_acc: 0.5715\n",
      "Epoch [1271], train_loss: 0.0000, val_loss: 5.5973, val_acc: 0.5767\n",
      "Epoch [1272], train_loss: 0.0000, val_loss: 5.7469, val_acc: 0.5659\n",
      "Epoch [1273], train_loss: 0.0000, val_loss: 5.5841, val_acc: 0.5689\n",
      "Epoch [1274], train_loss: 0.0067, val_loss: 4.9193, val_acc: 0.5581\n",
      "Epoch [1275], train_loss: 0.0265, val_loss: 3.5595, val_acc: 0.5825\n",
      "Epoch [1276], train_loss: 0.0073, val_loss: 3.6460, val_acc: 0.5770\n",
      "Epoch [1277], train_loss: 0.0020, val_loss: 3.8611, val_acc: 0.5662\n",
      "Epoch [1278], train_loss: 0.0002, val_loss: 4.4233, val_acc: 0.5689\n",
      "Epoch [1279], train_loss: 0.0001, val_loss: 4.7740, val_acc: 0.5633\n",
      "Epoch [1280], train_loss: 0.0001, val_loss: 4.6904, val_acc: 0.5715\n",
      "Epoch [1281], train_loss: 0.0000, val_loss: 5.0869, val_acc: 0.5822\n",
      "Epoch [1282], train_loss: 0.0000, val_loss: 4.9695, val_acc: 0.5715\n",
      "Epoch [1283], train_loss: 0.0000, val_loss: 5.2193, val_acc: 0.5715\n",
      "Epoch [1284], train_loss: 0.0000, val_loss: 5.3809, val_acc: 0.5770\n",
      "Epoch [1285], train_loss: 0.0000, val_loss: 5.4419, val_acc: 0.5770\n",
      "Epoch [1286], train_loss: 0.0000, val_loss: 5.4560, val_acc: 0.5715\n",
      "Epoch [1287], train_loss: 0.0000, val_loss: 5.4256, val_acc: 0.5689\n",
      "Epoch [1288], train_loss: 0.0000, val_loss: 5.7272, val_acc: 0.5767\n",
      "Epoch [1289], train_loss: 0.0000, val_loss: 5.4016, val_acc: 0.5770\n",
      "Epoch [1290], train_loss: 0.0000, val_loss: 5.7914, val_acc: 0.5607\n",
      "Epoch [1291], train_loss: 0.0000, val_loss: 5.6207, val_acc: 0.5741\n",
      "Epoch [1292], train_loss: 0.0000, val_loss: 5.5682, val_acc: 0.5796\n",
      "Epoch [1293], train_loss: 0.0000, val_loss: 5.6759, val_acc: 0.5770\n",
      "Epoch [1294], train_loss: 0.0000, val_loss: 5.6608, val_acc: 0.5848\n",
      "Epoch [1295], train_loss: 0.0000, val_loss: 5.6770, val_acc: 0.5741\n",
      "Epoch [1296], train_loss: 0.0000, val_loss: 5.7824, val_acc: 0.5744\n",
      "Epoch [1297], train_loss: 0.0000, val_loss: 5.6792, val_acc: 0.5796\n",
      "Epoch [1298], train_loss: 0.0000, val_loss: 5.5509, val_acc: 0.5770\n",
      "Epoch [1299], train_loss: 0.0000, val_loss: 5.5596, val_acc: 0.5822\n",
      "Epoch [1300], train_loss: 0.0000, val_loss: 5.7383, val_acc: 0.5718\n",
      "Epoch [1301], train_loss: 0.0000, val_loss: 5.7412, val_acc: 0.5796\n",
      "Epoch [1302], train_loss: 0.0001, val_loss: 5.8434, val_acc: 0.5822\n",
      "Epoch [1303], train_loss: 0.0003, val_loss: 6.0488, val_acc: 0.5610\n",
      "Epoch [1304], train_loss: 0.0033, val_loss: 3.7879, val_acc: 0.5669\n",
      "Epoch [1305], train_loss: 0.0168, val_loss: 5.8008, val_acc: 0.5770\n",
      "Epoch [1306], train_loss: 0.0206, val_loss: 4.0552, val_acc: 0.5669\n",
      "Epoch [1307], train_loss: 0.0059, val_loss: 5.4701, val_acc: 0.5741\n",
      "Epoch [1308], train_loss: 0.0055, val_loss: 5.3624, val_acc: 0.5633\n",
      "Epoch [1309], train_loss: 0.0006, val_loss: 6.1413, val_acc: 0.5662\n",
      "Epoch [1310], train_loss: 0.0001, val_loss: 6.3576, val_acc: 0.5848\n",
      "Epoch [1311], train_loss: 0.0000, val_loss: 6.6286, val_acc: 0.5662\n",
      "Epoch [1312], train_loss: 0.0000, val_loss: 6.6838, val_acc: 0.5636\n",
      "Epoch [1313], train_loss: 0.0000, val_loss: 6.7232, val_acc: 0.5636\n",
      "Epoch [1314], train_loss: 0.0000, val_loss: 6.3948, val_acc: 0.5874\n",
      "Epoch [1315], train_loss: 0.0000, val_loss: 6.7006, val_acc: 0.5610\n",
      "Epoch [1316], train_loss: 0.0000, val_loss: 6.8181, val_acc: 0.5715\n",
      "Epoch [1317], train_loss: 0.0000, val_loss: 6.8800, val_acc: 0.5851\n",
      "Epoch [1318], train_loss: 0.0000, val_loss: 6.8376, val_acc: 0.5822\n",
      "Epoch [1319], train_loss: 0.0000, val_loss: 7.0065, val_acc: 0.5636\n",
      "Epoch [1320], train_loss: 0.0000, val_loss: 7.0034, val_acc: 0.5744\n",
      "Epoch [1321], train_loss: 0.0000, val_loss: 7.0085, val_acc: 0.5718\n",
      "Epoch [1322], train_loss: 0.0000, val_loss: 6.9385, val_acc: 0.5796\n",
      "Epoch [1323], train_loss: 0.0000, val_loss: 6.9438, val_acc: 0.5770\n",
      "Epoch [1324], train_loss: 0.0000, val_loss: 7.1390, val_acc: 0.5636\n",
      "Epoch [1325], train_loss: 0.0000, val_loss: 6.9940, val_acc: 0.5796\n",
      "Epoch [1326], train_loss: 0.0000, val_loss: 7.1365, val_acc: 0.5744\n",
      "Epoch [1327], train_loss: 0.0000, val_loss: 7.0573, val_acc: 0.5744\n",
      "Epoch [1328], train_loss: 0.0000, val_loss: 7.2452, val_acc: 0.5689\n",
      "Epoch [1329], train_loss: 0.0003, val_loss: 7.1876, val_acc: 0.5822\n",
      "Epoch [1330], train_loss: 0.0529, val_loss: 2.2760, val_acc: 0.5252\n",
      "Epoch [1331], train_loss: 0.0340, val_loss: 4.1531, val_acc: 0.5880\n",
      "Epoch [1332], train_loss: 0.0015, val_loss: 5.0441, val_acc: 0.5871\n",
      "Epoch [1333], train_loss: 0.0001, val_loss: 5.3897, val_acc: 0.5848\n",
      "Epoch [1334], train_loss: 0.0063, val_loss: 4.3399, val_acc: 0.5851\n",
      "Epoch [1335], train_loss: 0.0044, val_loss: 4.2688, val_acc: 0.6037\n",
      "Epoch [1336], train_loss: 0.0004, val_loss: 4.7989, val_acc: 0.5874\n",
      "Epoch [1337], train_loss: 0.0001, val_loss: 4.8256, val_acc: 0.5926\n",
      "Epoch [1338], train_loss: 0.0001, val_loss: 4.9079, val_acc: 0.5929\n",
      "Epoch [1339], train_loss: 0.0010, val_loss: 5.2295, val_acc: 0.5978\n",
      "Epoch [1340], train_loss: 0.0034, val_loss: 5.8326, val_acc: 0.5851\n",
      "Epoch [1341], train_loss: 0.0012, val_loss: 5.5922, val_acc: 0.5770\n",
      "Epoch [1342], train_loss: 0.0002, val_loss: 5.5724, val_acc: 0.5770\n",
      "Epoch [1343], train_loss: 0.0001, val_loss: 5.8769, val_acc: 0.5822\n",
      "Epoch [1344], train_loss: 0.0004, val_loss: 5.2077, val_acc: 0.5718\n",
      "Epoch [1345], train_loss: 0.0047, val_loss: 5.9517, val_acc: 0.5906\n",
      "Epoch [1346], train_loss: 0.0021, val_loss: 5.3387, val_acc: 0.5851\n",
      "Epoch [1347], train_loss: 0.0006, val_loss: 5.6192, val_acc: 0.5718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1348], train_loss: 0.0004, val_loss: 6.1306, val_acc: 0.5932\n",
      "Epoch [1349], train_loss: 0.0001, val_loss: 6.3952, val_acc: 0.5854\n",
      "Epoch [1350], train_loss: 0.0001, val_loss: 6.5300, val_acc: 0.5854\n",
      "Epoch [1351], train_loss: 0.0000, val_loss: 6.6910, val_acc: 0.5880\n",
      "Epoch [1352], train_loss: 0.0141, val_loss: 7.1835, val_acc: 0.5666\n",
      "Epoch [1353], train_loss: 0.0559, val_loss: 3.8249, val_acc: 0.5265\n",
      "Epoch [1354], train_loss: 0.0065, val_loss: 5.4486, val_acc: 0.5831\n",
      "Epoch [1355], train_loss: 0.0062, val_loss: 6.1205, val_acc: 0.5776\n",
      "Epoch [1356], train_loss: 0.0003, val_loss: 6.3589, val_acc: 0.5776\n",
      "Epoch [1357], train_loss: 0.0002, val_loss: 6.4947, val_acc: 0.5724\n",
      "Epoch [1358], train_loss: 0.0001, val_loss: 6.5121, val_acc: 0.5776\n",
      "Epoch [1359], train_loss: 0.0001, val_loss: 6.7005, val_acc: 0.5802\n",
      "Epoch [1360], train_loss: 0.0202, val_loss: 6.7054, val_acc: 0.5617\n",
      "Epoch [1361], train_loss: 0.0088, val_loss: 4.6945, val_acc: 0.5588\n",
      "Epoch [1362], train_loss: 0.0014, val_loss: 5.2944, val_acc: 0.5669\n",
      "Epoch [1363], train_loss: 0.0005, val_loss: 5.6985, val_acc: 0.5591\n",
      "Epoch [1364], train_loss: 0.0002, val_loss: 5.8405, val_acc: 0.5483\n",
      "Epoch [1365], train_loss: 0.0001, val_loss: 5.8780, val_acc: 0.5617\n",
      "Epoch [1366], train_loss: 0.0000, val_loss: 6.0631, val_acc: 0.5509\n",
      "Epoch [1367], train_loss: 0.0004, val_loss: 6.0157, val_acc: 0.5591\n",
      "Epoch [1368], train_loss: 0.0010, val_loss: 6.0974, val_acc: 0.5666\n",
      "Epoch [1369], train_loss: 0.0002, val_loss: 6.5788, val_acc: 0.5669\n",
      "Epoch [1370], train_loss: 0.0001, val_loss: 6.7224, val_acc: 0.5614\n",
      "Epoch [1371], train_loss: 0.0000, val_loss: 6.8556, val_acc: 0.5480\n",
      "Epoch [1372], train_loss: 0.0000, val_loss: 6.6420, val_acc: 0.5695\n",
      "Epoch [1373], train_loss: 0.0000, val_loss: 6.9869, val_acc: 0.5425\n",
      "Epoch [1374], train_loss: 0.0003, val_loss: 7.0918, val_acc: 0.5347\n",
      "Epoch [1375], train_loss: 0.0000, val_loss: 7.0375, val_acc: 0.5373\n",
      "Epoch [1376], train_loss: 0.0000, val_loss: 7.1702, val_acc: 0.5399\n",
      "Epoch [1377], train_loss: 0.0000, val_loss: 7.0896, val_acc: 0.5399\n",
      "Epoch [1378], train_loss: 0.0064, val_loss: 6.6160, val_acc: 0.5695\n",
      "Epoch [1379], train_loss: 0.0315, val_loss: 3.7396, val_acc: 0.5542\n",
      "Epoch [1380], train_loss: 0.0070, val_loss: 4.5593, val_acc: 0.5350\n",
      "Epoch [1381], train_loss: 0.0018, val_loss: 5.2762, val_acc: 0.5640\n",
      "Epoch [1382], train_loss: 0.0002, val_loss: 5.7677, val_acc: 0.5614\n",
      "Epoch [1383], train_loss: 0.0001, val_loss: 5.9012, val_acc: 0.5588\n",
      "Epoch [1384], train_loss: 0.0001, val_loss: 6.2462, val_acc: 0.5454\n",
      "Epoch [1385], train_loss: 0.0036, val_loss: 4.9508, val_acc: 0.5561\n",
      "Epoch [1386], train_loss: 0.0003, val_loss: 5.0328, val_acc: 0.5535\n",
      "Epoch [1387], train_loss: 0.0081, val_loss: 5.0989, val_acc: 0.5480\n",
      "Epoch [1388], train_loss: 0.0214, val_loss: 3.8657, val_acc: 0.5831\n",
      "Epoch [1389], train_loss: 0.0084, val_loss: 4.0715, val_acc: 0.5724\n",
      "Epoch [1390], train_loss: 0.0043, val_loss: 4.0550, val_acc: 0.5698\n",
      "Epoch [1391], train_loss: 0.0003, val_loss: 4.3921, val_acc: 0.5617\n",
      "Epoch [1392], train_loss: 0.0005, val_loss: 4.8811, val_acc: 0.5776\n",
      "Epoch [1393], train_loss: 0.0004, val_loss: 5.1761, val_acc: 0.5831\n",
      "Epoch [1394], train_loss: 0.0001, val_loss: 5.3364, val_acc: 0.5857\n",
      "Epoch [1395], train_loss: 0.0001, val_loss: 5.5346, val_acc: 0.5831\n",
      "Epoch [1396], train_loss: 0.0001, val_loss: 5.8878, val_acc: 0.5884\n",
      "Epoch [1397], train_loss: 0.0001, val_loss: 5.9025, val_acc: 0.5884\n",
      "Epoch [1398], train_loss: 0.0000, val_loss: 5.9423, val_acc: 0.5831\n",
      "Epoch [1399], train_loss: 0.0000, val_loss: 6.0298, val_acc: 0.5884\n",
      "Epoch [1400], train_loss: 0.0000, val_loss: 6.0044, val_acc: 0.5831\n",
      "Epoch [1401], train_loss: 0.0001, val_loss: 5.9129, val_acc: 0.5831\n",
      "Epoch [1402], train_loss: 0.0000, val_loss: 6.0524, val_acc: 0.5805\n",
      "Epoch [1403], train_loss: 0.0000, val_loss: 6.1051, val_acc: 0.5779\n",
      "Epoch [1404], train_loss: 0.0000, val_loss: 6.1418, val_acc: 0.5805\n",
      "Epoch [1405], train_loss: 0.0000, val_loss: 6.2516, val_acc: 0.5805\n",
      "Epoch [1406], train_loss: 0.0000, val_loss: 6.3334, val_acc: 0.5805\n",
      "Epoch [1407], train_loss: 0.0001, val_loss: 6.2741, val_acc: 0.5831\n",
      "Epoch [1408], train_loss: 0.0000, val_loss: 6.5656, val_acc: 0.5779\n",
      "Epoch [1409], train_loss: 0.0000, val_loss: 6.5928, val_acc: 0.5831\n",
      "Epoch [1410], train_loss: 0.0000, val_loss: 6.6288, val_acc: 0.5831\n",
      "Epoch [1411], train_loss: 0.0000, val_loss: 6.7495, val_acc: 0.5831\n",
      "Epoch [1412], train_loss: 0.0000, val_loss: 6.7265, val_acc: 0.5831\n",
      "Epoch [1413], train_loss: 0.0000, val_loss: 6.7668, val_acc: 0.5805\n",
      "Epoch [1414], train_loss: 0.0000, val_loss: 6.9071, val_acc: 0.5805\n",
      "Epoch [1415], train_loss: 0.0000, val_loss: 6.8421, val_acc: 0.5805\n",
      "Epoch [1416], train_loss: 0.0000, val_loss: 6.8717, val_acc: 0.5831\n",
      "Epoch [1417], train_loss: 0.0000, val_loss: 6.9000, val_acc: 0.5779\n",
      "Epoch [1418], train_loss: 0.0000, val_loss: 7.0145, val_acc: 0.5753\n",
      "Epoch [1419], train_loss: 0.0000, val_loss: 6.8977, val_acc: 0.5831\n",
      "Epoch [1420], train_loss: 0.0000, val_loss: 6.9783, val_acc: 0.5779\n",
      "Epoch [1421], train_loss: 0.0000, val_loss: 7.0199, val_acc: 0.5831\n",
      "Epoch [1422], train_loss: 0.0000, val_loss: 7.0680, val_acc: 0.5779\n",
      "Epoch [1423], train_loss: 0.0000, val_loss: 7.0478, val_acc: 0.5805\n",
      "Epoch [1424], train_loss: 0.0000, val_loss: 7.0706, val_acc: 0.5779\n",
      "Epoch [1425], train_loss: 0.0000, val_loss: 7.0351, val_acc: 0.5805\n",
      "Epoch [1426], train_loss: 0.0000, val_loss: 7.1334, val_acc: 0.5779\n",
      "Epoch [1427], train_loss: 0.0000, val_loss: 7.1179, val_acc: 0.5779\n",
      "Epoch [1428], train_loss: 0.0000, val_loss: 7.1006, val_acc: 0.5779\n",
      "Epoch [1429], train_loss: 0.0000, val_loss: 7.3132, val_acc: 0.5753\n",
      "Epoch [1430], train_loss: 0.0000, val_loss: 7.1934, val_acc: 0.5753\n",
      "Epoch [1431], train_loss: 0.0000, val_loss: 7.2334, val_acc: 0.5779\n",
      "Epoch [1432], train_loss: 0.0000, val_loss: 7.3899, val_acc: 0.5857\n",
      "Epoch [1433], train_loss: 0.0000, val_loss: 7.3357, val_acc: 0.5753\n",
      "Epoch [1434], train_loss: 0.0000, val_loss: 7.2152, val_acc: 0.5779\n",
      "Epoch [1435], train_loss: 0.0000, val_loss: 7.3408, val_acc: 0.5779\n",
      "Epoch [1436], train_loss: 0.0000, val_loss: 7.3016, val_acc: 0.5753\n",
      "Epoch [1437], train_loss: 0.0000, val_loss: 7.4704, val_acc: 0.5779\n",
      "Epoch [1438], train_loss: 0.0040, val_loss: 7.6844, val_acc: 0.5750\n",
      "Epoch [1439], train_loss: 0.0686, val_loss: 3.5752, val_acc: 0.5802\n",
      "Epoch [1440], train_loss: 0.0225, val_loss: 3.4558, val_acc: 0.5617\n",
      "Epoch [1441], train_loss: 0.0020, val_loss: 5.7437, val_acc: 0.5509\n",
      "Epoch [1442], train_loss: 0.0013, val_loss: 6.3378, val_acc: 0.5565\n",
      "Epoch [1443], train_loss: 0.0013, val_loss: 6.5811, val_acc: 0.5672\n",
      "Epoch [1444], train_loss: 0.0025, val_loss: 5.8181, val_acc: 0.5561\n",
      "Epoch [1445], train_loss: 0.0006, val_loss: 6.4645, val_acc: 0.5776\n",
      "Epoch [1446], train_loss: 0.0001, val_loss: 6.8716, val_acc: 0.5776\n",
      "Epoch [1447], train_loss: 0.0000, val_loss: 6.8284, val_acc: 0.5721\n",
      "Epoch [1448], train_loss: 0.0001, val_loss: 7.0290, val_acc: 0.5695\n",
      "Epoch [1449], train_loss: 0.0000, val_loss: 7.1475, val_acc: 0.5747\n",
      "Epoch [1450], train_loss: 0.0000, val_loss: 7.2172, val_acc: 0.5747\n",
      "Epoch [1451], train_loss: 0.0000, val_loss: 7.2781, val_acc: 0.5695\n",
      "Epoch [1452], train_loss: 0.0000, val_loss: 7.3273, val_acc: 0.5695\n",
      "Epoch [1453], train_loss: 0.0000, val_loss: 7.1238, val_acc: 0.5695\n",
      "Epoch [1454], train_loss: 0.0000, val_loss: 7.3084, val_acc: 0.5721\n",
      "Epoch [1455], train_loss: 0.0000, val_loss: 7.4849, val_acc: 0.5721\n",
      "Epoch [1456], train_loss: 0.0000, val_loss: 7.4126, val_acc: 0.5721\n",
      "Epoch [1457], train_loss: 0.0000, val_loss: 7.6174, val_acc: 0.5747\n",
      "Epoch [1458], train_loss: 0.0057, val_loss: 7.5472, val_acc: 0.5695\n",
      "Epoch [1459], train_loss: 0.1507, val_loss: 2.9920, val_acc: 0.5708\n",
      "Epoch [1460], train_loss: 0.0710, val_loss: 3.3496, val_acc: 0.6000\n",
      "Epoch [1461], train_loss: 0.0256, val_loss: 4.1593, val_acc: 0.5698\n",
      "Epoch [1462], train_loss: 0.0047, val_loss: 4.7350, val_acc: 0.5490\n",
      "Epoch [1463], train_loss: 0.0027, val_loss: 4.7749, val_acc: 0.5724\n",
      "Epoch [1464], train_loss: 0.0015, val_loss: 5.5374, val_acc: 0.5597\n",
      "Epoch [1465], train_loss: 0.0006, val_loss: 5.7131, val_acc: 0.5620\n",
      "Epoch [1466], train_loss: 0.0002, val_loss: 5.9336, val_acc: 0.5698\n",
      "Epoch [1467], train_loss: 0.0002, val_loss: 6.1943, val_acc: 0.5643\n",
      "Epoch [1468], train_loss: 0.0001, val_loss: 6.0163, val_acc: 0.5721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1469], train_loss: 0.0000, val_loss: 6.1381, val_acc: 0.5721\n",
      "Epoch [1470], train_loss: 0.0002, val_loss: 6.1396, val_acc: 0.5750\n",
      "Epoch [1471], train_loss: 0.0000, val_loss: 6.0310, val_acc: 0.5646\n",
      "Epoch [1472], train_loss: 0.0000, val_loss: 6.3229, val_acc: 0.5646\n",
      "Epoch [1473], train_loss: 0.0000, val_loss: 6.3591, val_acc: 0.5724\n",
      "Epoch [1474], train_loss: 0.0000, val_loss: 6.2002, val_acc: 0.5698\n",
      "Epoch [1475], train_loss: 0.0089, val_loss: 6.2438, val_acc: 0.5701\n",
      "Epoch [1476], train_loss: 0.0108, val_loss: 5.2831, val_acc: 0.5779\n",
      "Epoch [1477], train_loss: 0.0055, val_loss: 5.6407, val_acc: 0.6150\n",
      "Epoch [1478], train_loss: 0.0006, val_loss: 5.5489, val_acc: 0.5910\n",
      "Epoch [1479], train_loss: 0.0002, val_loss: 5.6664, val_acc: 0.5991\n",
      "Epoch [1480], train_loss: 0.0002, val_loss: 5.5393, val_acc: 0.6046\n",
      "Epoch [1481], train_loss: 0.0034, val_loss: 5.7132, val_acc: 0.6206\n",
      "Epoch [1482], train_loss: 0.0068, val_loss: 6.2096, val_acc: 0.6040\n",
      "Epoch [1483], train_loss: 0.0043, val_loss: 5.6664, val_acc: 0.5724\n",
      "Epoch [1484], train_loss: 0.0024, val_loss: 5.0256, val_acc: 0.5591\n",
      "Epoch [1485], train_loss: 0.0049, val_loss: 5.4990, val_acc: 0.5962\n",
      "Epoch [1486], train_loss: 0.0060, val_loss: 5.4996, val_acc: 0.5910\n",
      "Epoch [1487], train_loss: 0.0031, val_loss: 4.0526, val_acc: 0.5880\n",
      "Epoch [1488], train_loss: 0.0013, val_loss: 5.1018, val_acc: 0.5851\n",
      "Epoch [1489], train_loss: 0.0002, val_loss: 5.5789, val_acc: 0.5828\n",
      "Epoch [1490], train_loss: 0.0001, val_loss: 5.3164, val_acc: 0.5877\n",
      "Epoch [1491], train_loss: 0.0001, val_loss: 5.5896, val_acc: 0.5773\n",
      "Epoch [1492], train_loss: 0.0001, val_loss: 5.4868, val_acc: 0.5825\n",
      "Epoch [1493], train_loss: 0.0000, val_loss: 5.8942, val_acc: 0.5854\n",
      "Epoch [1494], train_loss: 0.0000, val_loss: 5.8450, val_acc: 0.5854\n",
      "Epoch [1495], train_loss: 0.0000, val_loss: 5.7763, val_acc: 0.5747\n",
      "Epoch [1496], train_loss: 0.0000, val_loss: 5.7993, val_acc: 0.5799\n",
      "Epoch [1497], train_loss: 0.0001, val_loss: 6.0678, val_acc: 0.5747\n",
      "Epoch [1498], train_loss: 0.0000, val_loss: 5.9561, val_acc: 0.5851\n",
      "Epoch [1499], train_loss: 0.0000, val_loss: 6.2991, val_acc: 0.5799\n",
      "Epoch [1500], train_loss: 0.0000, val_loss: 6.0521, val_acc: 0.5799\n",
      "Epoch [1501], train_loss: 0.0000, val_loss: 6.1649, val_acc: 0.5773\n",
      "Epoch [1502], train_loss: 0.0000, val_loss: 6.1740, val_acc: 0.5851\n",
      "Epoch [1503], train_loss: 0.0000, val_loss: 6.1505, val_acc: 0.5851\n",
      "Epoch [1504], train_loss: 0.0000, val_loss: 6.4464, val_acc: 0.5825\n",
      "Epoch [1505], train_loss: 0.0000, val_loss: 6.1072, val_acc: 0.5799\n",
      "Epoch [1506], train_loss: 0.0000, val_loss: 6.4572, val_acc: 0.5799\n",
      "Epoch [1507], train_loss: 0.0000, val_loss: 6.2679, val_acc: 0.5799\n",
      "Epoch [1508], train_loss: 0.0000, val_loss: 6.4419, val_acc: 0.5825\n",
      "Epoch [1509], train_loss: 0.0000, val_loss: 6.3782, val_acc: 0.5825\n",
      "Epoch [1510], train_loss: 0.0049, val_loss: 5.9380, val_acc: 0.5773\n",
      "Epoch [1511], train_loss: 0.0403, val_loss: 5.0451, val_acc: 0.5828\n",
      "Epoch [1512], train_loss: 0.0062, val_loss: 5.2437, val_acc: 0.5910\n",
      "Epoch [1513], train_loss: 0.0003, val_loss: 5.2116, val_acc: 0.5988\n",
      "Epoch [1514], train_loss: 0.0002, val_loss: 5.5760, val_acc: 0.5936\n",
      "Epoch [1515], train_loss: 0.0001, val_loss: 5.4716, val_acc: 0.5988\n",
      "Epoch [1516], train_loss: 0.0001, val_loss: 5.6949, val_acc: 0.5910\n",
      "Epoch [1517], train_loss: 0.0005, val_loss: 5.8938, val_acc: 0.6069\n",
      "Epoch [1518], train_loss: 0.0001, val_loss: 5.7902, val_acc: 0.6124\n",
      "Epoch [1519], train_loss: 0.0001, val_loss: 5.8538, val_acc: 0.6124\n",
      "Epoch [1520], train_loss: 0.0000, val_loss: 5.9561, val_acc: 0.6153\n",
      "Epoch [1521], train_loss: 0.0000, val_loss: 5.8016, val_acc: 0.6179\n",
      "Epoch [1522], train_loss: 0.0054, val_loss: 6.1803, val_acc: 0.6095\n",
      "Epoch [1523], train_loss: 0.0296, val_loss: 2.7313, val_acc: 0.5884\n",
      "Epoch [1524], train_loss: 0.0086, val_loss: 4.1424, val_acc: 0.5910\n",
      "Epoch [1525], train_loss: 0.0040, val_loss: 4.6775, val_acc: 0.5955\n",
      "Epoch [1526], train_loss: 0.0018, val_loss: 5.0364, val_acc: 0.6118\n",
      "Epoch [1527], train_loss: 0.0004, val_loss: 5.0289, val_acc: 0.6043\n",
      "Epoch [1528], train_loss: 0.0001, val_loss: 5.2292, val_acc: 0.6069\n",
      "Epoch [1529], train_loss: 0.0001, val_loss: 5.2999, val_acc: 0.6147\n",
      "Epoch [1530], train_loss: 0.0000, val_loss: 5.3884, val_acc: 0.5962\n",
      "Epoch [1531], train_loss: 0.0002, val_loss: 5.4508, val_acc: 0.5884\n",
      "Epoch [1532], train_loss: 0.0008, val_loss: 5.7476, val_acc: 0.5962\n",
      "Epoch [1533], train_loss: 0.0011, val_loss: 6.6783, val_acc: 0.5962\n",
      "Epoch [1534], train_loss: 0.0138, val_loss: 3.5555, val_acc: 0.6092\n",
      "Epoch [1535], train_loss: 0.0028, val_loss: 4.3210, val_acc: 0.5880\n",
      "Epoch [1536], train_loss: 0.0003, val_loss: 4.8507, val_acc: 0.6040\n",
      "Epoch [1537], train_loss: 0.0001, val_loss: 5.0986, val_acc: 0.6014\n",
      "Epoch [1538], train_loss: 0.0001, val_loss: 5.1410, val_acc: 0.5988\n",
      "Epoch [1539], train_loss: 0.0000, val_loss: 4.9760, val_acc: 0.5936\n",
      "Epoch [1540], train_loss: 0.0000, val_loss: 5.0702, val_acc: 0.5936\n",
      "Epoch [1541], train_loss: 0.0001, val_loss: 5.1820, val_acc: 0.5910\n",
      "Epoch [1542], train_loss: 0.0000, val_loss: 5.3140, val_acc: 0.5962\n",
      "Epoch [1543], train_loss: 0.0000, val_loss: 5.2985, val_acc: 0.6040\n",
      "Epoch [1544], train_loss: 0.0000, val_loss: 5.3167, val_acc: 0.5962\n",
      "Epoch [1545], train_loss: 0.0000, val_loss: 5.5348, val_acc: 0.5962\n",
      "Epoch [1546], train_loss: 0.0000, val_loss: 5.4748, val_acc: 0.5962\n",
      "Epoch [1547], train_loss: 0.0001, val_loss: 5.4801, val_acc: 0.5988\n",
      "Epoch [1548], train_loss: 0.0000, val_loss: 5.6956, val_acc: 0.5988\n",
      "Epoch [1549], train_loss: 0.0000, val_loss: 5.5871, val_acc: 0.5962\n",
      "Epoch [1550], train_loss: 0.0000, val_loss: 5.6425, val_acc: 0.5962\n",
      "Epoch [1551], train_loss: 0.0000, val_loss: 5.5681, val_acc: 0.5936\n",
      "Epoch [1552], train_loss: 0.0000, val_loss: 5.6820, val_acc: 0.5910\n",
      "Epoch [1553], train_loss: 0.0000, val_loss: 5.7607, val_acc: 0.5936\n",
      "Epoch [1554], train_loss: 0.0000, val_loss: 5.7772, val_acc: 0.5910\n",
      "Epoch [1555], train_loss: 0.0000, val_loss: 5.8886, val_acc: 0.5962\n",
      "Epoch [1556], train_loss: 0.0000, val_loss: 5.8876, val_acc: 0.5936\n",
      "Epoch [1557], train_loss: 0.0000, val_loss: 5.7183, val_acc: 0.5936\n",
      "Epoch [1558], train_loss: 0.0000, val_loss: 5.7554, val_acc: 0.5936\n",
      "Epoch [1559], train_loss: 0.0000, val_loss: 6.0166, val_acc: 0.5936\n",
      "Epoch [1560], train_loss: 0.0000, val_loss: 6.0013, val_acc: 0.5988\n",
      "Epoch [1561], train_loss: 0.0000, val_loss: 6.0352, val_acc: 0.5936\n",
      "Epoch [1562], train_loss: 0.0000, val_loss: 6.0408, val_acc: 0.5988\n",
      "Epoch [1563], train_loss: 0.0000, val_loss: 6.0926, val_acc: 0.5988\n",
      "Epoch [1564], train_loss: 0.0000, val_loss: 5.8886, val_acc: 0.5936\n",
      "Epoch [1565], train_loss: 0.0000, val_loss: 6.0789, val_acc: 0.5962\n",
      "Epoch [1566], train_loss: 0.0000, val_loss: 6.1428, val_acc: 0.5910\n",
      "Epoch [1567], train_loss: 0.0000, val_loss: 6.1980, val_acc: 0.5936\n",
      "Epoch [1568], train_loss: 0.0000, val_loss: 6.1427, val_acc: 0.5962\n",
      "Epoch [1569], train_loss: 0.0000, val_loss: 6.1707, val_acc: 0.5962\n",
      "Epoch [1570], train_loss: 0.0000, val_loss: 6.1586, val_acc: 0.5962\n",
      "Epoch [1571], train_loss: 0.0000, val_loss: 6.0520, val_acc: 0.5988\n",
      "Epoch [1572], train_loss: 0.0000, val_loss: 6.1855, val_acc: 0.5962\n",
      "Epoch [1573], train_loss: 0.0052, val_loss: 5.8936, val_acc: 0.6014\n",
      "Epoch [1574], train_loss: 0.0019, val_loss: 5.6772, val_acc: 0.5962\n",
      "Epoch [1575], train_loss: 0.0050, val_loss: 5.9460, val_acc: 0.5929\n",
      "Epoch [1576], train_loss: 0.0146, val_loss: 3.2589, val_acc: 0.6144\n",
      "Epoch [1577], train_loss: 0.0124, val_loss: 4.7399, val_acc: 0.6176\n",
      "Epoch [1578], train_loss: 0.0075, val_loss: 4.1338, val_acc: 0.5988\n",
      "Epoch [1579], train_loss: 0.0011, val_loss: 5.2565, val_acc: 0.5880\n",
      "Epoch [1580], train_loss: 0.0001, val_loss: 5.4433, val_acc: 0.5906\n",
      "Epoch [1581], train_loss: 0.0000, val_loss: 5.6924, val_acc: 0.5854\n",
      "Epoch [1582], train_loss: 0.0000, val_loss: 5.6462, val_acc: 0.5773\n",
      "Epoch [1583], train_loss: 0.0000, val_loss: 5.8443, val_acc: 0.5854\n",
      "Epoch [1584], train_loss: 0.0000, val_loss: 5.8605, val_acc: 0.5906\n",
      "Epoch [1585], train_loss: 0.0000, val_loss: 5.9530, val_acc: 0.5854\n",
      "Epoch [1586], train_loss: 0.0000, val_loss: 5.9573, val_acc: 0.5854\n",
      "Epoch [1587], train_loss: 0.0000, val_loss: 6.1473, val_acc: 0.5936\n",
      "Epoch [1588], train_loss: 0.0000, val_loss: 6.1485, val_acc: 0.5880\n",
      "Epoch [1589], train_loss: 0.0000, val_loss: 6.3012, val_acc: 0.5854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1590], train_loss: 0.0000, val_loss: 6.3905, val_acc: 0.5880\n",
      "Epoch [1591], train_loss: 0.0000, val_loss: 6.3817, val_acc: 0.5854\n",
      "Epoch [1592], train_loss: 0.0000, val_loss: 6.4349, val_acc: 0.5880\n",
      "Epoch [1593], train_loss: 0.0000, val_loss: 6.3389, val_acc: 0.5962\n",
      "Epoch [1594], train_loss: 0.0000, val_loss: 6.4755, val_acc: 0.5962\n",
      "Epoch [1595], train_loss: 0.0000, val_loss: 6.5605, val_acc: 0.5906\n",
      "Epoch [1596], train_loss: 0.0000, val_loss: 6.4474, val_acc: 0.5962\n",
      "Epoch [1597], train_loss: 0.0000, val_loss: 6.5952, val_acc: 0.5854\n",
      "Epoch [1598], train_loss: 0.0000, val_loss: 6.6120, val_acc: 0.5962\n",
      "Epoch [1599], train_loss: 0.0000, val_loss: 6.6432, val_acc: 0.5988\n",
      "Epoch [1600], train_loss: 0.0000, val_loss: 6.8572, val_acc: 0.5962\n",
      "Epoch [1601], train_loss: 0.0000, val_loss: 6.8183, val_acc: 0.5906\n",
      "Epoch [1602], train_loss: 0.0000, val_loss: 6.5888, val_acc: 0.5988\n",
      "Epoch [1603], train_loss: 0.0000, val_loss: 6.5579, val_acc: 0.5988\n",
      "Epoch [1604], train_loss: 0.0000, val_loss: 6.8717, val_acc: 0.5936\n",
      "Epoch [1605], train_loss: 0.0000, val_loss: 6.8251, val_acc: 0.5906\n",
      "Epoch [1606], train_loss: 0.0000, val_loss: 6.7918, val_acc: 0.6014\n",
      "Epoch [1607], train_loss: 0.0000, val_loss: 6.8497, val_acc: 0.5962\n",
      "Epoch [1608], train_loss: 0.0000, val_loss: 6.8304, val_acc: 0.5962\n",
      "Epoch [1609], train_loss: 0.0000, val_loss: 6.8240, val_acc: 0.5906\n",
      "Epoch [1610], train_loss: 0.0000, val_loss: 7.1324, val_acc: 0.5884\n",
      "Epoch [1611], train_loss: 0.0261, val_loss: 6.6503, val_acc: 0.6072\n",
      "Epoch [1612], train_loss: 0.0656, val_loss: 2.7902, val_acc: 0.5861\n",
      "Epoch [1613], train_loss: 0.0193, val_loss: 3.5504, val_acc: 0.6202\n",
      "Epoch [1614], train_loss: 0.0086, val_loss: 3.4638, val_acc: 0.6528\n",
      "Epoch [1615], train_loss: 0.0015, val_loss: 4.3026, val_acc: 0.6284\n",
      "Epoch [1616], train_loss: 0.0022, val_loss: 4.3544, val_acc: 0.6284\n",
      "Epoch [1617], train_loss: 0.0042, val_loss: 3.8639, val_acc: 0.6524\n",
      "Epoch [1618], train_loss: 0.0004, val_loss: 4.0923, val_acc: 0.6576\n",
      "Epoch [1619], train_loss: 0.0002, val_loss: 4.3958, val_acc: 0.6550\n",
      "Epoch [1620], train_loss: 0.0001, val_loss: 4.4299, val_acc: 0.6550\n",
      "Epoch [1621], train_loss: 0.0001, val_loss: 4.5161, val_acc: 0.6550\n",
      "Epoch [1622], train_loss: 0.0001, val_loss: 4.6222, val_acc: 0.6498\n",
      "Epoch [1623], train_loss: 0.0001, val_loss: 4.6708, val_acc: 0.6550\n",
      "Epoch [1624], train_loss: 0.0001, val_loss: 4.8732, val_acc: 0.6576\n",
      "Epoch [1625], train_loss: 0.0000, val_loss: 4.8686, val_acc: 0.6576\n",
      "Epoch [1626], train_loss: 0.0001, val_loss: 5.0286, val_acc: 0.6550\n",
      "Epoch [1627], train_loss: 0.0001, val_loss: 4.9961, val_acc: 0.6576\n",
      "Epoch [1628], train_loss: 0.0000, val_loss: 5.2047, val_acc: 0.6524\n",
      "Epoch [1629], train_loss: 0.0000, val_loss: 5.2394, val_acc: 0.6576\n",
      "Epoch [1630], train_loss: 0.0000, val_loss: 5.2553, val_acc: 0.6550\n",
      "Epoch [1631], train_loss: 0.0000, val_loss: 5.2789, val_acc: 0.6550\n",
      "Epoch [1632], train_loss: 0.0000, val_loss: 5.4149, val_acc: 0.6550\n",
      "Epoch [1633], train_loss: 0.0000, val_loss: 5.2109, val_acc: 0.6603\n",
      "Epoch [1634], train_loss: 0.0000, val_loss: 5.3140, val_acc: 0.6576\n",
      "Epoch [1635], train_loss: 0.0000, val_loss: 5.4141, val_acc: 0.6550\n",
      "Epoch [1636], train_loss: 0.0000, val_loss: 5.3513, val_acc: 0.6576\n",
      "Epoch [1637], train_loss: 0.0000, val_loss: 5.4579, val_acc: 0.6524\n",
      "Epoch [1638], train_loss: 0.0000, val_loss: 5.5144, val_acc: 0.6498\n",
      "Epoch [1639], train_loss: 0.0000, val_loss: 5.4592, val_acc: 0.6443\n",
      "Epoch [1640], train_loss: 0.0000, val_loss: 5.6738, val_acc: 0.6524\n",
      "Epoch [1641], train_loss: 0.0000, val_loss: 5.7150, val_acc: 0.6469\n",
      "Epoch [1642], train_loss: 0.0000, val_loss: 5.7371, val_acc: 0.6443\n",
      "Epoch [1643], train_loss: 0.0000, val_loss: 5.6624, val_acc: 0.6576\n",
      "Epoch [1644], train_loss: 0.0000, val_loss: 5.8520, val_acc: 0.6550\n",
      "Epoch [1645], train_loss: 0.0000, val_loss: 5.7962, val_acc: 0.6524\n",
      "Epoch [1646], train_loss: 0.0000, val_loss: 5.7375, val_acc: 0.6524\n",
      "Epoch [1647], train_loss: 0.0000, val_loss: 5.6950, val_acc: 0.6469\n",
      "Epoch [1648], train_loss: 0.0000, val_loss: 5.8023, val_acc: 0.6469\n",
      "Epoch [1649], train_loss: 0.0000, val_loss: 5.9496, val_acc: 0.6443\n",
      "Epoch [1650], train_loss: 0.0000, val_loss: 5.8377, val_acc: 0.6469\n",
      "Epoch [1651], train_loss: 0.0000, val_loss: 5.8347, val_acc: 0.6498\n",
      "Epoch [1652], train_loss: 0.0000, val_loss: 5.8507, val_acc: 0.6524\n",
      "Epoch [1653], train_loss: 0.0000, val_loss: 5.8898, val_acc: 0.6443\n",
      "Epoch [1654], train_loss: 0.0000, val_loss: 5.8831, val_acc: 0.6524\n",
      "Epoch [1655], train_loss: 0.0000, val_loss: 5.9252, val_acc: 0.6443\n",
      "Epoch [1656], train_loss: 0.0000, val_loss: 5.9293, val_acc: 0.6443\n",
      "Epoch [1657], train_loss: 0.0000, val_loss: 5.9294, val_acc: 0.6550\n",
      "Epoch [1658], train_loss: 0.0000, val_loss: 6.1121, val_acc: 0.6469\n",
      "Epoch [1659], train_loss: 0.0000, val_loss: 6.0567, val_acc: 0.6443\n",
      "Epoch [1660], train_loss: 0.0000, val_loss: 6.0750, val_acc: 0.6550\n",
      "Epoch [1661], train_loss: 0.0000, val_loss: 6.1123, val_acc: 0.6524\n",
      "Epoch [1662], train_loss: 0.0000, val_loss: 6.1728, val_acc: 0.6524\n",
      "Epoch [1663], train_loss: 0.0000, val_loss: 6.1914, val_acc: 0.6524\n",
      "Epoch [1664], train_loss: 0.0000, val_loss: 6.1592, val_acc: 0.6524\n",
      "Epoch [1665], train_loss: 0.0000, val_loss: 6.1047, val_acc: 0.6524\n",
      "Epoch [1666], train_loss: 0.0000, val_loss: 6.2224, val_acc: 0.6443\n",
      "Epoch [1667], train_loss: 0.0000, val_loss: 6.2772, val_acc: 0.6443\n",
      "Epoch [1668], train_loss: 0.0000, val_loss: 6.1099, val_acc: 0.6443\n",
      "Epoch [1669], train_loss: 0.0000, val_loss: 6.3085, val_acc: 0.6443\n",
      "Epoch [1670], train_loss: 0.0000, val_loss: 6.2641, val_acc: 0.6443\n",
      "Epoch [1671], train_loss: 0.0000, val_loss: 6.2714, val_acc: 0.6524\n",
      "Epoch [1672], train_loss: 0.0000, val_loss: 6.3556, val_acc: 0.6524\n",
      "Epoch [1673], train_loss: 0.0000, val_loss: 6.3479, val_acc: 0.6443\n",
      "Epoch [1674], train_loss: 0.0000, val_loss: 6.3065, val_acc: 0.6524\n",
      "Epoch [1675], train_loss: 0.0000, val_loss: 6.1169, val_acc: 0.6391\n",
      "Epoch [1676], train_loss: 0.0000, val_loss: 6.3262, val_acc: 0.6391\n",
      "Epoch [1677], train_loss: 0.0000, val_loss: 6.2648, val_acc: 0.6446\n",
      "Epoch [1678], train_loss: 0.0000, val_loss: 6.1139, val_acc: 0.6391\n",
      "Epoch [1679], train_loss: 0.0000, val_loss: 6.2367, val_acc: 0.6446\n",
      "Epoch [1680], train_loss: 0.0000, val_loss: 6.2647, val_acc: 0.6365\n",
      "Epoch [1681], train_loss: 0.0000, val_loss: 6.3310, val_acc: 0.6365\n",
      "Epoch [1682], train_loss: 0.0000, val_loss: 6.3597, val_acc: 0.6365\n",
      "Epoch [1683], train_loss: 0.0343, val_loss: 5.7635, val_acc: 0.6147\n",
      "Epoch [1684], train_loss: 0.0468, val_loss: 3.8056, val_acc: 0.6095\n",
      "Epoch [1685], train_loss: 0.0200, val_loss: 3.2318, val_acc: 0.5962\n",
      "Epoch [1686], train_loss: 0.0123, val_loss: 3.9885, val_acc: 0.5913\n",
      "Epoch [1687], train_loss: 0.0023, val_loss: 4.2031, val_acc: 0.5913\n",
      "Epoch [1688], train_loss: 0.0005, val_loss: 4.5655, val_acc: 0.5880\n",
      "Epoch [1689], train_loss: 0.0003, val_loss: 4.9050, val_acc: 0.5932\n",
      "Epoch [1690], train_loss: 0.0006, val_loss: 5.6934, val_acc: 0.5880\n",
      "Epoch [1691], train_loss: 0.0119, val_loss: 5.0653, val_acc: 0.5929\n",
      "Epoch [1692], train_loss: 0.0351, val_loss: 3.7136, val_acc: 0.6176\n",
      "Epoch [1693], train_loss: 0.0047, val_loss: 3.6683, val_acc: 0.6228\n",
      "Epoch [1694], train_loss: 0.0019, val_loss: 4.1111, val_acc: 0.6121\n",
      "Epoch [1695], train_loss: 0.0004, val_loss: 4.3757, val_acc: 0.6362\n",
      "Epoch [1696], train_loss: 0.0001, val_loss: 4.4879, val_acc: 0.6254\n",
      "Epoch [1697], train_loss: 0.0001, val_loss: 4.5482, val_acc: 0.6173\n",
      "Epoch [1698], train_loss: 0.0006, val_loss: 4.3908, val_acc: 0.6336\n",
      "Epoch [1699], train_loss: 0.0002, val_loss: 4.4726, val_acc: 0.6336\n",
      "Epoch [1700], train_loss: 0.0518, val_loss: 4.4806, val_acc: 0.6173\n",
      "Epoch [1701], train_loss: 0.0178, val_loss: 3.8902, val_acc: 0.5965\n",
      "Epoch [1702], train_loss: 0.0020, val_loss: 4.1148, val_acc: 0.6072\n",
      "Epoch [1703], train_loss: 0.0013, val_loss: 4.6598, val_acc: 0.5994\n",
      "Epoch [1704], train_loss: 0.0003, val_loss: 4.7514, val_acc: 0.5994\n",
      "Epoch [1705], train_loss: 0.0004, val_loss: 4.7979, val_acc: 0.6020\n",
      "Epoch [1706], train_loss: 0.0001, val_loss: 5.1845, val_acc: 0.6095\n",
      "Epoch [1707], train_loss: 0.0001, val_loss: 5.1230, val_acc: 0.6095\n",
      "Epoch [1708], train_loss: 0.0001, val_loss: 5.0981, val_acc: 0.6069\n",
      "Epoch [1709], train_loss: 0.0000, val_loss: 5.1684, val_acc: 0.6150\n",
      "Epoch [1710], train_loss: 0.0000, val_loss: 5.1635, val_acc: 0.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1711], train_loss: 0.0001, val_loss: 4.8167, val_acc: 0.6150\n",
      "Epoch [1712], train_loss: 0.0001, val_loss: 4.8123, val_acc: 0.6124\n",
      "Epoch [1713], train_loss: 0.0000, val_loss: 4.8706, val_acc: 0.6098\n",
      "Epoch [1714], train_loss: 0.0000, val_loss: 4.8772, val_acc: 0.6098\n",
      "Epoch [1715], train_loss: 0.0001, val_loss: 4.9781, val_acc: 0.6072\n",
      "Epoch [1716], train_loss: 0.0000, val_loss: 5.0818, val_acc: 0.6072\n",
      "Epoch [1717], train_loss: 0.0000, val_loss: 5.0640, val_acc: 0.6098\n",
      "Epoch [1718], train_loss: 0.0000, val_loss: 5.1377, val_acc: 0.6098\n",
      "Epoch [1719], train_loss: 0.0000, val_loss: 5.1279, val_acc: 0.6072\n",
      "Epoch [1720], train_loss: 0.0000, val_loss: 4.9772, val_acc: 0.6072\n",
      "Epoch [1721], train_loss: 0.0048, val_loss: 5.3591, val_acc: 0.6098\n",
      "Epoch [1722], train_loss: 0.0112, val_loss: 4.4892, val_acc: 0.5906\n",
      "Epoch [1723], train_loss: 0.0016, val_loss: 4.8489, val_acc: 0.5880\n",
      "Epoch [1724], train_loss: 0.0022, val_loss: 4.5464, val_acc: 0.5799\n",
      "Epoch [1725], train_loss: 0.0002, val_loss: 4.2414, val_acc: 0.5747\n",
      "Epoch [1726], train_loss: 0.0001, val_loss: 4.3383, val_acc: 0.5747\n",
      "Epoch [1727], train_loss: 0.0001, val_loss: 4.4842, val_acc: 0.5828\n",
      "Epoch [1728], train_loss: 0.0001, val_loss: 4.5637, val_acc: 0.5854\n",
      "Epoch [1729], train_loss: 0.0001, val_loss: 4.7241, val_acc: 0.5802\n",
      "Epoch [1730], train_loss: 0.0000, val_loss: 4.6802, val_acc: 0.5854\n",
      "Epoch [1731], train_loss: 0.0039, val_loss: 4.4879, val_acc: 0.5962\n",
      "Epoch [1732], train_loss: 0.0229, val_loss: 3.5557, val_acc: 0.6362\n",
      "Epoch [1733], train_loss: 0.0026, val_loss: 3.7867, val_acc: 0.6072\n",
      "Epoch [1734], train_loss: 0.0008, val_loss: 4.0476, val_acc: 0.6232\n",
      "Epoch [1735], train_loss: 0.0002, val_loss: 4.1818, val_acc: 0.6153\n",
      "Epoch [1736], train_loss: 0.0001, val_loss: 4.2755, val_acc: 0.6153\n",
      "Epoch [1737], train_loss: 0.0001, val_loss: 4.2611, val_acc: 0.6127\n",
      "Epoch [1738], train_loss: 0.0001, val_loss: 4.2884, val_acc: 0.6206\n",
      "Epoch [1739], train_loss: 0.0013, val_loss: 4.1985, val_acc: 0.6258\n",
      "Epoch [1740], train_loss: 0.0065, val_loss: 4.4156, val_acc: 0.6127\n",
      "Epoch [1741], train_loss: 0.0164, val_loss: 3.5275, val_acc: 0.6095\n",
      "Epoch [1742], train_loss: 0.0066, val_loss: 3.3660, val_acc: 0.5877\n",
      "Epoch [1743], train_loss: 0.0006, val_loss: 3.6400, val_acc: 0.6092\n",
      "Epoch [1744], train_loss: 0.0002, val_loss: 3.8153, val_acc: 0.6014\n",
      "Epoch [1745], train_loss: 0.0002, val_loss: 4.1865, val_acc: 0.6092\n",
      "Epoch [1746], train_loss: 0.0001, val_loss: 4.2849, val_acc: 0.5932\n",
      "Epoch [1747], train_loss: 0.0001, val_loss: 4.3043, val_acc: 0.6014\n",
      "Epoch [1748], train_loss: 0.0001, val_loss: 4.3885, val_acc: 0.5932\n",
      "Epoch [1749], train_loss: 0.0000, val_loss: 4.5444, val_acc: 0.5958\n",
      "Epoch [1750], train_loss: 0.0006, val_loss: 4.3901, val_acc: 0.6092\n",
      "Epoch [1751], train_loss: 0.0001, val_loss: 4.4125, val_acc: 0.6173\n",
      "Epoch [1752], train_loss: 0.0001, val_loss: 4.5659, val_acc: 0.6225\n",
      "Epoch [1753], train_loss: 0.0003, val_loss: 4.6734, val_acc: 0.6147\n",
      "Epoch [1754], train_loss: 0.0001, val_loss: 4.7594, val_acc: 0.6014\n",
      "Epoch [1755], train_loss: 0.0000, val_loss: 4.9137, val_acc: 0.6014\n",
      "Epoch [1756], train_loss: 0.0001, val_loss: 5.0814, val_acc: 0.6063\n",
      "Epoch [1757], train_loss: 0.0000, val_loss: 5.1986, val_acc: 0.6066\n",
      "Epoch [1758], train_loss: 0.0000, val_loss: 5.0896, val_acc: 0.6092\n",
      "Epoch [1759], train_loss: 0.0000, val_loss: 5.2286, val_acc: 0.6092\n",
      "Epoch [1760], train_loss: 0.0000, val_loss: 5.3945, val_acc: 0.6092\n",
      "Epoch [1761], train_loss: 0.0000, val_loss: 5.0077, val_acc: 0.6092\n",
      "Epoch [1762], train_loss: 0.0000, val_loss: 5.1225, val_acc: 0.6092\n",
      "Epoch [1763], train_loss: 0.0000, val_loss: 5.1977, val_acc: 0.6092\n",
      "Epoch [1764], train_loss: 0.0000, val_loss: 5.1120, val_acc: 0.6066\n",
      "Epoch [1765], train_loss: 0.0000, val_loss: 5.2615, val_acc: 0.6118\n",
      "Epoch [1766], train_loss: 0.0000, val_loss: 5.3683, val_acc: 0.6092\n",
      "Epoch [1767], train_loss: 0.0000, val_loss: 5.3172, val_acc: 0.6118\n",
      "Epoch [1768], train_loss: 0.0000, val_loss: 5.2804, val_acc: 0.6092\n",
      "Epoch [1769], train_loss: 0.0000, val_loss: 5.5627, val_acc: 0.6066\n",
      "Epoch [1770], train_loss: 0.0000, val_loss: 5.4819, val_acc: 0.6066\n",
      "Epoch [1771], train_loss: 0.0000, val_loss: 5.4948, val_acc: 0.6066\n",
      "Epoch [1772], train_loss: 0.0000, val_loss: 5.3946, val_acc: 0.6066\n",
      "Epoch [1773], train_loss: 0.0000, val_loss: 5.4704, val_acc: 0.6066\n",
      "Epoch [1774], train_loss: 0.0000, val_loss: 5.5083, val_acc: 0.6092\n",
      "Epoch [1775], train_loss: 0.0000, val_loss: 5.4888, val_acc: 0.6040\n",
      "Epoch [1776], train_loss: 0.0000, val_loss: 5.4720, val_acc: 0.6092\n",
      "Epoch [1777], train_loss: 0.0000, val_loss: 5.5115, val_acc: 0.6092\n",
      "Epoch [1778], train_loss: 0.0000, val_loss: 5.7171, val_acc: 0.6040\n",
      "Epoch [1779], train_loss: 0.0000, val_loss: 5.4718, val_acc: 0.6066\n",
      "Epoch [1780], train_loss: 0.0000, val_loss: 5.6049, val_acc: 0.6092\n",
      "Epoch [1781], train_loss: 0.0000, val_loss: 5.8078, val_acc: 0.6066\n",
      "Epoch [1782], train_loss: 0.0000, val_loss: 5.6526, val_acc: 0.6092\n",
      "Epoch [1783], train_loss: 0.0000, val_loss: 5.5029, val_acc: 0.6066\n",
      "Epoch [1784], train_loss: 0.0000, val_loss: 5.7401, val_acc: 0.6092\n",
      "Epoch [1785], train_loss: 0.0000, val_loss: 5.8075, val_acc: 0.6040\n",
      "Epoch [1786], train_loss: 0.0000, val_loss: 5.7342, val_acc: 0.6092\n",
      "Epoch [1787], train_loss: 0.0000, val_loss: 5.6103, val_acc: 0.6118\n",
      "Epoch [1788], train_loss: 0.0000, val_loss: 5.7243, val_acc: 0.6066\n",
      "Epoch [1789], train_loss: 0.0000, val_loss: 5.6887, val_acc: 0.6066\n",
      "Epoch [1790], train_loss: 0.0000, val_loss: 5.7620, val_acc: 0.6118\n",
      "Epoch [1791], train_loss: 0.0000, val_loss: 5.7412, val_acc: 0.6118\n",
      "Epoch [1792], train_loss: 0.0000, val_loss: 5.8572, val_acc: 0.6066\n",
      "Epoch [1793], train_loss: 0.0131, val_loss: 5.2548, val_acc: 0.6118\n",
      "Epoch [1794], train_loss: 0.0189, val_loss: 5.0113, val_acc: 0.6121\n",
      "Epoch [1795], train_loss: 0.0255, val_loss: 4.3686, val_acc: 0.6043\n",
      "Epoch [1796], train_loss: 0.0043, val_loss: 4.7596, val_acc: 0.6150\n",
      "Epoch [1797], train_loss: 0.0010, val_loss: 5.1320, val_acc: 0.6017\n",
      "Epoch [1798], train_loss: 0.0003, val_loss: 5.3224, val_acc: 0.6069\n",
      "Epoch [1799], train_loss: 0.0001, val_loss: 5.5655, val_acc: 0.6095\n",
      "Epoch [1800], train_loss: 0.0001, val_loss: 5.7770, val_acc: 0.6173\n",
      "Epoch [1801], train_loss: 0.0001, val_loss: 5.8340, val_acc: 0.6173\n",
      "Epoch [1802], train_loss: 0.0000, val_loss: 6.0196, val_acc: 0.6199\n",
      "Epoch [1803], train_loss: 0.0000, val_loss: 5.9054, val_acc: 0.6199\n",
      "Epoch [1804], train_loss: 0.0040, val_loss: 4.6417, val_acc: 0.5910\n",
      "Epoch [1805], train_loss: 0.0128, val_loss: 4.3647, val_acc: 0.6014\n",
      "Epoch [1806], train_loss: 0.0044, val_loss: 4.5238, val_acc: 0.5965\n",
      "Epoch [1807], train_loss: 0.0108, val_loss: 4.3171, val_acc: 0.6359\n",
      "Epoch [1808], train_loss: 0.0052, val_loss: 3.6578, val_acc: 0.6225\n",
      "Epoch [1809], train_loss: 0.0125, val_loss: 3.2560, val_acc: 0.5880\n",
      "Epoch [1810], train_loss: 0.0015, val_loss: 3.8886, val_acc: 0.5985\n",
      "Epoch [1811], train_loss: 0.0003, val_loss: 4.4178, val_acc: 0.6011\n",
      "Epoch [1812], train_loss: 0.0002, val_loss: 4.4457, val_acc: 0.5958\n",
      "Epoch [1813], train_loss: 0.0001, val_loss: 4.6978, val_acc: 0.5958\n",
      "Epoch [1814], train_loss: 0.0001, val_loss: 4.8427, val_acc: 0.5932\n",
      "Epoch [1815], train_loss: 0.0003, val_loss: 4.9764, val_acc: 0.5958\n",
      "Epoch [1816], train_loss: 0.0011, val_loss: 4.9435, val_acc: 0.6011\n",
      "Epoch [1817], train_loss: 0.0001, val_loss: 5.1465, val_acc: 0.5958\n",
      "Epoch [1818], train_loss: 0.0001, val_loss: 5.2866, val_acc: 0.6037\n",
      "Epoch [1819], train_loss: 0.0001, val_loss: 5.3176, val_acc: 0.5903\n",
      "Epoch [1820], train_loss: 0.0000, val_loss: 5.4263, val_acc: 0.6118\n",
      "Epoch [1821], train_loss: 0.0000, val_loss: 5.4767, val_acc: 0.5903\n",
      "Epoch [1822], train_loss: 0.0000, val_loss: 5.5447, val_acc: 0.5985\n",
      "Epoch [1823], train_loss: 0.0000, val_loss: 5.6092, val_acc: 0.6011\n",
      "Epoch [1824], train_loss: 0.0000, val_loss: 5.6401, val_acc: 0.5958\n",
      "Epoch [1825], train_loss: 0.0000, val_loss: 5.6952, val_acc: 0.5929\n",
      "Epoch [1826], train_loss: 0.0000, val_loss: 5.5926, val_acc: 0.5958\n",
      "Epoch [1827], train_loss: 0.0000, val_loss: 5.9679, val_acc: 0.5906\n",
      "Epoch [1828], train_loss: 0.0000, val_loss: 5.7522, val_acc: 0.5877\n",
      "Epoch [1829], train_loss: 0.0000, val_loss: 5.8986, val_acc: 0.5851\n",
      "Epoch [1830], train_loss: 0.0000, val_loss: 5.8767, val_acc: 0.5903\n",
      "Epoch [1831], train_loss: 0.0000, val_loss: 5.9299, val_acc: 0.5932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1832], train_loss: 0.0000, val_loss: 6.0049, val_acc: 0.5874\n",
      "Epoch [1833], train_loss: 0.0000, val_loss: 6.0112, val_acc: 0.6011\n",
      "Epoch [1834], train_loss: 0.0000, val_loss: 5.9463, val_acc: 0.5877\n",
      "Epoch [1835], train_loss: 0.0000, val_loss: 6.0078, val_acc: 0.5796\n",
      "Epoch [1836], train_loss: 0.0000, val_loss: 6.0793, val_acc: 0.5903\n",
      "Epoch [1837], train_loss: 0.0000, val_loss: 6.0877, val_acc: 0.5903\n",
      "Epoch [1838], train_loss: 0.0000, val_loss: 6.1241, val_acc: 0.5851\n",
      "Epoch [1839], train_loss: 0.0000, val_loss: 6.1709, val_acc: 0.5825\n",
      "Epoch [1840], train_loss: 0.0000, val_loss: 6.0662, val_acc: 0.5929\n",
      "Epoch [1841], train_loss: 0.0000, val_loss: 6.1873, val_acc: 0.5877\n",
      "Epoch [1842], train_loss: 0.0000, val_loss: 6.1660, val_acc: 0.5985\n",
      "Epoch [1843], train_loss: 0.0000, val_loss: 6.2699, val_acc: 0.5958\n",
      "Epoch [1844], train_loss: 0.0000, val_loss: 6.2256, val_acc: 0.5929\n",
      "Epoch [1845], train_loss: 0.0000, val_loss: 6.1708, val_acc: 0.5851\n",
      "Epoch [1846], train_loss: 0.0000, val_loss: 6.2163, val_acc: 0.5929\n",
      "Epoch [1847], train_loss: 0.0001, val_loss: 6.4818, val_acc: 0.5770\n",
      "Epoch [1848], train_loss: 0.0000, val_loss: 6.4976, val_acc: 0.5903\n",
      "Epoch [1849], train_loss: 0.0000, val_loss: 6.4289, val_acc: 0.5744\n",
      "Epoch [1850], train_loss: 0.0000, val_loss: 6.4901, val_acc: 0.5877\n",
      "Epoch [1851], train_loss: 0.0000, val_loss: 6.5083, val_acc: 0.5932\n",
      "Epoch [1852], train_loss: 0.0000, val_loss: 6.5551, val_acc: 0.5796\n",
      "Epoch [1853], train_loss: 0.0000, val_loss: 6.7027, val_acc: 0.5822\n",
      "Epoch [1854], train_loss: 0.0001, val_loss: 7.0276, val_acc: 0.5906\n",
      "Epoch [1855], train_loss: 0.0000, val_loss: 6.9347, val_acc: 0.6118\n",
      "Epoch [1856], train_loss: 0.0000, val_loss: 6.9890, val_acc: 0.6014\n",
      "Epoch [1857], train_loss: 0.0000, val_loss: 6.7875, val_acc: 0.6014\n",
      "Epoch [1858], train_loss: 0.0000, val_loss: 6.7971, val_acc: 0.6014\n",
      "Epoch [1859], train_loss: 0.0000, val_loss: 6.6562, val_acc: 0.6014\n",
      "Epoch [1860], train_loss: 0.0000, val_loss: 6.9513, val_acc: 0.6040\n",
      "Epoch [1861], train_loss: 0.0000, val_loss: 6.9530, val_acc: 0.6014\n",
      "Epoch [1862], train_loss: 0.0000, val_loss: 6.7360, val_acc: 0.6066\n",
      "Epoch [1863], train_loss: 0.0000, val_loss: 7.0816, val_acc: 0.6040\n",
      "Epoch [1864], train_loss: 0.0000, val_loss: 6.9243, val_acc: 0.6066\n",
      "Epoch [1865], train_loss: 0.0000, val_loss: 6.9894, val_acc: 0.6066\n",
      "Epoch [1866], train_loss: 0.0000, val_loss: 6.8771, val_acc: 0.6092\n",
      "Epoch [1867], train_loss: 0.0000, val_loss: 6.9362, val_acc: 0.6118\n",
      "Epoch [1868], train_loss: 0.0000, val_loss: 6.8664, val_acc: 0.6066\n",
      "Epoch [1869], train_loss: 0.0000, val_loss: 6.9302, val_acc: 0.6040\n",
      "Epoch [1870], train_loss: 0.0000, val_loss: 7.0277, val_acc: 0.6040\n",
      "Epoch [1871], train_loss: 0.0000, val_loss: 7.0575, val_acc: 0.6066\n",
      "Epoch [1872], train_loss: 0.0000, val_loss: 6.9028, val_acc: 0.6118\n",
      "Epoch [1873], train_loss: 0.0000, val_loss: 7.0713, val_acc: 0.6066\n",
      "Epoch [1874], train_loss: 0.0000, val_loss: 7.0870, val_acc: 0.6040\n",
      "Epoch [1875], train_loss: 0.0000, val_loss: 6.9589, val_acc: 0.6092\n",
      "Epoch [1876], train_loss: 0.0000, val_loss: 7.1670, val_acc: 0.6040\n",
      "Epoch [1877], train_loss: 0.0000, val_loss: 7.1190, val_acc: 0.6092\n",
      "Epoch [1878], train_loss: 0.0000, val_loss: 6.9781, val_acc: 0.5932\n",
      "Epoch [1879], train_loss: 0.0054, val_loss: 5.3525, val_acc: 0.5962\n",
      "Epoch [1880], train_loss: 0.0003, val_loss: 5.8582, val_acc: 0.6043\n",
      "Epoch [1881], train_loss: 0.0009, val_loss: 6.3773, val_acc: 0.5932\n",
      "Epoch [1882], train_loss: 0.0038, val_loss: 6.2423, val_acc: 0.5906\n",
      "Epoch [1883], train_loss: 0.0140, val_loss: 8.2655, val_acc: 0.5239\n",
      "Epoch [1884], train_loss: 0.0370, val_loss: 4.9705, val_acc: 0.6147\n",
      "Epoch [1885], train_loss: 0.0236, val_loss: 3.5190, val_acc: 0.5910\n",
      "Epoch [1886], train_loss: 0.0073, val_loss: 3.7285, val_acc: 0.5685\n",
      "Epoch [1887], train_loss: 0.0094, val_loss: 3.3268, val_acc: 0.5822\n",
      "Epoch [1888], train_loss: 0.0075, val_loss: 3.9260, val_acc: 0.5848\n",
      "Epoch [1889], train_loss: 0.0026, val_loss: 4.2426, val_acc: 0.5900\n",
      "Epoch [1890], train_loss: 0.0004, val_loss: 4.1668, val_acc: 0.5952\n",
      "Epoch [1891], train_loss: 0.0001, val_loss: 4.2819, val_acc: 0.5897\n",
      "Epoch [1892], train_loss: 0.0001, val_loss: 4.3378, val_acc: 0.5978\n",
      "Epoch [1893], train_loss: 0.0000, val_loss: 4.4677, val_acc: 0.5926\n",
      "Epoch [1894], train_loss: 0.0000, val_loss: 4.5212, val_acc: 0.5952\n",
      "Epoch [1895], train_loss: 0.0001, val_loss: 4.6881, val_acc: 0.6007\n",
      "Epoch [1896], train_loss: 0.0000, val_loss: 4.9562, val_acc: 0.5900\n",
      "Epoch [1897], train_loss: 0.0000, val_loss: 4.8190, val_acc: 0.5819\n",
      "Epoch [1898], train_loss: 0.0000, val_loss: 4.8932, val_acc: 0.5900\n",
      "Epoch [1899], train_loss: 0.0000, val_loss: 5.0378, val_acc: 0.5900\n",
      "Epoch [1900], train_loss: 0.0000, val_loss: 5.0769, val_acc: 0.5848\n",
      "Epoch [1901], train_loss: 0.0000, val_loss: 5.0990, val_acc: 0.5926\n",
      "Epoch [1902], train_loss: 0.0000, val_loss: 5.1927, val_acc: 0.5981\n",
      "Epoch [1903], train_loss: 0.0000, val_loss: 5.2078, val_acc: 0.5874\n",
      "Epoch [1904], train_loss: 0.0000, val_loss: 5.0862, val_acc: 0.5845\n",
      "Epoch [1905], train_loss: 0.0000, val_loss: 5.1678, val_acc: 0.5871\n",
      "Epoch [1906], train_loss: 0.0000, val_loss: 5.3366, val_acc: 0.5981\n",
      "Epoch [1907], train_loss: 0.0000, val_loss: 5.1681, val_acc: 0.5874\n",
      "Epoch [1908], train_loss: 0.0000, val_loss: 5.4052, val_acc: 0.5874\n",
      "Epoch [1909], train_loss: 0.0000, val_loss: 5.3805, val_acc: 0.5900\n",
      "Epoch [1910], train_loss: 0.0000, val_loss: 5.2981, val_acc: 0.5874\n",
      "Epoch [1911], train_loss: 0.0000, val_loss: 5.3924, val_acc: 0.5903\n",
      "Epoch [1912], train_loss: 0.0000, val_loss: 5.3475, val_acc: 0.5900\n",
      "Epoch [1913], train_loss: 0.0000, val_loss: 5.3770, val_acc: 0.5952\n",
      "Epoch [1914], train_loss: 0.0000, val_loss: 5.4241, val_acc: 0.5903\n",
      "Epoch [1915], train_loss: 0.0000, val_loss: 5.5289, val_acc: 0.5874\n",
      "Epoch [1916], train_loss: 0.0000, val_loss: 5.4120, val_acc: 0.5900\n",
      "Epoch [1917], train_loss: 0.0000, val_loss: 5.4698, val_acc: 0.5926\n",
      "Epoch [1918], train_loss: 0.0000, val_loss: 5.4827, val_acc: 0.5900\n",
      "Epoch [1919], train_loss: 0.0000, val_loss: 5.4593, val_acc: 0.5926\n",
      "Epoch [1920], train_loss: 0.0000, val_loss: 5.5430, val_acc: 0.5955\n",
      "Epoch [1921], train_loss: 0.0000, val_loss: 5.5194, val_acc: 0.5903\n",
      "Epoch [1922], train_loss: 0.0000, val_loss: 5.3792, val_acc: 0.5926\n",
      "Epoch [1923], train_loss: 0.0000, val_loss: 5.4482, val_acc: 0.5981\n",
      "Epoch [1924], train_loss: 0.0000, val_loss: 5.5707, val_acc: 0.6007\n",
      "Epoch [1925], train_loss: 0.0000, val_loss: 5.7323, val_acc: 0.5900\n",
      "Epoch [1926], train_loss: 0.0000, val_loss: 5.6095, val_acc: 0.5845\n",
      "Epoch [1927], train_loss: 0.0000, val_loss: 5.7181, val_acc: 0.5903\n",
      "Epoch [1928], train_loss: 0.0000, val_loss: 5.8013, val_acc: 0.5929\n",
      "Epoch [1929], train_loss: 0.0000, val_loss: 5.9336, val_acc: 0.5929\n",
      "Epoch [1930], train_loss: 0.0000, val_loss: 5.7086, val_acc: 0.5848\n",
      "Epoch [1931], train_loss: 0.0000, val_loss: 5.7393, val_acc: 0.5929\n",
      "Epoch [1932], train_loss: 0.0000, val_loss: 5.7306, val_acc: 0.5900\n",
      "Epoch [1933], train_loss: 0.0000, val_loss: 5.7913, val_acc: 0.6033\n",
      "Epoch [1934], train_loss: 0.0000, val_loss: 5.8712, val_acc: 0.5955\n",
      "Epoch [1935], train_loss: 0.0000, val_loss: 5.8655, val_acc: 0.5900\n",
      "Epoch [1936], train_loss: 0.0000, val_loss: 5.8263, val_acc: 0.5874\n",
      "Epoch [1937], train_loss: 0.0000, val_loss: 5.9142, val_acc: 0.5981\n",
      "Epoch [1938], train_loss: 0.0000, val_loss: 5.9311, val_acc: 0.6033\n",
      "Epoch [1939], train_loss: 0.0000, val_loss: 5.9548, val_acc: 0.6007\n",
      "Epoch [1940], train_loss: 0.0000, val_loss: 5.9308, val_acc: 0.5929\n",
      "Epoch [1941], train_loss: 0.0000, val_loss: 5.9392, val_acc: 0.6007\n",
      "Epoch [1942], train_loss: 0.0000, val_loss: 5.8006, val_acc: 0.5900\n",
      "Epoch [1943], train_loss: 0.0000, val_loss: 6.0438, val_acc: 0.5929\n",
      "Epoch [1944], train_loss: 0.0000, val_loss: 5.9848, val_acc: 0.5981\n",
      "Epoch [1945], train_loss: 0.0000, val_loss: 6.0961, val_acc: 0.5981\n",
      "Epoch [1946], train_loss: 0.0000, val_loss: 6.0834, val_acc: 0.5955\n",
      "Epoch [1947], train_loss: 0.0000, val_loss: 6.1362, val_acc: 0.6011\n",
      "Epoch [1948], train_loss: 0.0000, val_loss: 5.9378, val_acc: 0.6037\n",
      "Epoch [1949], train_loss: 0.0000, val_loss: 6.0065, val_acc: 0.6007\n",
      "Epoch [1950], train_loss: 0.0000, val_loss: 6.0194, val_acc: 0.5981\n",
      "Epoch [1951], train_loss: 0.0000, val_loss: 6.0992, val_acc: 0.5955\n",
      "Epoch [1952], train_loss: 0.0000, val_loss: 6.0436, val_acc: 0.5955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1953], train_loss: 0.0000, val_loss: 5.8364, val_acc: 0.6063\n",
      "Epoch [1954], train_loss: 0.0000, val_loss: 6.1352, val_acc: 0.6033\n",
      "Epoch [1955], train_loss: 0.0000, val_loss: 6.2275, val_acc: 0.6007\n",
      "Epoch [1956], train_loss: 0.0000, val_loss: 6.0784, val_acc: 0.5955\n",
      "Epoch [1957], train_loss: 0.0000, val_loss: 5.9176, val_acc: 0.6089\n",
      "Epoch [1958], train_loss: 0.0000, val_loss: 6.3563, val_acc: 0.6007\n",
      "Epoch [1959], train_loss: 0.0000, val_loss: 6.1898, val_acc: 0.6037\n",
      "Epoch [1960], train_loss: 0.0000, val_loss: 6.2609, val_acc: 0.5981\n",
      "Epoch [1961], train_loss: 0.0000, val_loss: 6.1663, val_acc: 0.5981\n",
      "Epoch [1962], train_loss: 0.0000, val_loss: 6.2431, val_acc: 0.6007\n",
      "Epoch [1963], train_loss: 0.0000, val_loss: 6.3215, val_acc: 0.5955\n",
      "Epoch [1964], train_loss: 0.0000, val_loss: 6.3864, val_acc: 0.6033\n",
      "Epoch [1965], train_loss: 0.0000, val_loss: 6.3450, val_acc: 0.6033\n",
      "Epoch [1966], train_loss: 0.0000, val_loss: 6.3444, val_acc: 0.6011\n",
      "Epoch [1967], train_loss: 0.0000, val_loss: 6.3022, val_acc: 0.6007\n",
      "Epoch [1968], train_loss: 0.0000, val_loss: 6.3564, val_acc: 0.6033\n",
      "Epoch [1969], train_loss: 0.0211, val_loss: 6.6474, val_acc: 0.5822\n",
      "Epoch [1970], train_loss: 0.0612, val_loss: 2.9655, val_acc: 0.5588\n",
      "Epoch [1971], train_loss: 0.0177, val_loss: 3.9615, val_acc: 0.5822\n",
      "Epoch [1972], train_loss: 0.0032, val_loss: 5.0712, val_acc: 0.5689\n",
      "Epoch [1973], train_loss: 0.0008, val_loss: 5.4050, val_acc: 0.5741\n",
      "Epoch [1974], train_loss: 0.0013, val_loss: 5.4895, val_acc: 0.5793\n",
      "Epoch [1975], train_loss: 0.0005, val_loss: 5.5250, val_acc: 0.5767\n",
      "Epoch [1976], train_loss: 0.0001, val_loss: 5.6468, val_acc: 0.5845\n",
      "Epoch [1977], train_loss: 0.0000, val_loss: 5.7863, val_acc: 0.5793\n",
      "Epoch [1978], train_loss: 0.0000, val_loss: 5.7124, val_acc: 0.5793\n",
      "Epoch [1979], train_loss: 0.0000, val_loss: 5.8397, val_acc: 0.5819\n",
      "Epoch [1980], train_loss: 0.0000, val_loss: 5.8492, val_acc: 0.5900\n",
      "Epoch [1981], train_loss: 0.0000, val_loss: 5.9378, val_acc: 0.5900\n",
      "Epoch [1982], train_loss: 0.0000, val_loss: 5.9305, val_acc: 0.5819\n",
      "Epoch [1983], train_loss: 0.0000, val_loss: 5.9331, val_acc: 0.5793\n",
      "Epoch [1984], train_loss: 0.0000, val_loss: 6.1555, val_acc: 0.5793\n",
      "Epoch [1985], train_loss: 0.0000, val_loss: 6.1478, val_acc: 0.5900\n",
      "Epoch [1986], train_loss: 0.0000, val_loss: 6.1547, val_acc: 0.5874\n",
      "Epoch [1987], train_loss: 0.0000, val_loss: 6.2838, val_acc: 0.5952\n",
      "Epoch [1988], train_loss: 0.0000, val_loss: 6.2496, val_acc: 0.5819\n",
      "Epoch [1989], train_loss: 0.0000, val_loss: 6.2222, val_acc: 0.5819\n",
      "Epoch [1990], train_loss: 0.0000, val_loss: 6.3414, val_acc: 0.5926\n",
      "Epoch [1991], train_loss: 0.0000, val_loss: 6.2354, val_acc: 0.5926\n",
      "Epoch [1992], train_loss: 0.0000, val_loss: 6.3647, val_acc: 0.5819\n",
      "Epoch [1993], train_loss: 0.0034, val_loss: 6.3396, val_acc: 0.5926\n",
      "Epoch [1994], train_loss: 0.0117, val_loss: 3.6936, val_acc: 0.5744\n",
      "Epoch [1995], train_loss: 0.0158, val_loss: 3.3953, val_acc: 0.6011\n",
      "Epoch [1996], train_loss: 0.0054, val_loss: 4.1650, val_acc: 0.6037\n",
      "Epoch [1997], train_loss: 0.0032, val_loss: 4.5303, val_acc: 0.6170\n",
      "Epoch [1998], train_loss: 0.0048, val_loss: 4.0134, val_acc: 0.6196\n",
      "Epoch [1999], train_loss: 0.0080, val_loss: 3.2362, val_acc: 0.5877\n",
      "Epoch [2000], train_loss: 0.0031, val_loss: 4.0529, val_acc: 0.6033\n",
      "Epoch [2001], train_loss: 0.0004, val_loss: 4.5318, val_acc: 0.5952\n",
      "Epoch [2002], train_loss: 0.0001, val_loss: 4.8171, val_acc: 0.5926\n",
      "Epoch [2003], train_loss: 0.0002, val_loss: 4.9716, val_acc: 0.6007\n",
      "Epoch [2004], train_loss: 0.0001, val_loss: 4.9814, val_acc: 0.5926\n",
      "Epoch [2005], train_loss: 0.0001, val_loss: 5.2070, val_acc: 0.5926\n",
      "Epoch [2006], train_loss: 0.0000, val_loss: 5.4341, val_acc: 0.5955\n",
      "Epoch [2007], train_loss: 0.0001, val_loss: 5.4552, val_acc: 0.5978\n",
      "Epoch [2008], train_loss: 0.0000, val_loss: 5.4915, val_acc: 0.5926\n",
      "Epoch [2009], train_loss: 0.0000, val_loss: 5.5739, val_acc: 0.5952\n",
      "Epoch [2010], train_loss: 0.0000, val_loss: 5.5306, val_acc: 0.5926\n",
      "Epoch [2011], train_loss: 0.0000, val_loss: 5.6167, val_acc: 0.5952\n",
      "Epoch [2012], train_loss: 0.0000, val_loss: 5.7339, val_acc: 0.5952\n",
      "Epoch [2013], train_loss: 0.0000, val_loss: 5.7749, val_acc: 0.5952\n",
      "Epoch [2014], train_loss: 0.0000, val_loss: 5.7727, val_acc: 0.5978\n",
      "Epoch [2015], train_loss: 0.0003, val_loss: 5.5119, val_acc: 0.5877\n",
      "Epoch [2016], train_loss: 0.0002, val_loss: 5.8125, val_acc: 0.5770\n",
      "Epoch [2017], train_loss: 0.0000, val_loss: 5.8933, val_acc: 0.5796\n",
      "Epoch [2018], train_loss: 0.0000, val_loss: 6.0466, val_acc: 0.5770\n",
      "Epoch [2019], train_loss: 0.0000, val_loss: 6.0544, val_acc: 0.5796\n",
      "Epoch [2020], train_loss: 0.0000, val_loss: 6.1786, val_acc: 0.5822\n",
      "Epoch [2021], train_loss: 0.0000, val_loss: 6.1220, val_acc: 0.5822\n",
      "Epoch [2022], train_loss: 0.0000, val_loss: 6.1005, val_acc: 0.5874\n",
      "Epoch [2023], train_loss: 0.0000, val_loss: 6.1345, val_acc: 0.5848\n",
      "Epoch [2024], train_loss: 0.0000, val_loss: 6.1869, val_acc: 0.5822\n",
      "Epoch [2025], train_loss: 0.0000, val_loss: 6.2012, val_acc: 0.5770\n",
      "Epoch [2026], train_loss: 0.0000, val_loss: 6.2710, val_acc: 0.5796\n",
      "Epoch [2027], train_loss: 0.0000, val_loss: 6.2274, val_acc: 0.5796\n",
      "Epoch [2028], train_loss: 0.0000, val_loss: 6.2695, val_acc: 0.5796\n",
      "Epoch [2029], train_loss: 0.0000, val_loss: 6.2883, val_acc: 0.5770\n",
      "Epoch [2030], train_loss: 0.0000, val_loss: 6.3812, val_acc: 0.5770\n",
      "Epoch [2031], train_loss: 0.0000, val_loss: 6.3752, val_acc: 0.5770\n",
      "Epoch [2032], train_loss: 0.0000, val_loss: 6.3809, val_acc: 0.5796\n",
      "Epoch [2033], train_loss: 0.0000, val_loss: 6.4018, val_acc: 0.5822\n",
      "Epoch [2034], train_loss: 0.0000, val_loss: 6.3410, val_acc: 0.5796\n",
      "Epoch [2035], train_loss: 0.0000, val_loss: 6.2417, val_acc: 0.5796\n",
      "Epoch [2036], train_loss: 0.0000, val_loss: 6.3743, val_acc: 0.5822\n",
      "Epoch [2037], train_loss: 0.0114, val_loss: 6.7518, val_acc: 0.6011\n",
      "Epoch [2038], train_loss: 0.1073, val_loss: 2.3482, val_acc: 0.5991\n",
      "Epoch [2039], train_loss: 0.0140, val_loss: 3.2081, val_acc: 0.5831\n",
      "Epoch [2040], train_loss: 0.0038, val_loss: 3.7298, val_acc: 0.6098\n",
      "Epoch [2041], train_loss: 0.0009, val_loss: 4.3190, val_acc: 0.5884\n",
      "Epoch [2042], train_loss: 0.0003, val_loss: 4.8036, val_acc: 0.5884\n",
      "Epoch [2043], train_loss: 0.0002, val_loss: 5.2103, val_acc: 0.5721\n",
      "Epoch [2044], train_loss: 0.0001, val_loss: 5.2543, val_acc: 0.5828\n",
      "Epoch [2045], train_loss: 0.0001, val_loss: 5.4496, val_acc: 0.5802\n",
      "Epoch [2046], train_loss: 0.0000, val_loss: 5.3719, val_acc: 0.5936\n",
      "Epoch [2047], train_loss: 0.0001, val_loss: 5.4885, val_acc: 0.5802\n",
      "Epoch [2048], train_loss: 0.0000, val_loss: 5.8963, val_acc: 0.5695\n",
      "Epoch [2049], train_loss: 0.0000, val_loss: 5.9692, val_acc: 0.5880\n",
      "Epoch [2050], train_loss: 0.0000, val_loss: 5.8799, val_acc: 0.5991\n",
      "Epoch [2051], train_loss: 0.0000, val_loss: 6.0963, val_acc: 0.5906\n",
      "Epoch [2052], train_loss: 0.0026, val_loss: 6.2510, val_acc: 0.5773\n",
      "Epoch [2053], train_loss: 0.0179, val_loss: 4.0734, val_acc: 0.5750\n",
      "Epoch [2054], train_loss: 0.0151, val_loss: 3.5752, val_acc: 0.5851\n",
      "Epoch [2055], train_loss: 0.0012, val_loss: 5.0140, val_acc: 0.5718\n",
      "Epoch [2056], train_loss: 0.0002, val_loss: 5.4746, val_acc: 0.5799\n",
      "Epoch [2057], train_loss: 0.0001, val_loss: 5.6082, val_acc: 0.5692\n",
      "Epoch [2058], train_loss: 0.0000, val_loss: 5.7166, val_acc: 0.5828\n",
      "Epoch [2059], train_loss: 0.0001, val_loss: 5.7839, val_acc: 0.5773\n",
      "Epoch [2060], train_loss: 0.0000, val_loss: 5.8186, val_acc: 0.5773\n",
      "Epoch [2061], train_loss: 0.0001, val_loss: 5.9702, val_acc: 0.5773\n",
      "Epoch [2062], train_loss: 0.0000, val_loss: 6.0323, val_acc: 0.5747\n",
      "Epoch [2063], train_loss: 0.0000, val_loss: 6.1935, val_acc: 0.5799\n",
      "Epoch [2064], train_loss: 0.0000, val_loss: 6.3684, val_acc: 0.5799\n",
      "Epoch [2065], train_loss: 0.0000, val_loss: 6.5122, val_acc: 0.5854\n",
      "Epoch [2066], train_loss: 0.0000, val_loss: 6.2937, val_acc: 0.5854\n",
      "Epoch [2067], train_loss: 0.0000, val_loss: 6.5525, val_acc: 0.5747\n",
      "Epoch [2068], train_loss: 0.0000, val_loss: 6.4716, val_acc: 0.5854\n",
      "Epoch [2069], train_loss: 0.0000, val_loss: 6.6008, val_acc: 0.5773\n",
      "Epoch [2070], train_loss: 0.0000, val_loss: 6.6892, val_acc: 0.5910\n",
      "Epoch [2071], train_loss: 0.0000, val_loss: 6.7390, val_acc: 0.5854\n",
      "Epoch [2072], train_loss: 0.0000, val_loss: 6.6651, val_acc: 0.5828\n",
      "Epoch [2073], train_loss: 0.0000, val_loss: 7.0055, val_acc: 0.5936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2074], train_loss: 0.0000, val_loss: 6.6132, val_acc: 0.5747\n",
      "Epoch [2075], train_loss: 0.0000, val_loss: 6.6972, val_acc: 0.5854\n",
      "Epoch [2076], train_loss: 0.0000, val_loss: 6.9145, val_acc: 0.5880\n",
      "Epoch [2077], train_loss: 0.0000, val_loss: 6.8836, val_acc: 0.5880\n",
      "Epoch [2078], train_loss: 0.0000, val_loss: 6.9946, val_acc: 0.5773\n",
      "Epoch [2079], train_loss: 0.0000, val_loss: 6.8324, val_acc: 0.5854\n",
      "Epoch [2080], train_loss: 0.0000, val_loss: 6.9384, val_acc: 0.5854\n",
      "Epoch [2081], train_loss: 0.0000, val_loss: 6.9026, val_acc: 0.5773\n",
      "Epoch [2082], train_loss: 0.0000, val_loss: 7.0890, val_acc: 0.5747\n",
      "Epoch [2083], train_loss: 0.0000, val_loss: 7.0948, val_acc: 0.5721\n",
      "Epoch [2084], train_loss: 0.0000, val_loss: 6.9917, val_acc: 0.5747\n",
      "Epoch [2085], train_loss: 0.0000, val_loss: 7.2814, val_acc: 0.5854\n",
      "Epoch [2086], train_loss: 0.0000, val_loss: 7.2727, val_acc: 0.5721\n",
      "Epoch [2087], train_loss: 0.0000, val_loss: 7.2039, val_acc: 0.5666\n",
      "Epoch [2088], train_loss: 0.0000, val_loss: 7.3475, val_acc: 0.5721\n",
      "Epoch [2089], train_loss: 0.0000, val_loss: 7.4091, val_acc: 0.5880\n",
      "Epoch [2090], train_loss: 0.0000, val_loss: 7.4423, val_acc: 0.5773\n",
      "Epoch [2091], train_loss: 0.0000, val_loss: 7.3964, val_acc: 0.5721\n",
      "Epoch [2092], train_loss: 0.0000, val_loss: 7.4645, val_acc: 0.5721\n",
      "Epoch [2093], train_loss: 0.0000, val_loss: 7.3694, val_acc: 0.5880\n",
      "Epoch [2094], train_loss: 0.0000, val_loss: 7.6485, val_acc: 0.5776\n",
      "Epoch [2095], train_loss: 0.0000, val_loss: 7.5417, val_acc: 0.5695\n",
      "Epoch [2096], train_loss: 0.0000, val_loss: 7.4596, val_acc: 0.5669\n",
      "Epoch [2097], train_loss: 0.0000, val_loss: 7.6106, val_acc: 0.5747\n",
      "Epoch [2098], train_loss: 0.0000, val_loss: 7.6856, val_acc: 0.5828\n",
      "Epoch [2099], train_loss: 0.0000, val_loss: 7.5180, val_acc: 0.5669\n",
      "Epoch [2100], train_loss: 0.0000, val_loss: 7.6077, val_acc: 0.5666\n",
      "Epoch [2101], train_loss: 0.0000, val_loss: 7.7471, val_acc: 0.5828\n",
      "Epoch [2102], train_loss: 0.0000, val_loss: 7.6187, val_acc: 0.5773\n",
      "Epoch [2103], train_loss: 0.0000, val_loss: 7.7113, val_acc: 0.5747\n",
      "Epoch [2104], train_loss: 0.0000, val_loss: 7.6863, val_acc: 0.5747\n",
      "Epoch [2105], train_loss: 0.0000, val_loss: 8.0240, val_acc: 0.5828\n",
      "Epoch [2106], train_loss: 0.0000, val_loss: 7.7349, val_acc: 0.5828\n",
      "Epoch [2107], train_loss: 0.0000, val_loss: 7.8846, val_acc: 0.5695\n",
      "Epoch [2108], train_loss: 0.0000, val_loss: 7.7477, val_acc: 0.5721\n",
      "Epoch [2109], train_loss: 0.0000, val_loss: 7.8130, val_acc: 0.5695\n",
      "Epoch [2110], train_loss: 0.0000, val_loss: 7.8466, val_acc: 0.5721\n",
      "Epoch [2111], train_loss: 0.0000, val_loss: 7.9312, val_acc: 0.5828\n",
      "Epoch [2112], train_loss: 0.0000, val_loss: 7.8741, val_acc: 0.5828\n",
      "Epoch [2113], train_loss: 0.0000, val_loss: 7.9762, val_acc: 0.5802\n",
      "Epoch [2114], train_loss: 0.0001, val_loss: 7.8930, val_acc: 0.5854\n",
      "Epoch [2115], train_loss: 0.0770, val_loss: 2.6609, val_acc: 0.5805\n",
      "Epoch [2116], train_loss: 0.0415, val_loss: 3.6368, val_acc: 0.6037\n",
      "Epoch [2117], train_loss: 0.0106, val_loss: 4.0204, val_acc: 0.5796\n",
      "Epoch [2118], train_loss: 0.0060, val_loss: 4.8151, val_acc: 0.5480\n",
      "Epoch [2119], train_loss: 0.0016, val_loss: 5.2803, val_acc: 0.5877\n",
      "Epoch [2120], train_loss: 0.0003, val_loss: 5.6881, val_acc: 0.5825\n",
      "Epoch [2121], train_loss: 0.0001, val_loss: 6.0170, val_acc: 0.5744\n",
      "Epoch [2122], train_loss: 0.0001, val_loss: 5.9006, val_acc: 0.5773\n",
      "Epoch [2123], train_loss: 0.0000, val_loss: 5.8914, val_acc: 0.5747\n",
      "Epoch [2124], train_loss: 0.0003, val_loss: 5.9590, val_acc: 0.5692\n",
      "Epoch [2125], train_loss: 0.0008, val_loss: 6.5334, val_acc: 0.5985\n",
      "Epoch [2126], train_loss: 0.0013, val_loss: 7.0713, val_acc: 0.5747\n",
      "Epoch [2127], train_loss: 0.0028, val_loss: 7.4097, val_acc: 0.5614\n",
      "Epoch [2128], train_loss: 0.0102, val_loss: 6.3971, val_acc: 0.5932\n",
      "Epoch [2129], train_loss: 0.0220, val_loss: 3.7042, val_acc: 0.5750\n",
      "Epoch [2130], train_loss: 0.0025, val_loss: 4.8840, val_acc: 0.6011\n",
      "Epoch [2131], train_loss: 0.0091, val_loss: 4.1713, val_acc: 0.5741\n",
      "Epoch [2132], train_loss: 0.0019, val_loss: 4.0140, val_acc: 0.5718\n",
      "Epoch [2133], train_loss: 0.0010, val_loss: 4.5568, val_acc: 0.5666\n",
      "Epoch [2134], train_loss: 0.0001, val_loss: 4.7267, val_acc: 0.5770\n",
      "Epoch [2135], train_loss: 0.0001, val_loss: 4.7510, val_acc: 0.5929\n",
      "Epoch [2136], train_loss: 0.0002, val_loss: 4.8405, val_acc: 0.5796\n",
      "Epoch [2137], train_loss: 0.0001, val_loss: 5.2223, val_acc: 0.5744\n",
      "Epoch [2138], train_loss: 0.0002, val_loss: 5.4968, val_acc: 0.5744\n",
      "Epoch [2139], train_loss: 0.0001, val_loss: 5.5307, val_acc: 0.5744\n",
      "Epoch [2140], train_loss: 0.0001, val_loss: 5.8248, val_acc: 0.5796\n",
      "Epoch [2141], train_loss: 0.0000, val_loss: 5.8302, val_acc: 0.5796\n",
      "Epoch [2142], train_loss: 0.0000, val_loss: 5.9888, val_acc: 0.5770\n",
      "Epoch [2143], train_loss: 0.0000, val_loss: 5.8791, val_acc: 0.5744\n",
      "Epoch [2144], train_loss: 0.0000, val_loss: 5.8620, val_acc: 0.5718\n",
      "Epoch [2145], train_loss: 0.0000, val_loss: 6.0322, val_acc: 0.5692\n",
      "Epoch [2146], train_loss: 0.0000, val_loss: 6.0088, val_acc: 0.5718\n",
      "Epoch [2147], train_loss: 0.0000, val_loss: 5.9500, val_acc: 0.5770\n",
      "Epoch [2148], train_loss: 0.0000, val_loss: 6.1993, val_acc: 0.5744\n",
      "Epoch [2149], train_loss: 0.0000, val_loss: 6.0108, val_acc: 0.5796\n",
      "Epoch [2150], train_loss: 0.0000, val_loss: 6.0951, val_acc: 0.5718\n",
      "Epoch [2151], train_loss: 0.0000, val_loss: 6.1868, val_acc: 0.5744\n",
      "Epoch [2152], train_loss: 0.0001, val_loss: 6.2415, val_acc: 0.5744\n",
      "Epoch [2153], train_loss: 0.0000, val_loss: 6.3353, val_acc: 0.5659\n",
      "Epoch [2154], train_loss: 0.0000, val_loss: 6.1480, val_acc: 0.5770\n",
      "Epoch [2155], train_loss: 0.0000, val_loss: 6.3525, val_acc: 0.5715\n",
      "Epoch [2156], train_loss: 0.0000, val_loss: 6.3608, val_acc: 0.5715\n",
      "Epoch [2157], train_loss: 0.0000, val_loss: 6.4074, val_acc: 0.5715\n",
      "Epoch [2158], train_loss: 0.0000, val_loss: 6.4323, val_acc: 0.5689\n",
      "Epoch [2159], train_loss: 0.0000, val_loss: 6.5100, val_acc: 0.5689\n",
      "Epoch [2160], train_loss: 0.0000, val_loss: 6.3309, val_acc: 0.5689\n",
      "Epoch [2161], train_loss: 0.0000, val_loss: 6.4387, val_acc: 0.5689\n",
      "Epoch [2162], train_loss: 0.0000, val_loss: 6.5238, val_acc: 0.5689\n",
      "Epoch [2163], train_loss: 0.0000, val_loss: 6.4418, val_acc: 0.5715\n",
      "Epoch [2164], train_loss: 0.0000, val_loss: 6.4122, val_acc: 0.5715\n",
      "Epoch [2165], train_loss: 0.0000, val_loss: 6.7548, val_acc: 0.5715\n",
      "Epoch [2166], train_loss: 0.0000, val_loss: 6.5410, val_acc: 0.5715\n",
      "Epoch [2167], train_loss: 0.0000, val_loss: 6.7100, val_acc: 0.5662\n",
      "Epoch [2168], train_loss: 0.0000, val_loss: 6.7057, val_acc: 0.5715\n",
      "Epoch [2169], train_loss: 0.0000, val_loss: 6.4932, val_acc: 0.5741\n",
      "Epoch [2170], train_loss: 0.0000, val_loss: 6.8961, val_acc: 0.5689\n",
      "Epoch [2171], train_loss: 0.0000, val_loss: 6.7206, val_acc: 0.5662\n",
      "Epoch [2172], train_loss: 0.0000, val_loss: 6.7576, val_acc: 0.5689\n",
      "Epoch [2173], train_loss: 0.0000, val_loss: 6.8429, val_acc: 0.5715\n",
      "Epoch [2174], train_loss: 0.0000, val_loss: 6.8569, val_acc: 0.5662\n",
      "Epoch [2175], train_loss: 0.0000, val_loss: 6.5868, val_acc: 0.5689\n",
      "Epoch [2176], train_loss: 0.0000, val_loss: 6.8900, val_acc: 0.5689\n",
      "Epoch [2177], train_loss: 0.0000, val_loss: 6.6031, val_acc: 0.5715\n",
      "Epoch [2178], train_loss: 0.0000, val_loss: 6.8413, val_acc: 0.5662\n",
      "Epoch [2179], train_loss: 0.0000, val_loss: 6.9638, val_acc: 0.5715\n",
      "Epoch [2180], train_loss: 0.0000, val_loss: 6.9879, val_acc: 0.5689\n",
      "Epoch [2181], train_loss: 0.0000, val_loss: 6.9672, val_acc: 0.5689\n",
      "Epoch [2182], train_loss: 0.0000, val_loss: 6.9745, val_acc: 0.5715\n",
      "Epoch [2183], train_loss: 0.0000, val_loss: 7.1147, val_acc: 0.5715\n",
      "Epoch [2184], train_loss: 0.0000, val_loss: 6.9438, val_acc: 0.5715\n",
      "Epoch [2185], train_loss: 0.0000, val_loss: 7.0322, val_acc: 0.5715\n",
      "Epoch [2186], train_loss: 0.0000, val_loss: 6.8154, val_acc: 0.5689\n",
      "Epoch [2187], train_loss: 0.0000, val_loss: 6.9757, val_acc: 0.5715\n",
      "Epoch [2188], train_loss: 0.0000, val_loss: 7.0315, val_acc: 0.5689\n",
      "Epoch [2189], train_loss: 0.0000, val_loss: 7.1337, val_acc: 0.5689\n",
      "Epoch [2190], train_loss: 0.0000, val_loss: 6.9643, val_acc: 0.5689\n",
      "Epoch [2191], train_loss: 0.0000, val_loss: 7.2781, val_acc: 0.5662\n",
      "Epoch [2192], train_loss: 0.0000, val_loss: 6.9307, val_acc: 0.5689\n",
      "Epoch [2193], train_loss: 0.0000, val_loss: 7.2205, val_acc: 0.5689\n",
      "Epoch [2194], train_loss: 0.0000, val_loss: 7.2723, val_acc: 0.5689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2195], train_loss: 0.0000, val_loss: 7.2003, val_acc: 0.5662\n",
      "Epoch [2196], train_loss: 0.0000, val_loss: 7.2779, val_acc: 0.5715\n",
      "Epoch [2197], train_loss: 0.0000, val_loss: 7.1878, val_acc: 0.5741\n",
      "Epoch [2198], train_loss: 0.0000, val_loss: 7.1862, val_acc: 0.5662\n",
      "Epoch [2199], train_loss: 0.0000, val_loss: 7.2005, val_acc: 0.5715\n",
      "Epoch [2200], train_loss: 0.0000, val_loss: 7.2984, val_acc: 0.5662\n",
      "Epoch [2201], train_loss: 0.0000, val_loss: 7.3323, val_acc: 0.5689\n",
      "Epoch [2202], train_loss: 0.0000, val_loss: 7.2001, val_acc: 0.5715\n",
      "Epoch [2203], train_loss: 0.0000, val_loss: 7.5202, val_acc: 0.5689\n",
      "Epoch [2204], train_loss: 0.0000, val_loss: 7.2717, val_acc: 0.5770\n",
      "Epoch [2205], train_loss: 0.0000, val_loss: 7.3654, val_acc: 0.5689\n",
      "Epoch [2206], train_loss: 0.0000, val_loss: 7.2501, val_acc: 0.5689\n",
      "Epoch [2207], train_loss: 0.0000, val_loss: 7.1702, val_acc: 0.5715\n",
      "Epoch [2208], train_loss: 0.0000, val_loss: 7.3189, val_acc: 0.5662\n",
      "Epoch [2209], train_loss: 0.0000, val_loss: 7.0829, val_acc: 0.5689\n",
      "Epoch [2210], train_loss: 0.0000, val_loss: 7.3583, val_acc: 0.5689\n",
      "Epoch [2211], train_loss: 0.0000, val_loss: 7.2825, val_acc: 0.5689\n",
      "Epoch [2212], train_loss: 0.0000, val_loss: 7.5104, val_acc: 0.5689\n",
      "Epoch [2213], train_loss: 0.0000, val_loss: 7.4787, val_acc: 0.5715\n",
      "Epoch [2214], train_loss: 0.0000, val_loss: 7.5841, val_acc: 0.5689\n",
      "Epoch [2215], train_loss: 0.0000, val_loss: 7.7323, val_acc: 0.5636\n",
      "Epoch [2216], train_loss: 0.0000, val_loss: 7.3608, val_acc: 0.5715\n",
      "Epoch [2217], train_loss: 0.0000, val_loss: 7.2655, val_acc: 0.5607\n",
      "Epoch [2218], train_loss: 0.0000, val_loss: 7.3875, val_acc: 0.5662\n",
      "Epoch [2219], train_loss: 0.0000, val_loss: 7.7011, val_acc: 0.5636\n",
      "Epoch [2220], train_loss: 0.0000, val_loss: 7.6967, val_acc: 0.5636\n",
      "Epoch [2221], train_loss: 0.0000, val_loss: 7.6204, val_acc: 0.5662\n",
      "Epoch [2222], train_loss: 0.0000, val_loss: 7.7954, val_acc: 0.5689\n",
      "Epoch [2223], train_loss: 0.0000, val_loss: 7.9736, val_acc: 0.5689\n",
      "Epoch [2224], train_loss: 0.0000, val_loss: 7.7239, val_acc: 0.5715\n",
      "Epoch [2225], train_loss: 0.0000, val_loss: 7.5976, val_acc: 0.5636\n",
      "Epoch [2226], train_loss: 0.0000, val_loss: 7.7734, val_acc: 0.5689\n",
      "Epoch [2227], train_loss: 0.0000, val_loss: 7.7983, val_acc: 0.5662\n",
      "Epoch [2228], train_loss: 0.0000, val_loss: 7.8582, val_acc: 0.5662\n",
      "Epoch [2229], train_loss: 0.0000, val_loss: 7.7118, val_acc: 0.5689\n",
      "Epoch [2230], train_loss: 0.0000, val_loss: 7.9102, val_acc: 0.5689\n",
      "Epoch [2231], train_loss: 0.0000, val_loss: 7.7197, val_acc: 0.5636\n",
      "Epoch [2232], train_loss: 0.0000, val_loss: 7.7924, val_acc: 0.5662\n",
      "Epoch [2233], train_loss: 0.0000, val_loss: 7.7106, val_acc: 0.5636\n",
      "Epoch [2234], train_loss: 0.0000, val_loss: 7.8944, val_acc: 0.5636\n",
      "Epoch [2235], train_loss: 0.0000, val_loss: 7.9773, val_acc: 0.5636\n",
      "Epoch [2236], train_loss: 0.0000, val_loss: 7.9600, val_acc: 0.5636\n",
      "Epoch [2237], train_loss: 0.0000, val_loss: 7.9716, val_acc: 0.5636\n",
      "Epoch [2238], train_loss: 0.0000, val_loss: 7.8561, val_acc: 0.5662\n",
      "Epoch [2239], train_loss: 0.0000, val_loss: 7.9963, val_acc: 0.5662\n",
      "Epoch [2240], train_loss: 0.0000, val_loss: 8.0614, val_acc: 0.5636\n",
      "Epoch [2241], train_loss: 0.0000, val_loss: 7.7166, val_acc: 0.5662\n",
      "Epoch [2242], train_loss: 0.0000, val_loss: 8.0516, val_acc: 0.5662\n",
      "Epoch [2243], train_loss: 0.0000, val_loss: 8.2082, val_acc: 0.5636\n",
      "Epoch [2244], train_loss: 0.0000, val_loss: 8.1776, val_acc: 0.5689\n",
      "Epoch [2245], train_loss: 0.0000, val_loss: 7.9500, val_acc: 0.5662\n",
      "Epoch [2246], train_loss: 0.0000, val_loss: 7.9665, val_acc: 0.5581\n",
      "Epoch [2247], train_loss: 0.0000, val_loss: 8.0708, val_acc: 0.5610\n",
      "Epoch [2248], train_loss: 0.0000, val_loss: 8.0327, val_acc: 0.5555\n",
      "Epoch [2249], train_loss: 0.0000, val_loss: 8.0325, val_acc: 0.5610\n",
      "Epoch [2250], train_loss: 0.0000, val_loss: 8.1189, val_acc: 0.5610\n",
      "Epoch [2251], train_loss: 0.0000, val_loss: 8.0697, val_acc: 0.5636\n",
      "Epoch [2252], train_loss: 0.0000, val_loss: 7.9775, val_acc: 0.5636\n",
      "Epoch [2253], train_loss: 0.0000, val_loss: 8.1183, val_acc: 0.5636\n",
      "Epoch [2254], train_loss: 0.0000, val_loss: 8.1540, val_acc: 0.5555\n",
      "Epoch [2255], train_loss: 0.0000, val_loss: 7.9647, val_acc: 0.5662\n",
      "Epoch [2256], train_loss: 0.0000, val_loss: 8.1017, val_acc: 0.5610\n",
      "Epoch [2257], train_loss: 0.0000, val_loss: 8.3466, val_acc: 0.5610\n",
      "Epoch [2258], train_loss: 0.0000, val_loss: 8.1247, val_acc: 0.5636\n",
      "Epoch [2259], train_loss: 0.0000, val_loss: 8.0137, val_acc: 0.5636\n",
      "Epoch [2260], train_loss: 0.0000, val_loss: 7.9312, val_acc: 0.5636\n",
      "Epoch [2261], train_loss: 0.0000, val_loss: 8.3469, val_acc: 0.5662\n",
      "Epoch [2262], train_loss: 0.0000, val_loss: 8.2029, val_acc: 0.5584\n",
      "Epoch [2263], train_loss: 0.0000, val_loss: 8.5706, val_acc: 0.5636\n",
      "Epoch [2264], train_loss: 0.0000, val_loss: 9.1971, val_acc: 0.5666\n",
      "Epoch [2265], train_loss: 0.0000, val_loss: 9.0303, val_acc: 0.5610\n",
      "Epoch [2266], train_loss: 0.0000, val_loss: 8.8775, val_acc: 0.5503\n",
      "Epoch [2267], train_loss: 0.0000, val_loss: 9.3564, val_acc: 0.5529\n",
      "Epoch [2268], train_loss: 0.0000, val_loss: 9.5458, val_acc: 0.5503\n",
      "Epoch [2269], train_loss: 0.0000, val_loss: 9.3841, val_acc: 0.5477\n",
      "Epoch [2270], train_loss: 0.0000, val_loss: 9.2214, val_acc: 0.5503\n",
      "Epoch [2271], train_loss: 0.0000, val_loss: 9.5157, val_acc: 0.5529\n",
      "Epoch [2272], train_loss: 0.0000, val_loss: 9.1890, val_acc: 0.5529\n",
      "Epoch [2273], train_loss: 0.0000, val_loss: 9.4831, val_acc: 0.5555\n",
      "Epoch [2274], train_loss: 0.0000, val_loss: 9.4637, val_acc: 0.5477\n",
      "Epoch [2275], train_loss: 0.0000, val_loss: 9.5348, val_acc: 0.5503\n",
      "Epoch [2276], train_loss: 0.0000, val_loss: 9.5787, val_acc: 0.5477\n",
      "Epoch [2277], train_loss: 0.0000, val_loss: 9.6925, val_acc: 0.5555\n",
      "Epoch [2278], train_loss: 0.0000, val_loss: 9.2990, val_acc: 0.5555\n",
      "Epoch [2279], train_loss: 0.0000, val_loss: 9.5372, val_acc: 0.5503\n",
      "Epoch [2280], train_loss: 0.0000, val_loss: 9.6769, val_acc: 0.5529\n",
      "Epoch [2281], train_loss: 0.0000, val_loss: 9.5610, val_acc: 0.5503\n",
      "Epoch [2282], train_loss: 0.0000, val_loss: 9.5927, val_acc: 0.5503\n",
      "Epoch [2283], train_loss: 0.0000, val_loss: 9.4131, val_acc: 0.5477\n",
      "Epoch [2284], train_loss: 0.0000, val_loss: 9.8286, val_acc: 0.5529\n",
      "Epoch [2285], train_loss: 0.0000, val_loss: 9.2981, val_acc: 0.5503\n",
      "Epoch [2286], train_loss: 0.0000, val_loss: 9.7145, val_acc: 0.5503\n",
      "Epoch [2287], train_loss: 0.0000, val_loss: 9.4926, val_acc: 0.5503\n",
      "Epoch [2288], train_loss: 0.0000, val_loss: 9.6884, val_acc: 0.5503\n",
      "Epoch [2289], train_loss: 0.0000, val_loss: 9.7281, val_acc: 0.5555\n",
      "Epoch [2290], train_loss: 0.0000, val_loss: 9.6106, val_acc: 0.5451\n",
      "Epoch [2291], train_loss: 0.0000, val_loss: 9.5267, val_acc: 0.5529\n",
      "Epoch [2292], train_loss: 0.0000, val_loss: 9.3391, val_acc: 0.5477\n",
      "Epoch [2293], train_loss: 0.0000, val_loss: 9.7521, val_acc: 0.5477\n",
      "Epoch [2294], train_loss: 0.0000, val_loss: 9.7467, val_acc: 0.5477\n",
      "Epoch [2295], train_loss: 0.0000, val_loss: 9.8076, val_acc: 0.5503\n",
      "Epoch [2296], train_loss: 0.0000, val_loss: 9.6792, val_acc: 0.5503\n",
      "Epoch [2297], train_loss: 0.0000, val_loss: 9.7032, val_acc: 0.5503\n",
      "Epoch [2298], train_loss: 0.0000, val_loss: 9.7835, val_acc: 0.5477\n",
      "Epoch [2299], train_loss: 0.0000, val_loss: 9.5584, val_acc: 0.5451\n",
      "Epoch [2300], train_loss: 0.0000, val_loss: 9.8818, val_acc: 0.5503\n",
      "Epoch [2301], train_loss: 0.0000, val_loss: 9.7800, val_acc: 0.5477\n",
      "Epoch [2302], train_loss: 0.0000, val_loss: 9.7464, val_acc: 0.5477\n",
      "Epoch [2303], train_loss: 0.0000, val_loss: 9.5971, val_acc: 0.5529\n",
      "Epoch [2304], train_loss: 0.0000, val_loss: 9.9077, val_acc: 0.5503\n",
      "Epoch [2305], train_loss: 0.0000, val_loss: 9.5863, val_acc: 0.5503\n",
      "Epoch [2306], train_loss: 0.0000, val_loss: 9.4923, val_acc: 0.5503\n",
      "Epoch [2307], train_loss: 0.0000, val_loss: 9.8692, val_acc: 0.5477\n",
      "Epoch [2308], train_loss: 0.0000, val_loss: 9.7968, val_acc: 0.5503\n",
      "Epoch [2309], train_loss: 0.0000, val_loss: 9.7271, val_acc: 0.5529\n",
      "Epoch [2310], train_loss: 0.0000, val_loss: 9.6144, val_acc: 0.5477\n",
      "Epoch [2311], train_loss: 0.0000, val_loss: 9.9615, val_acc: 0.5503\n",
      "Epoch [2312], train_loss: 0.0000, val_loss: 9.8098, val_acc: 0.5451\n",
      "Epoch [2313], train_loss: 0.0000, val_loss: 10.0601, val_acc: 0.5477\n",
      "Epoch [2314], train_loss: 0.0000, val_loss: 9.7613, val_acc: 0.5503\n",
      "Epoch [2315], train_loss: 0.0000, val_loss: 10.0184, val_acc: 0.5503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2316], train_loss: 0.0000, val_loss: 9.7952, val_acc: 0.5503\n",
      "Epoch [2317], train_loss: 0.0000, val_loss: 9.9400, val_acc: 0.5529\n",
      "Epoch [2318], train_loss: 0.0000, val_loss: 9.8864, val_acc: 0.5503\n",
      "Epoch [2319], train_loss: 0.0000, val_loss: 9.9861, val_acc: 0.5503\n",
      "Epoch [2320], train_loss: 0.0000, val_loss: 9.9725, val_acc: 0.5610\n",
      "Epoch [2321], train_loss: 0.0000, val_loss: 10.1679, val_acc: 0.5558\n",
      "Epoch [2322], train_loss: 0.0000, val_loss: 10.2202, val_acc: 0.5610\n",
      "Epoch [2323], train_loss: 0.0000, val_loss: 9.9192, val_acc: 0.5529\n",
      "Epoch [2324], train_loss: 0.0000, val_loss: 10.2289, val_acc: 0.5503\n",
      "Epoch [2325], train_loss: 0.0000, val_loss: 10.2375, val_acc: 0.5555\n",
      "Epoch [2326], train_loss: 0.0000, val_loss: 9.9557, val_acc: 0.5529\n",
      "Epoch [2327], train_loss: 0.0000, val_loss: 10.2271, val_acc: 0.5529\n",
      "Epoch [2328], train_loss: 0.0002, val_loss: 10.3082, val_acc: 0.5503\n",
      "Epoch [2329], train_loss: 0.0457, val_loss: 7.7870, val_acc: 0.5880\n",
      "Epoch [2330], train_loss: 0.1361, val_loss: 1.7441, val_acc: 0.6040\n",
      "Epoch [2331], train_loss: 0.0129, val_loss: 3.5372, val_acc: 0.5929\n",
      "Epoch [2332], train_loss: 0.0017, val_loss: 4.0781, val_acc: 0.5877\n",
      "Epoch [2333], train_loss: 0.0002, val_loss: 4.5653, val_acc: 0.5851\n",
      "Epoch [2334], train_loss: 0.0001, val_loss: 4.7904, val_acc: 0.5851\n",
      "Epoch [2335], train_loss: 0.0001, val_loss: 4.9134, val_acc: 0.5903\n",
      "Epoch [2336], train_loss: 0.0002, val_loss: 5.0349, val_acc: 0.5929\n",
      "Epoch [2337], train_loss: 0.0001, val_loss: 5.1513, val_acc: 0.5955\n",
      "Epoch [2338], train_loss: 0.0001, val_loss: 5.4532, val_acc: 0.5929\n",
      "Epoch [2339], train_loss: 0.0000, val_loss: 5.6284, val_acc: 0.5929\n",
      "Epoch [2340], train_loss: 0.0000, val_loss: 5.6824, val_acc: 0.5903\n",
      "Epoch [2341], train_loss: 0.0000, val_loss: 5.7678, val_acc: 0.5929\n",
      "Epoch [2342], train_loss: 0.0000, val_loss: 5.7985, val_acc: 0.5903\n",
      "Epoch [2343], train_loss: 0.0000, val_loss: 5.6794, val_acc: 0.5903\n",
      "Epoch [2344], train_loss: 0.0000, val_loss: 5.8755, val_acc: 0.5929\n",
      "Epoch [2345], train_loss: 0.0000, val_loss: 5.8875, val_acc: 0.5929\n",
      "Epoch [2346], train_loss: 0.0000, val_loss: 5.7301, val_acc: 0.5955\n",
      "Epoch [2347], train_loss: 0.0000, val_loss: 5.8426, val_acc: 0.5929\n",
      "Epoch [2348], train_loss: 0.0000, val_loss: 5.9752, val_acc: 0.5929\n",
      "Epoch [2349], train_loss: 0.0000, val_loss: 6.0172, val_acc: 0.5929\n",
      "Epoch [2350], train_loss: 0.0000, val_loss: 6.1007, val_acc: 0.5903\n",
      "Epoch [2351], train_loss: 0.0000, val_loss: 6.0570, val_acc: 0.5929\n",
      "Epoch [2352], train_loss: 0.0000, val_loss: 5.9790, val_acc: 0.5929\n",
      "Epoch [2353], train_loss: 0.0000, val_loss: 6.1604, val_acc: 0.5929\n",
      "Epoch [2354], train_loss: 0.0000, val_loss: 6.0129, val_acc: 0.5929\n",
      "Epoch [2355], train_loss: 0.0000, val_loss: 6.2380, val_acc: 0.5903\n",
      "Epoch [2356], train_loss: 0.0000, val_loss: 6.2487, val_acc: 0.5929\n",
      "Epoch [2357], train_loss: 0.0000, val_loss: 6.2259, val_acc: 0.5929\n",
      "Epoch [2358], train_loss: 0.0000, val_loss: 6.1285, val_acc: 0.5929\n",
      "Epoch [2359], train_loss: 0.0000, val_loss: 6.1566, val_acc: 0.5929\n",
      "Epoch [2360], train_loss: 0.0082, val_loss: 5.0728, val_acc: 0.5955\n",
      "Epoch [2361], train_loss: 0.0220, val_loss: 3.1994, val_acc: 0.5851\n",
      "Epoch [2362], train_loss: 0.0117, val_loss: 3.6732, val_acc: 0.5877\n",
      "Epoch [2363], train_loss: 0.0010, val_loss: 4.4030, val_acc: 0.5770\n",
      "Epoch [2364], train_loss: 0.0002, val_loss: 4.7238, val_acc: 0.5718\n",
      "Epoch [2365], train_loss: 0.0001, val_loss: 4.7276, val_acc: 0.5692\n",
      "Epoch [2366], train_loss: 0.0001, val_loss: 4.9695, val_acc: 0.5666\n",
      "Epoch [2367], train_loss: 0.0001, val_loss: 4.8193, val_acc: 0.5744\n",
      "Epoch [2368], train_loss: 0.0001, val_loss: 5.0282, val_acc: 0.5744\n",
      "Epoch [2369], train_loss: 0.0000, val_loss: 5.2456, val_acc: 0.5796\n",
      "Epoch [2370], train_loss: 0.0000, val_loss: 5.1591, val_acc: 0.5796\n",
      "Epoch [2371], train_loss: 0.0000, val_loss: 5.1227, val_acc: 0.5822\n",
      "Epoch [2372], train_loss: 0.0000, val_loss: 5.2144, val_acc: 0.5796\n",
      "Epoch [2373], train_loss: 0.0000, val_loss: 5.3080, val_acc: 0.5848\n",
      "Epoch [2374], train_loss: 0.0000, val_loss: 5.2135, val_acc: 0.5796\n",
      "Epoch [2375], train_loss: 0.0000, val_loss: 5.3396, val_acc: 0.5796\n",
      "Epoch [2376], train_loss: 0.0000, val_loss: 5.4437, val_acc: 0.5796\n",
      "Epoch [2377], train_loss: 0.0000, val_loss: 5.6064, val_acc: 0.5822\n",
      "Epoch [2378], train_loss: 0.0000, val_loss: 5.3814, val_acc: 0.5848\n",
      "Epoch [2379], train_loss: 0.0000, val_loss: 5.4355, val_acc: 0.5848\n",
      "Epoch [2380], train_loss: 0.0000, val_loss: 5.6442, val_acc: 0.5848\n",
      "Epoch [2381], train_loss: 0.0000, val_loss: 5.6367, val_acc: 0.5822\n",
      "Epoch [2382], train_loss: 0.0000, val_loss: 5.5686, val_acc: 0.5848\n",
      "Epoch [2383], train_loss: 0.0000, val_loss: 5.4976, val_acc: 0.5874\n",
      "Epoch [2384], train_loss: 0.0000, val_loss: 5.8248, val_acc: 0.5874\n",
      "Epoch [2385], train_loss: 0.0000, val_loss: 5.4931, val_acc: 0.5848\n",
      "Epoch [2386], train_loss: 0.0000, val_loss: 5.6723, val_acc: 0.5848\n",
      "Epoch [2387], train_loss: 0.0000, val_loss: 5.9571, val_acc: 0.5796\n",
      "Epoch [2388], train_loss: 0.0000, val_loss: 5.8182, val_acc: 0.5848\n",
      "Epoch [2389], train_loss: 0.0000, val_loss: 5.7329, val_acc: 0.5796\n",
      "Epoch [2390], train_loss: 0.0000, val_loss: 5.9329, val_acc: 0.5796\n",
      "Epoch [2391], train_loss: 0.0000, val_loss: 6.1515, val_acc: 0.5822\n",
      "Epoch [2392], train_loss: 0.0000, val_loss: 6.0664, val_acc: 0.5770\n",
      "Epoch [2393], train_loss: 0.0000, val_loss: 5.7690, val_acc: 0.5796\n",
      "Epoch [2394], train_loss: 0.0000, val_loss: 6.1266, val_acc: 0.5796\n",
      "Epoch [2395], train_loss: 0.0000, val_loss: 6.0844, val_acc: 0.5848\n",
      "Epoch [2396], train_loss: 0.0001, val_loss: 6.0904, val_acc: 0.5874\n",
      "Epoch [2397], train_loss: 0.0001, val_loss: 6.0793, val_acc: 0.5796\n",
      "Epoch [2398], train_loss: 0.0002, val_loss: 6.0460, val_acc: 0.5955\n",
      "Epoch [2399], train_loss: 0.0000, val_loss: 6.5809, val_acc: 0.5822\n",
      "Epoch [2400], train_loss: 0.0000, val_loss: 6.3642, val_acc: 0.5796\n",
      "Epoch [2401], train_loss: 0.0000, val_loss: 6.5168, val_acc: 0.5822\n",
      "Epoch [2402], train_loss: 0.0000, val_loss: 6.5666, val_acc: 0.5848\n",
      "Epoch [2403], train_loss: 0.0000, val_loss: 6.7229, val_acc: 0.5848\n",
      "Epoch [2404], train_loss: 0.0000, val_loss: 6.5982, val_acc: 0.5848\n",
      "Epoch [2405], train_loss: 0.0000, val_loss: 6.5298, val_acc: 0.5822\n",
      "Epoch [2406], train_loss: 0.0000, val_loss: 6.7257, val_acc: 0.5848\n",
      "Epoch [2407], train_loss: 0.0000, val_loss: 6.5039, val_acc: 0.5848\n",
      "Epoch [2408], train_loss: 0.0000, val_loss: 6.7400, val_acc: 0.5848\n",
      "Epoch [2409], train_loss: 0.0000, val_loss: 6.3255, val_acc: 0.5796\n",
      "Epoch [2410], train_loss: 0.0000, val_loss: 6.5538, val_acc: 0.5874\n",
      "Epoch [2411], train_loss: 0.0001, val_loss: 6.4804, val_acc: 0.5796\n",
      "Epoch [2412], train_loss: 0.0000, val_loss: 6.7989, val_acc: 0.5874\n",
      "Epoch [2413], train_loss: 0.0000, val_loss: 7.0824, val_acc: 0.5874\n",
      "Epoch [2414], train_loss: 0.0000, val_loss: 6.8389, val_acc: 0.5874\n",
      "Epoch [2415], train_loss: 0.0000, val_loss: 6.9092, val_acc: 0.5874\n",
      "Epoch [2416], train_loss: 0.0000, val_loss: 7.1879, val_acc: 0.5874\n",
      "Epoch [2417], train_loss: 0.0000, val_loss: 7.0312, val_acc: 0.5874\n",
      "Epoch [2418], train_loss: 0.0000, val_loss: 6.8137, val_acc: 0.5848\n",
      "Epoch [2419], train_loss: 0.0000, val_loss: 6.8347, val_acc: 0.5874\n",
      "Epoch [2420], train_loss: 0.0000, val_loss: 6.9760, val_acc: 0.5848\n",
      "Epoch [2421], train_loss: 0.0000, val_loss: 7.0950, val_acc: 0.5848\n",
      "Epoch [2422], train_loss: 0.0000, val_loss: 7.3300, val_acc: 0.5874\n",
      "Epoch [2423], train_loss: 0.0000, val_loss: 7.2584, val_acc: 0.5848\n",
      "Epoch [2424], train_loss: 0.0000, val_loss: 7.0860, val_acc: 0.5848\n",
      "Epoch [2425], train_loss: 0.0000, val_loss: 7.1085, val_acc: 0.5822\n",
      "Epoch [2426], train_loss: 0.0000, val_loss: 6.9978, val_acc: 0.5848\n",
      "Epoch [2427], train_loss: 0.0000, val_loss: 6.9262, val_acc: 0.5900\n",
      "Epoch [2428], train_loss: 0.0000, val_loss: 7.1864, val_acc: 0.5874\n",
      "Epoch [2429], train_loss: 0.0000, val_loss: 7.2622, val_acc: 0.5874\n",
      "Epoch [2430], train_loss: 0.0000, val_loss: 7.2781, val_acc: 0.5874\n",
      "Epoch [2431], train_loss: 0.0000, val_loss: 7.0660, val_acc: 0.5822\n",
      "Epoch [2432], train_loss: 0.0000, val_loss: 7.1728, val_acc: 0.5848\n",
      "Epoch [2433], train_loss: 0.0000, val_loss: 7.0961, val_acc: 0.5900\n",
      "Epoch [2434], train_loss: 0.0000, val_loss: 6.8411, val_acc: 0.5848\n",
      "Epoch [2435], train_loss: 0.0000, val_loss: 7.3156, val_acc: 0.5874\n",
      "Epoch [2436], train_loss: 0.0000, val_loss: 7.2706, val_acc: 0.5900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2437], train_loss: 0.0000, val_loss: 7.1283, val_acc: 0.5874\n",
      "Epoch [2438], train_loss: 0.0000, val_loss: 7.2915, val_acc: 0.5874\n",
      "Epoch [2439], train_loss: 0.0000, val_loss: 7.2797, val_acc: 0.5848\n",
      "Epoch [2440], train_loss: 0.0000, val_loss: 7.2996, val_acc: 0.5848\n",
      "Epoch [2441], train_loss: 0.0000, val_loss: 7.2747, val_acc: 0.5848\n",
      "Epoch [2442], train_loss: 0.0000, val_loss: 7.3487, val_acc: 0.5874\n",
      "Epoch [2443], train_loss: 0.0000, val_loss: 7.1787, val_acc: 0.5848\n",
      "Epoch [2444], train_loss: 0.0000, val_loss: 7.3363, val_acc: 0.5874\n",
      "Epoch [2445], train_loss: 0.0000, val_loss: 7.0335, val_acc: 0.5900\n",
      "Epoch [2446], train_loss: 0.0000, val_loss: 7.2272, val_acc: 0.5874\n",
      "Epoch [2447], train_loss: 0.0000, val_loss: 7.2998, val_acc: 0.5822\n",
      "Epoch [2448], train_loss: 0.0000, val_loss: 7.1150, val_acc: 0.5874\n",
      "Epoch [2449], train_loss: 0.0000, val_loss: 7.0953, val_acc: 0.5796\n",
      "Epoch [2450], train_loss: 0.0000, val_loss: 7.3120, val_acc: 0.5874\n",
      "Epoch [2451], train_loss: 0.0000, val_loss: 7.3790, val_acc: 0.5874\n",
      "Epoch [2452], train_loss: 0.0000, val_loss: 7.0737, val_acc: 0.5848\n",
      "Epoch [2453], train_loss: 0.0000, val_loss: 7.5051, val_acc: 0.5848\n",
      "Epoch [2454], train_loss: 0.0000, val_loss: 7.3329, val_acc: 0.5874\n",
      "Epoch [2455], train_loss: 0.0000, val_loss: 7.3304, val_acc: 0.5900\n",
      "Epoch [2456], train_loss: 0.0000, val_loss: 7.3075, val_acc: 0.5848\n",
      "Epoch [2457], train_loss: 0.0000, val_loss: 7.2780, val_acc: 0.5822\n",
      "Epoch [2458], train_loss: 0.0000, val_loss: 7.4816, val_acc: 0.5874\n",
      "Epoch [2459], train_loss: 0.0000, val_loss: 7.3420, val_acc: 0.5848\n",
      "Epoch [2460], train_loss: 0.0000, val_loss: 7.2532, val_acc: 0.5848\n",
      "Epoch [2461], train_loss: 0.0000, val_loss: 7.1060, val_acc: 0.5900\n",
      "Epoch [2462], train_loss: 0.0000, val_loss: 7.3006, val_acc: 0.5900\n",
      "Epoch [2463], train_loss: 0.0000, val_loss: 7.3941, val_acc: 0.5848\n",
      "Epoch [2464], train_loss: 0.0000, val_loss: 7.7234, val_acc: 0.5822\n",
      "Epoch [2465], train_loss: 0.0000, val_loss: 7.2622, val_acc: 0.5848\n",
      "Epoch [2466], train_loss: 0.0000, val_loss: 7.3223, val_acc: 0.5900\n",
      "Epoch [2467], train_loss: 0.0000, val_loss: 7.3974, val_acc: 0.5822\n",
      "Epoch [2468], train_loss: 0.0000, val_loss: 7.2217, val_acc: 0.5848\n",
      "Epoch [2469], train_loss: 0.0000, val_loss: 7.5721, val_acc: 0.5822\n",
      "Epoch [2470], train_loss: 0.0000, val_loss: 7.7363, val_acc: 0.5848\n",
      "Epoch [2471], train_loss: 0.0000, val_loss: 7.6266, val_acc: 0.5848\n",
      "Epoch [2472], train_loss: 0.0000, val_loss: 7.4063, val_acc: 0.5900\n",
      "Epoch [2473], train_loss: 0.0000, val_loss: 7.5757, val_acc: 0.5926\n",
      "Epoch [2474], train_loss: 0.0000, val_loss: 7.6229, val_acc: 0.5848\n",
      "Epoch [2475], train_loss: 0.0000, val_loss: 7.6379, val_acc: 0.5822\n",
      "Epoch [2476], train_loss: 0.0000, val_loss: 7.3892, val_acc: 0.5926\n",
      "Epoch [2477], train_loss: 0.0000, val_loss: 7.7849, val_acc: 0.5848\n",
      "Epoch [2478], train_loss: 0.0000, val_loss: 7.7449, val_acc: 0.5874\n",
      "Epoch [2479], train_loss: 0.0000, val_loss: 7.4620, val_acc: 0.5874\n",
      "Epoch [2480], train_loss: 0.0000, val_loss: 7.5405, val_acc: 0.5848\n",
      "Epoch [2481], train_loss: 0.0000, val_loss: 7.5774, val_acc: 0.5822\n",
      "Epoch [2482], train_loss: 0.0000, val_loss: 7.4133, val_acc: 0.5848\n",
      "Epoch [2483], train_loss: 0.0000, val_loss: 7.6212, val_acc: 0.5822\n",
      "Epoch [2484], train_loss: 0.0000, val_loss: 7.6906, val_acc: 0.5874\n",
      "Epoch [2485], train_loss: 0.0000, val_loss: 7.6485, val_acc: 0.5900\n",
      "Epoch [2486], train_loss: 0.0000, val_loss: 7.6335, val_acc: 0.5874\n",
      "Epoch [2487], train_loss: 0.0000, val_loss: 7.6213, val_acc: 0.5822\n",
      "Epoch [2488], train_loss: 0.0000, val_loss: 7.4413, val_acc: 0.5874\n",
      "Epoch [2489], train_loss: 0.0000, val_loss: 7.4537, val_acc: 0.5874\n",
      "Epoch [2490], train_loss: 0.0000, val_loss: 7.7700, val_acc: 0.5874\n",
      "Epoch [2491], train_loss: 0.0000, val_loss: 7.7423, val_acc: 0.5900\n",
      "Epoch [2492], train_loss: 0.0000, val_loss: 7.9188, val_acc: 0.5822\n",
      "Epoch [2493], train_loss: 0.0000, val_loss: 7.7600, val_acc: 0.5822\n",
      "Epoch [2494], train_loss: 0.0000, val_loss: 7.8139, val_acc: 0.5822\n",
      "Epoch [2495], train_loss: 0.0000, val_loss: 7.8771, val_acc: 0.5822\n",
      "Epoch [2496], train_loss: 0.0000, val_loss: 7.9339, val_acc: 0.5822\n",
      "Epoch [2497], train_loss: 0.0000, val_loss: 7.6840, val_acc: 0.5874\n",
      "Epoch [2498], train_loss: 0.0000, val_loss: 7.8959, val_acc: 0.5822\n",
      "Epoch [2499], train_loss: 0.0000, val_loss: 7.8402, val_acc: 0.5822\n",
      "Epoch [2500], train_loss: 0.0000, val_loss: 7.7853, val_acc: 0.5822\n",
      "Epoch [2501], train_loss: 0.0000, val_loss: 7.9932, val_acc: 0.5874\n",
      "Epoch [2502], train_loss: 0.0000, val_loss: 7.9452, val_acc: 0.5848\n",
      "Epoch [2503], train_loss: 0.0000, val_loss: 7.8290, val_acc: 0.5900\n",
      "Epoch [2504], train_loss: 0.0000, val_loss: 8.0578, val_acc: 0.5874\n",
      "Epoch [2505], train_loss: 0.0000, val_loss: 7.8131, val_acc: 0.5900\n",
      "Epoch [2506], train_loss: 0.0003, val_loss: 7.8097, val_acc: 0.5822\n",
      "Epoch [2507], train_loss: 0.0720, val_loss: 3.4221, val_acc: 0.5460\n",
      "Epoch [2508], train_loss: 0.0463, val_loss: 2.0206, val_acc: 0.5890\n",
      "Epoch [2509], train_loss: 0.0160, val_loss: 3.6655, val_acc: 0.6014\n",
      "Epoch [2510], train_loss: 0.0071, val_loss: 3.4699, val_acc: 0.6011\n",
      "Epoch [2511], train_loss: 0.0096, val_loss: 3.9855, val_acc: 0.6014\n",
      "Epoch [2512], train_loss: 0.0016, val_loss: 4.6521, val_acc: 0.6176\n",
      "Epoch [2513], train_loss: 0.0005, val_loss: 4.9166, val_acc: 0.6095\n",
      "Epoch [2514], train_loss: 0.0002, val_loss: 5.0980, val_acc: 0.5988\n",
      "Epoch [2515], train_loss: 0.0001, val_loss: 5.0495, val_acc: 0.6014\n",
      "Epoch [2516], train_loss: 0.0001, val_loss: 5.2608, val_acc: 0.6066\n",
      "Epoch [2517], train_loss: 0.0001, val_loss: 5.3792, val_acc: 0.5988\n",
      "Epoch [2518], train_loss: 0.0001, val_loss: 5.3224, val_acc: 0.6040\n",
      "Epoch [2519], train_loss: 0.0000, val_loss: 5.4436, val_acc: 0.6040\n",
      "Epoch [2520], train_loss: 0.0000, val_loss: 5.4478, val_acc: 0.6040\n",
      "Epoch [2521], train_loss: 0.0000, val_loss: 5.5377, val_acc: 0.6040\n",
      "Epoch [2522], train_loss: 0.0000, val_loss: 5.4609, val_acc: 0.6066\n",
      "Epoch [2523], train_loss: 0.0000, val_loss: 5.6307, val_acc: 0.6040\n",
      "Epoch [2524], train_loss: 0.0000, val_loss: 5.6297, val_acc: 0.6092\n",
      "Epoch [2525], train_loss: 0.0000, val_loss: 5.6186, val_acc: 0.6066\n",
      "Epoch [2526], train_loss: 0.0000, val_loss: 5.6961, val_acc: 0.6040\n",
      "Epoch [2527], train_loss: 0.0000, val_loss: 5.7537, val_acc: 0.6066\n",
      "Epoch [2528], train_loss: 0.0001, val_loss: 5.8613, val_acc: 0.6066\n",
      "Epoch [2529], train_loss: 0.0000, val_loss: 6.1624, val_acc: 0.6040\n",
      "Epoch [2530], train_loss: 0.0000, val_loss: 6.0939, val_acc: 0.6040\n",
      "Epoch [2531], train_loss: 0.0000, val_loss: 6.1362, val_acc: 0.6040\n",
      "Epoch [2532], train_loss: 0.0000, val_loss: 6.0573, val_acc: 0.6040\n",
      "Epoch [2533], train_loss: 0.0000, val_loss: 6.0160, val_acc: 0.6040\n",
      "Epoch [2534], train_loss: 0.0000, val_loss: 6.1144, val_acc: 0.6040\n",
      "Epoch [2535], train_loss: 0.0000, val_loss: 6.1282, val_acc: 0.6040\n",
      "Epoch [2536], train_loss: 0.0000, val_loss: 6.2391, val_acc: 0.6014\n",
      "Epoch [2537], train_loss: 0.0000, val_loss: 6.0986, val_acc: 0.5988\n",
      "Epoch [2538], train_loss: 0.0000, val_loss: 6.1419, val_acc: 0.6066\n",
      "Epoch [2539], train_loss: 0.0000, val_loss: 6.2772, val_acc: 0.6040\n",
      "Epoch [2540], train_loss: 0.0000, val_loss: 6.2337, val_acc: 0.6040\n",
      "Epoch [2541], train_loss: 0.0000, val_loss: 6.2429, val_acc: 0.6040\n",
      "Epoch [2542], train_loss: 0.0000, val_loss: 6.3290, val_acc: 0.6014\n",
      "Epoch [2543], train_loss: 0.0000, val_loss: 6.1922, val_acc: 0.6014\n",
      "Epoch [2544], train_loss: 0.0043, val_loss: 5.8603, val_acc: 0.5988\n",
      "Epoch [2545], train_loss: 0.0489, val_loss: 2.9396, val_acc: 0.5985\n",
      "Epoch [2546], train_loss: 0.0079, val_loss: 5.3498, val_acc: 0.5776\n",
      "Epoch [2547], train_loss: 0.0055, val_loss: 4.7443, val_acc: 0.5910\n",
      "Epoch [2548], train_loss: 0.0100, val_loss: 5.1869, val_acc: 0.5857\n",
      "Epoch [2549], train_loss: 0.0144, val_loss: 3.1998, val_acc: 0.5965\n",
      "Epoch [2550], train_loss: 0.0031, val_loss: 4.2671, val_acc: 0.5854\n",
      "Epoch [2551], train_loss: 0.0007, val_loss: 4.4199, val_acc: 0.6020\n",
      "Epoch [2552], train_loss: 0.0002, val_loss: 4.5605, val_acc: 0.5965\n",
      "Epoch [2553], train_loss: 0.0003, val_loss: 4.9066, val_acc: 0.5884\n",
      "Epoch [2554], train_loss: 0.0001, val_loss: 5.0390, val_acc: 0.5991\n",
      "Epoch [2555], train_loss: 0.0001, val_loss: 5.2325, val_acc: 0.5994\n",
      "Epoch [2556], train_loss: 0.0000, val_loss: 5.3542, val_acc: 0.5994\n",
      "Epoch [2557], train_loss: 0.0000, val_loss: 5.2988, val_acc: 0.6072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2558], train_loss: 0.0001, val_loss: 5.3720, val_acc: 0.5776\n",
      "Epoch [2559], train_loss: 0.0000, val_loss: 5.5956, val_acc: 0.5884\n",
      "Epoch [2560], train_loss: 0.0000, val_loss: 5.6967, val_acc: 0.5939\n",
      "Epoch [2561], train_loss: 0.0000, val_loss: 5.4789, val_acc: 0.5884\n",
      "Epoch [2562], train_loss: 0.0000, val_loss: 5.6986, val_acc: 0.5991\n",
      "Epoch [2563], train_loss: 0.0000, val_loss: 5.6627, val_acc: 0.5884\n",
      "Epoch [2564], train_loss: 0.0000, val_loss: 5.7666, val_acc: 0.5884\n",
      "Epoch [2565], train_loss: 0.0000, val_loss: 5.6855, val_acc: 0.5939\n",
      "Epoch [2566], train_loss: 0.0000, val_loss: 5.9326, val_acc: 0.5991\n",
      "Epoch [2567], train_loss: 0.0000, val_loss: 5.8947, val_acc: 0.5939\n",
      "Epoch [2568], train_loss: 0.0000, val_loss: 5.9925, val_acc: 0.6017\n",
      "Epoch [2569], train_loss: 0.0000, val_loss: 6.0786, val_acc: 0.5939\n",
      "Epoch [2570], train_loss: 0.0001, val_loss: 6.0253, val_acc: 0.5939\n",
      "Epoch [2571], train_loss: 0.0000, val_loss: 6.4269, val_acc: 0.6017\n",
      "Epoch [2572], train_loss: 0.0000, val_loss: 6.4665, val_acc: 0.5913\n",
      "Epoch [2573], train_loss: 0.0000, val_loss: 6.6315, val_acc: 0.5965\n",
      "Epoch [2574], train_loss: 0.0000, val_loss: 6.5036, val_acc: 0.5887\n",
      "Epoch [2575], train_loss: 0.0000, val_loss: 6.4743, val_acc: 0.5939\n",
      "Epoch [2576], train_loss: 0.0000, val_loss: 6.3321, val_acc: 0.6017\n",
      "Epoch [2577], train_loss: 0.0000, val_loss: 6.2211, val_acc: 0.6017\n",
      "Epoch [2578], train_loss: 0.0000, val_loss: 6.6380, val_acc: 0.5939\n",
      "Epoch [2579], train_loss: 0.0002, val_loss: 6.7447, val_acc: 0.5965\n",
      "Epoch [2580], train_loss: 0.0001, val_loss: 7.1671, val_acc: 0.5857\n",
      "Epoch [2581], train_loss: 0.0007, val_loss: 6.9570, val_acc: 0.5880\n",
      "Epoch [2582], train_loss: 0.0219, val_loss: 3.3815, val_acc: 0.5487\n",
      "Epoch [2583], train_loss: 0.0045, val_loss: 5.3951, val_acc: 0.5636\n",
      "Epoch [2584], train_loss: 0.0008, val_loss: 5.9596, val_acc: 0.5695\n",
      "Epoch [2585], train_loss: 0.0001, val_loss: 5.9355, val_acc: 0.5617\n",
      "Epoch [2586], train_loss: 0.0000, val_loss: 5.9800, val_acc: 0.5588\n",
      "Epoch [2587], train_loss: 0.0000, val_loss: 6.0755, val_acc: 0.5561\n",
      "Epoch [2588], train_loss: 0.0000, val_loss: 6.0338, val_acc: 0.5535\n",
      "Epoch [2589], train_loss: 0.0000, val_loss: 6.2353, val_acc: 0.5561\n",
      "Epoch [2590], train_loss: 0.0000, val_loss: 6.1375, val_acc: 0.5535\n",
      "Epoch [2591], train_loss: 0.0000, val_loss: 6.1793, val_acc: 0.5561\n",
      "Epoch [2592], train_loss: 0.0000, val_loss: 6.1783, val_acc: 0.5588\n",
      "Epoch [2593], train_loss: 0.0000, val_loss: 6.3097, val_acc: 0.5509\n",
      "Epoch [2594], train_loss: 0.0000, val_loss: 6.2353, val_acc: 0.5588\n",
      "Epoch [2595], train_loss: 0.0000, val_loss: 6.3901, val_acc: 0.5588\n",
      "Epoch [2596], train_loss: 0.0000, val_loss: 6.4853, val_acc: 0.5561\n",
      "Epoch [2597], train_loss: 0.0000, val_loss: 6.5730, val_acc: 0.5535\n",
      "Epoch [2598], train_loss: 0.0000, val_loss: 6.4885, val_acc: 0.5614\n",
      "Epoch [2599], train_loss: 0.0000, val_loss: 6.6840, val_acc: 0.5509\n",
      "Epoch [2600], train_loss: 0.0000, val_loss: 6.6157, val_acc: 0.5588\n",
      "Epoch [2601], train_loss: 0.0000, val_loss: 6.6922, val_acc: 0.5483\n",
      "Epoch [2602], train_loss: 0.0000, val_loss: 6.6092, val_acc: 0.5614\n",
      "Epoch [2603], train_loss: 0.0000, val_loss: 6.5324, val_acc: 0.5640\n",
      "Epoch [2604], train_loss: 0.0000, val_loss: 6.7118, val_acc: 0.5640\n",
      "Epoch [2605], train_loss: 0.0000, val_loss: 6.7738, val_acc: 0.5561\n",
      "Epoch [2606], train_loss: 0.0000, val_loss: 6.7814, val_acc: 0.5483\n",
      "Epoch [2607], train_loss: 0.0000, val_loss: 6.8144, val_acc: 0.5509\n",
      "Epoch [2608], train_loss: 0.0000, val_loss: 6.8138, val_acc: 0.5535\n",
      "Epoch [2609], train_loss: 0.0000, val_loss: 6.8335, val_acc: 0.5509\n",
      "Epoch [2610], train_loss: 0.0000, val_loss: 7.0158, val_acc: 0.5506\n",
      "Epoch [2611], train_loss: 0.0000, val_loss: 7.1100, val_acc: 0.5506\n",
      "Epoch [2612], train_loss: 0.0000, val_loss: 7.0334, val_acc: 0.5480\n",
      "Epoch [2613], train_loss: 0.0000, val_loss: 7.0337, val_acc: 0.5480\n",
      "Epoch [2614], train_loss: 0.0000, val_loss: 7.1196, val_acc: 0.5454\n",
      "Epoch [2615], train_loss: 0.0000, val_loss: 6.9068, val_acc: 0.5480\n",
      "Epoch [2616], train_loss: 0.0000, val_loss: 7.0917, val_acc: 0.5480\n",
      "Epoch [2617], train_loss: 0.0000, val_loss: 7.1739, val_acc: 0.5454\n",
      "Epoch [2618], train_loss: 0.0000, val_loss: 6.9329, val_acc: 0.5535\n",
      "Epoch [2619], train_loss: 0.0000, val_loss: 7.1362, val_acc: 0.5509\n",
      "Epoch [2620], train_loss: 0.0000, val_loss: 7.1421, val_acc: 0.5428\n",
      "Epoch [2621], train_loss: 0.0000, val_loss: 7.1488, val_acc: 0.5454\n",
      "Epoch [2622], train_loss: 0.0000, val_loss: 7.4045, val_acc: 0.5509\n",
      "Epoch [2623], train_loss: 0.0000, val_loss: 7.1643, val_acc: 0.5532\n",
      "Epoch [2624], train_loss: 0.0000, val_loss: 7.2334, val_acc: 0.5532\n",
      "Epoch [2625], train_loss: 0.0000, val_loss: 7.1915, val_acc: 0.5532\n",
      "Epoch [2626], train_loss: 0.0000, val_loss: 7.0793, val_acc: 0.5640\n",
      "Epoch [2627], train_loss: 0.0000, val_loss: 7.2647, val_acc: 0.5480\n",
      "Epoch [2628], train_loss: 0.0000, val_loss: 7.2500, val_acc: 0.5454\n",
      "Epoch [2629], train_loss: 0.0000, val_loss: 7.2143, val_acc: 0.5558\n",
      "Epoch [2630], train_loss: 0.0000, val_loss: 7.4746, val_acc: 0.5454\n",
      "Epoch [2631], train_loss: 0.0000, val_loss: 7.3067, val_acc: 0.5506\n",
      "Epoch [2632], train_loss: 0.0000, val_loss: 7.3730, val_acc: 0.5558\n",
      "Epoch [2633], train_loss: 0.0000, val_loss: 7.4467, val_acc: 0.5428\n",
      "Epoch [2634], train_loss: 0.0000, val_loss: 7.5455, val_acc: 0.5454\n",
      "Epoch [2635], train_loss: 0.0000, val_loss: 7.4507, val_acc: 0.5509\n",
      "Epoch [2636], train_loss: 0.0000, val_loss: 7.4972, val_acc: 0.5428\n",
      "Epoch [2637], train_loss: 0.0000, val_loss: 7.3528, val_acc: 0.5640\n",
      "Epoch [2638], train_loss: 0.0000, val_loss: 7.5669, val_acc: 0.5454\n",
      "Epoch [2639], train_loss: 0.0000, val_loss: 7.6304, val_acc: 0.5483\n",
      "Epoch [2640], train_loss: 0.0000, val_loss: 7.5408, val_acc: 0.5428\n",
      "Epoch [2641], train_loss: 0.0000, val_loss: 7.6916, val_acc: 0.5506\n",
      "Epoch [2642], train_loss: 0.0000, val_loss: 7.4351, val_acc: 0.5454\n",
      "Epoch [2643], train_loss: 0.0000, val_loss: 7.6727, val_acc: 0.5428\n",
      "Epoch [2644], train_loss: 0.0000, val_loss: 7.7821, val_acc: 0.5454\n",
      "Epoch [2645], train_loss: 0.0000, val_loss: 7.4413, val_acc: 0.5480\n",
      "Epoch [2646], train_loss: 0.0000, val_loss: 7.6192, val_acc: 0.5454\n",
      "Epoch [2647], train_loss: 0.0000, val_loss: 7.6912, val_acc: 0.5428\n",
      "Epoch [2648], train_loss: 0.0000, val_loss: 7.8040, val_acc: 0.5428\n",
      "Epoch [2649], train_loss: 0.0000, val_loss: 7.8943, val_acc: 0.5428\n",
      "Epoch [2650], train_loss: 0.0000, val_loss: 7.9082, val_acc: 0.5532\n",
      "Epoch [2651], train_loss: 0.0000, val_loss: 7.6218, val_acc: 0.5428\n",
      "Epoch [2652], train_loss: 0.0000, val_loss: 7.7077, val_acc: 0.5480\n",
      "Epoch [2653], train_loss: 0.0000, val_loss: 7.7203, val_acc: 0.5428\n",
      "Epoch [2654], train_loss: 0.0000, val_loss: 7.8740, val_acc: 0.5428\n",
      "Epoch [2655], train_loss: 0.0000, val_loss: 7.8659, val_acc: 0.5480\n",
      "Epoch [2656], train_loss: 0.0000, val_loss: 7.9344, val_acc: 0.5428\n",
      "Epoch [2657], train_loss: 0.0000, val_loss: 7.7521, val_acc: 0.5480\n",
      "Epoch [2658], train_loss: 0.0001, val_loss: 8.0889, val_acc: 0.5402\n",
      "Epoch [2659], train_loss: 0.0002, val_loss: 8.7497, val_acc: 0.5588\n",
      "Epoch [2660], train_loss: 0.0150, val_loss: 5.3045, val_acc: 0.5376\n",
      "Epoch [2661], train_loss: 0.0587, val_loss: 2.8257, val_acc: 0.5730\n",
      "Epoch [2662], train_loss: 0.0142, val_loss: 2.9566, val_acc: 0.5701\n",
      "Epoch [2663], train_loss: 0.0027, val_loss: 4.0443, val_acc: 0.5542\n",
      "Epoch [2664], train_loss: 0.0013, val_loss: 4.6125, val_acc: 0.5831\n",
      "Epoch [2665], train_loss: 0.0006, val_loss: 4.8586, val_acc: 0.5724\n",
      "Epoch [2666], train_loss: 0.0001, val_loss: 5.0350, val_acc: 0.5750\n",
      "Epoch [2667], train_loss: 0.0000, val_loss: 5.0774, val_acc: 0.5698\n",
      "Epoch [2668], train_loss: 0.0000, val_loss: 5.1835, val_acc: 0.5698\n",
      "Epoch [2669], train_loss: 0.0000, val_loss: 5.2370, val_acc: 0.5698\n",
      "Epoch [2670], train_loss: 0.0000, val_loss: 5.2479, val_acc: 0.5750\n",
      "Epoch [2671], train_loss: 0.0000, val_loss: 5.3088, val_acc: 0.5724\n",
      "Epoch [2672], train_loss: 0.0000, val_loss: 5.4157, val_acc: 0.5672\n",
      "Epoch [2673], train_loss: 0.0000, val_loss: 5.4613, val_acc: 0.5698\n",
      "Epoch [2674], train_loss: 0.0000, val_loss: 5.4685, val_acc: 0.5724\n",
      "Epoch [2675], train_loss: 0.0000, val_loss: 5.5618, val_acc: 0.5724\n",
      "Epoch [2676], train_loss: 0.0000, val_loss: 5.3594, val_acc: 0.5750\n",
      "Epoch [2677], train_loss: 0.0000, val_loss: 5.5664, val_acc: 0.5750\n",
      "Epoch [2678], train_loss: 0.0000, val_loss: 5.5089, val_acc: 0.5698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2679], train_loss: 0.0000, val_loss: 5.5506, val_acc: 0.5646\n",
      "Epoch [2680], train_loss: 0.0000, val_loss: 5.5079, val_acc: 0.5698\n",
      "Epoch [2681], train_loss: 0.0000, val_loss: 5.6312, val_acc: 0.5776\n",
      "Epoch [2682], train_loss: 0.0000, val_loss: 5.7658, val_acc: 0.5698\n",
      "Epoch [2683], train_loss: 0.0000, val_loss: 5.7224, val_acc: 0.5750\n",
      "Epoch [2684], train_loss: 0.0000, val_loss: 5.7852, val_acc: 0.5724\n",
      "Epoch [2685], train_loss: 0.0000, val_loss: 5.6281, val_acc: 0.5724\n",
      "Epoch [2686], train_loss: 0.0000, val_loss: 5.6532, val_acc: 0.5698\n",
      "Epoch [2687], train_loss: 0.0000, val_loss: 5.6654, val_acc: 0.5750\n",
      "Epoch [2688], train_loss: 0.0000, val_loss: 5.8214, val_acc: 0.5776\n",
      "Epoch [2689], train_loss: 0.0000, val_loss: 5.8122, val_acc: 0.5750\n",
      "Epoch [2690], train_loss: 0.0000, val_loss: 5.8676, val_acc: 0.5776\n",
      "Epoch [2691], train_loss: 0.0000, val_loss: 5.8782, val_acc: 0.5750\n",
      "Epoch [2692], train_loss: 0.0000, val_loss: 5.7850, val_acc: 0.5802\n",
      "Epoch [2693], train_loss: 0.0000, val_loss: 5.8576, val_acc: 0.5750\n",
      "Epoch [2694], train_loss: 0.0000, val_loss: 5.9283, val_acc: 0.5802\n",
      "Epoch [2695], train_loss: 0.0000, val_loss: 5.9367, val_acc: 0.5776\n",
      "Epoch [2696], train_loss: 0.0000, val_loss: 5.9606, val_acc: 0.5750\n",
      "Epoch [2697], train_loss: 0.0000, val_loss: 6.0886, val_acc: 0.5724\n",
      "Epoch [2698], train_loss: 0.0000, val_loss: 6.0036, val_acc: 0.5776\n",
      "Epoch [2699], train_loss: 0.0000, val_loss: 6.0004, val_acc: 0.5750\n",
      "Epoch [2700], train_loss: 0.0000, val_loss: 5.9951, val_acc: 0.5724\n",
      "Epoch [2701], train_loss: 0.0000, val_loss: 6.0693, val_acc: 0.5750\n",
      "Epoch [2702], train_loss: 0.0000, val_loss: 6.1721, val_acc: 0.5750\n",
      "Epoch [2703], train_loss: 0.0000, val_loss: 6.0628, val_acc: 0.5724\n",
      "Epoch [2704], train_loss: 0.0000, val_loss: 5.9620, val_acc: 0.5857\n",
      "Epoch [2705], train_loss: 0.0000, val_loss: 6.1625, val_acc: 0.5724\n",
      "Epoch [2706], train_loss: 0.0000, val_loss: 6.1070, val_acc: 0.5698\n",
      "Epoch [2707], train_loss: 0.0000, val_loss: 6.1375, val_acc: 0.5750\n",
      "Epoch [2708], train_loss: 0.0002, val_loss: 6.4512, val_acc: 0.5776\n",
      "Epoch [2709], train_loss: 0.0000, val_loss: 6.3209, val_acc: 0.5750\n",
      "Epoch [2710], train_loss: 0.0000, val_loss: 6.2673, val_acc: 0.5724\n",
      "Epoch [2711], train_loss: 0.0000, val_loss: 6.3228, val_acc: 0.5724\n",
      "Epoch [2712], train_loss: 0.0000, val_loss: 6.3698, val_acc: 0.5724\n",
      "Epoch [2713], train_loss: 0.0000, val_loss: 6.4945, val_acc: 0.5672\n",
      "Epoch [2714], train_loss: 0.0000, val_loss: 6.3910, val_acc: 0.5698\n",
      "Epoch [2715], train_loss: 0.0000, val_loss: 6.4895, val_acc: 0.5750\n",
      "Epoch [2716], train_loss: 0.0000, val_loss: 6.4158, val_acc: 0.5698\n",
      "Epoch [2717], train_loss: 0.0000, val_loss: 6.5777, val_acc: 0.5750\n",
      "Epoch [2718], train_loss: 0.0000, val_loss: 6.5519, val_acc: 0.5750\n",
      "Epoch [2719], train_loss: 0.0000, val_loss: 6.4692, val_acc: 0.5724\n",
      "Epoch [2720], train_loss: 0.0000, val_loss: 6.4538, val_acc: 0.5698\n",
      "Epoch [2721], train_loss: 0.0000, val_loss: 6.4943, val_acc: 0.5698\n",
      "Epoch [2722], train_loss: 0.0000, val_loss: 6.5071, val_acc: 0.5805\n",
      "Epoch [2723], train_loss: 0.0000, val_loss: 6.6288, val_acc: 0.5724\n",
      "Epoch [2724], train_loss: 0.0000, val_loss: 6.6599, val_acc: 0.5750\n",
      "Epoch [2725], train_loss: 0.0000, val_loss: 6.4462, val_acc: 0.5776\n",
      "Epoch [2726], train_loss: 0.0000, val_loss: 6.6181, val_acc: 0.5724\n",
      "Epoch [2727], train_loss: 0.0000, val_loss: 6.7955, val_acc: 0.5724\n",
      "Epoch [2728], train_loss: 0.0000, val_loss: 6.6742, val_acc: 0.5724\n",
      "Epoch [2729], train_loss: 0.0000, val_loss: 6.6709, val_acc: 0.5698\n",
      "Epoch [2730], train_loss: 0.0000, val_loss: 6.7115, val_acc: 0.5672\n",
      "Epoch [2731], train_loss: 0.0000, val_loss: 6.5952, val_acc: 0.5750\n",
      "Epoch [2732], train_loss: 0.0000, val_loss: 6.7408, val_acc: 0.5724\n",
      "Epoch [2733], train_loss: 0.0000, val_loss: 6.5349, val_acc: 0.5698\n",
      "Epoch [2734], train_loss: 0.0000, val_loss: 6.6532, val_acc: 0.5936\n",
      "Epoch [2735], train_loss: 0.0000, val_loss: 6.6278, val_acc: 0.5724\n",
      "Epoch [2736], train_loss: 0.0000, val_loss: 6.7258, val_acc: 0.5724\n",
      "Epoch [2737], train_loss: 0.0000, val_loss: 6.7025, val_acc: 0.5724\n",
      "Epoch [2738], train_loss: 0.0000, val_loss: 6.6690, val_acc: 0.5724\n",
      "Epoch [2739], train_loss: 0.0000, val_loss: 6.7303, val_acc: 0.5857\n",
      "Epoch [2740], train_loss: 0.0000, val_loss: 6.7264, val_acc: 0.5750\n",
      "Epoch [2741], train_loss: 0.0000, val_loss: 6.9050, val_acc: 0.5724\n",
      "Epoch [2742], train_loss: 0.0000, val_loss: 6.8029, val_acc: 0.5779\n",
      "Epoch [2743], train_loss: 0.0000, val_loss: 6.8123, val_acc: 0.5672\n",
      "Epoch [2744], train_loss: 0.0000, val_loss: 6.8333, val_acc: 0.5724\n",
      "Epoch [2745], train_loss: 0.0000, val_loss: 6.9041, val_acc: 0.5724\n",
      "Epoch [2746], train_loss: 0.0000, val_loss: 6.8538, val_acc: 0.5724\n",
      "Epoch [2747], train_loss: 0.0000, val_loss: 6.7463, val_acc: 0.5831\n",
      "Epoch [2748], train_loss: 0.0000, val_loss: 6.9104, val_acc: 0.5857\n",
      "Epoch [2749], train_loss: 0.0000, val_loss: 6.7322, val_acc: 0.5750\n",
      "Epoch [2750], train_loss: 0.0000, val_loss: 6.8833, val_acc: 0.5776\n",
      "Epoch [2751], train_loss: 0.0000, val_loss: 6.9974, val_acc: 0.5724\n",
      "Epoch [2752], train_loss: 0.0000, val_loss: 6.8389, val_acc: 0.5724\n",
      "Epoch [2753], train_loss: 0.0000, val_loss: 6.9065, val_acc: 0.5750\n",
      "Epoch [2754], train_loss: 0.0000, val_loss: 6.9088, val_acc: 0.5750\n",
      "Epoch [2755], train_loss: 0.0000, val_loss: 7.0643, val_acc: 0.5910\n",
      "Epoch [2756], train_loss: 0.0000, val_loss: 6.8836, val_acc: 0.5884\n",
      "Epoch [2757], train_loss: 0.0000, val_loss: 6.8842, val_acc: 0.5936\n",
      "Epoch [2758], train_loss: 0.0000, val_loss: 6.9702, val_acc: 0.5857\n",
      "Epoch [2759], train_loss: 0.0000, val_loss: 6.9958, val_acc: 0.5857\n",
      "Epoch [2760], train_loss: 0.0000, val_loss: 7.2146, val_acc: 0.5831\n",
      "Epoch [2761], train_loss: 0.0000, val_loss: 7.3080, val_acc: 0.5857\n",
      "Epoch [2762], train_loss: 0.0000, val_loss: 7.4064, val_acc: 0.5831\n",
      "Epoch [2763], train_loss: 0.0000, val_loss: 7.3314, val_acc: 0.5805\n",
      "Epoch [2764], train_loss: 0.0000, val_loss: 7.4481, val_acc: 0.5884\n",
      "Epoch [2765], train_loss: 0.0000, val_loss: 7.4457, val_acc: 0.5857\n",
      "Epoch [2766], train_loss: 0.0000, val_loss: 7.4132, val_acc: 0.5857\n",
      "Epoch [2767], train_loss: 0.0000, val_loss: 7.4499, val_acc: 0.5805\n",
      "Epoch [2768], train_loss: 0.0000, val_loss: 7.4118, val_acc: 0.5857\n",
      "Epoch [2769], train_loss: 0.0000, val_loss: 7.4469, val_acc: 0.5831\n",
      "Epoch [2770], train_loss: 0.0000, val_loss: 7.3892, val_acc: 0.5831\n",
      "Epoch [2771], train_loss: 0.0000, val_loss: 7.4109, val_acc: 0.5805\n",
      "Epoch [2772], train_loss: 0.0000, val_loss: 7.3916, val_acc: 0.5831\n",
      "Epoch [2773], train_loss: 0.0000, val_loss: 7.4422, val_acc: 0.5831\n",
      "Epoch [2774], train_loss: 0.0000, val_loss: 7.4563, val_acc: 0.5805\n",
      "Epoch [2775], train_loss: 0.0000, val_loss: 7.5857, val_acc: 0.5831\n",
      "Epoch [2776], train_loss: 0.0000, val_loss: 7.5800, val_acc: 0.5831\n",
      "Epoch [2777], train_loss: 0.0000, val_loss: 7.6069, val_acc: 0.5857\n",
      "Epoch [2778], train_loss: 0.0000, val_loss: 7.5179, val_acc: 0.5831\n",
      "Epoch [2779], train_loss: 0.0000, val_loss: 7.6316, val_acc: 0.5857\n",
      "Epoch [2780], train_loss: 0.0000, val_loss: 7.7586, val_acc: 0.5857\n",
      "Epoch [2781], train_loss: 0.0000, val_loss: 7.7268, val_acc: 0.5857\n",
      "Epoch [2782], train_loss: 0.0000, val_loss: 7.6614, val_acc: 0.5884\n",
      "Epoch [2783], train_loss: 0.0000, val_loss: 7.5150, val_acc: 0.5884\n",
      "Epoch [2784], train_loss: 0.0000, val_loss: 7.8693, val_acc: 0.5857\n",
      "Epoch [2785], train_loss: 0.0000, val_loss: 7.8361, val_acc: 0.5936\n",
      "Epoch [2786], train_loss: 0.0000, val_loss: 7.4184, val_acc: 0.5643\n",
      "Epoch [2787], train_loss: 0.0000, val_loss: 8.2993, val_acc: 0.5594\n",
      "Epoch [2788], train_loss: 0.0000, val_loss: 8.4154, val_acc: 0.5568\n",
      "Epoch [2789], train_loss: 0.0000, val_loss: 8.4638, val_acc: 0.5646\n",
      "Epoch [2790], train_loss: 0.0000, val_loss: 8.6338, val_acc: 0.5620\n",
      "Epoch [2791], train_loss: 0.0000, val_loss: 8.4635, val_acc: 0.5620\n",
      "Epoch [2792], train_loss: 0.0000, val_loss: 8.5784, val_acc: 0.5620\n",
      "Epoch [2793], train_loss: 0.0000, val_loss: 8.6672, val_acc: 0.5568\n",
      "Epoch [2794], train_loss: 0.0000, val_loss: 8.5910, val_acc: 0.5620\n",
      "Epoch [2795], train_loss: 0.0000, val_loss: 8.4401, val_acc: 0.5620\n",
      "Epoch [2796], train_loss: 0.0000, val_loss: 8.9418, val_acc: 0.5672\n",
      "Epoch [2797], train_loss: 0.0000, val_loss: 8.7289, val_acc: 0.5646\n",
      "Epoch [2798], train_loss: 0.0000, val_loss: 8.6661, val_acc: 0.5568\n",
      "Epoch [2799], train_loss: 0.0000, val_loss: 8.7288, val_acc: 0.5568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2800], train_loss: 0.0000, val_loss: 8.6902, val_acc: 0.5568\n",
      "Epoch [2801], train_loss: 0.0000, val_loss: 8.8097, val_acc: 0.5620\n",
      "Epoch [2802], train_loss: 0.0000, val_loss: 8.7349, val_acc: 0.5646\n",
      "Epoch [2803], train_loss: 0.0000, val_loss: 8.7125, val_acc: 0.5646\n",
      "Epoch [2804], train_loss: 0.0000, val_loss: 8.8006, val_acc: 0.5672\n",
      "Epoch [2805], train_loss: 0.0000, val_loss: 8.9158, val_acc: 0.5594\n",
      "Epoch [2806], train_loss: 0.0000, val_loss: 8.9040, val_acc: 0.5672\n",
      "Epoch [2807], train_loss: 0.0000, val_loss: 8.7133, val_acc: 0.5620\n",
      "Epoch [2808], train_loss: 0.0000, val_loss: 8.6648, val_acc: 0.5646\n",
      "Epoch [2809], train_loss: 0.0000, val_loss: 8.6950, val_acc: 0.5542\n",
      "Epoch [2810], train_loss: 0.0000, val_loss: 8.6903, val_acc: 0.5646\n",
      "Epoch [2811], train_loss: 0.0000, val_loss: 8.7451, val_acc: 0.5620\n",
      "Epoch [2812], train_loss: 0.0045, val_loss: 8.5877, val_acc: 0.5643\n",
      "Epoch [2813], train_loss: 0.0319, val_loss: 3.3087, val_acc: 0.6394\n",
      "Epoch [2814], train_loss: 0.0187, val_loss: 4.5128, val_acc: 0.5776\n",
      "Epoch [2815], train_loss: 0.0049, val_loss: 4.7884, val_acc: 0.6339\n",
      "Epoch [2816], train_loss: 0.0052, val_loss: 6.7614, val_acc: 0.5828\n",
      "Epoch [2817], train_loss: 0.0208, val_loss: 3.9001, val_acc: 0.5880\n",
      "Epoch [2818], train_loss: 0.0046, val_loss: 6.1679, val_acc: 0.5884\n",
      "Epoch [2819], train_loss: 0.0038, val_loss: 5.9452, val_acc: 0.5939\n",
      "Epoch [2820], train_loss: 0.0138, val_loss: 3.2032, val_acc: 0.5802\n",
      "Epoch [2821], train_loss: 0.0037, val_loss: 4.4481, val_acc: 0.5861\n",
      "Epoch [2822], train_loss: 0.0008, val_loss: 5.2392, val_acc: 0.5965\n",
      "Epoch [2823], train_loss: 0.0002, val_loss: 5.1407, val_acc: 0.5884\n",
      "Epoch [2824], train_loss: 0.0001, val_loss: 5.0447, val_acc: 0.5857\n",
      "Epoch [2825], train_loss: 0.0001, val_loss: 5.3901, val_acc: 0.5805\n",
      "Epoch [2826], train_loss: 0.0021, val_loss: 5.6194, val_acc: 0.6043\n",
      "Epoch [2827], train_loss: 0.0160, val_loss: 4.2253, val_acc: 0.5965\n",
      "Epoch [2828], train_loss: 0.0034, val_loss: 5.6570, val_acc: 0.5857\n",
      "Epoch [2829], train_loss: 0.0002, val_loss: 6.0518, val_acc: 0.6072\n",
      "Epoch [2830], train_loss: 0.0001, val_loss: 6.1801, val_acc: 0.5965\n",
      "Epoch [2831], train_loss: 0.0001, val_loss: 6.2035, val_acc: 0.5936\n",
      "Epoch [2832], train_loss: 0.0001, val_loss: 6.4928, val_acc: 0.6069\n",
      "Epoch [2833], train_loss: 0.0000, val_loss: 6.6114, val_acc: 0.5988\n",
      "Epoch [2834], train_loss: 0.0000, val_loss: 6.7742, val_acc: 0.5936\n",
      "Epoch [2835], train_loss: 0.0000, val_loss: 6.7054, val_acc: 0.5936\n",
      "Epoch [2836], train_loss: 0.0000, val_loss: 6.6773, val_acc: 0.5988\n",
      "Epoch [2837], train_loss: 0.0000, val_loss: 6.8649, val_acc: 0.5962\n",
      "Epoch [2838], train_loss: 0.0000, val_loss: 7.0197, val_acc: 0.6043\n",
      "Epoch [2839], train_loss: 0.0000, val_loss: 7.1789, val_acc: 0.5936\n",
      "Epoch [2840], train_loss: 0.0000, val_loss: 6.7775, val_acc: 0.6043\n",
      "Epoch [2841], train_loss: 0.0000, val_loss: 7.1758, val_acc: 0.6069\n",
      "Epoch [2842], train_loss: 0.0000, val_loss: 7.0428, val_acc: 0.6043\n",
      "Epoch [2843], train_loss: 0.0000, val_loss: 7.1480, val_acc: 0.6014\n",
      "Epoch [2844], train_loss: 0.0000, val_loss: 7.2949, val_acc: 0.6014\n",
      "Epoch [2845], train_loss: 0.0000, val_loss: 7.3692, val_acc: 0.5962\n",
      "Epoch [2846], train_loss: 0.0000, val_loss: 7.3090, val_acc: 0.6014\n",
      "Epoch [2847], train_loss: 0.0000, val_loss: 7.4995, val_acc: 0.6069\n",
      "Epoch [2848], train_loss: 0.0003, val_loss: 6.4839, val_acc: 0.5884\n",
      "Epoch [2849], train_loss: 0.0051, val_loss: 6.3119, val_acc: 0.5991\n",
      "Epoch [2850], train_loss: 0.0005, val_loss: 5.7075, val_acc: 0.5910\n",
      "Epoch [2851], train_loss: 0.0001, val_loss: 6.0083, val_acc: 0.5880\n",
      "Epoch [2852], train_loss: 0.0000, val_loss: 6.1681, val_acc: 0.5906\n",
      "Epoch [2853], train_loss: 0.0006, val_loss: 6.1790, val_acc: 0.5906\n",
      "Epoch [2854], train_loss: 0.0008, val_loss: 6.5620, val_acc: 0.5880\n",
      "Epoch [2855], train_loss: 0.0045, val_loss: 5.9666, val_acc: 0.5906\n",
      "Epoch [2856], train_loss: 0.0020, val_loss: 5.9852, val_acc: 0.6173\n",
      "Epoch [2857], train_loss: 0.0027, val_loss: 6.1928, val_acc: 0.5936\n",
      "Epoch [2858], train_loss: 0.0004, val_loss: 5.9331, val_acc: 0.6011\n",
      "Epoch [2859], train_loss: 0.0002, val_loss: 5.9793, val_acc: 0.5932\n",
      "Epoch [2860], train_loss: 0.0000, val_loss: 6.1434, val_acc: 0.6092\n",
      "Epoch [2861], train_loss: 0.0000, val_loss: 6.3083, val_acc: 0.5932\n",
      "Epoch [2862], train_loss: 0.0052, val_loss: 6.2967, val_acc: 0.5828\n",
      "Epoch [2863], train_loss: 0.0233, val_loss: 3.2390, val_acc: 0.6329\n",
      "Epoch [2864], train_loss: 0.0029, val_loss: 4.5753, val_acc: 0.6066\n",
      "Epoch [2865], train_loss: 0.0015, val_loss: 4.5495, val_acc: 0.6092\n",
      "Epoch [2866], train_loss: 0.0006, val_loss: 5.0650, val_acc: 0.6196\n",
      "Epoch [2867], train_loss: 0.0287, val_loss: 5.2080, val_acc: 0.6066\n",
      "Epoch [2868], train_loss: 0.0141, val_loss: 5.3776, val_acc: 0.6049\n",
      "Epoch [2869], train_loss: 0.0294, val_loss: 4.6216, val_acc: 0.6258\n",
      "Epoch [2870], train_loss: 0.0075, val_loss: 4.1697, val_acc: 0.6336\n",
      "Epoch [2871], train_loss: 0.0006, val_loss: 4.6834, val_acc: 0.6310\n",
      "Epoch [2872], train_loss: 0.0002, val_loss: 4.9293, val_acc: 0.6228\n",
      "Epoch [2873], train_loss: 0.0001, val_loss: 4.9818, val_acc: 0.6254\n",
      "Epoch [2874], train_loss: 0.0001, val_loss: 5.1123, val_acc: 0.6280\n",
      "Epoch [2875], train_loss: 0.0001, val_loss: 5.3085, val_acc: 0.6307\n",
      "Epoch [2876], train_loss: 0.0001, val_loss: 5.4380, val_acc: 0.6333\n",
      "Epoch [2877], train_loss: 0.0000, val_loss: 5.6139, val_acc: 0.6254\n",
      "Epoch [2878], train_loss: 0.0000, val_loss: 5.6333, val_acc: 0.6228\n",
      "Epoch [2879], train_loss: 0.0000, val_loss: 5.6217, val_acc: 0.6307\n",
      "Epoch [2880], train_loss: 0.0000, val_loss: 5.5830, val_acc: 0.6280\n",
      "Epoch [2881], train_loss: 0.0000, val_loss: 5.6293, val_acc: 0.6280\n",
      "Epoch [2882], train_loss: 0.0000, val_loss: 5.7624, val_acc: 0.6280\n",
      "Epoch [2883], train_loss: 0.0000, val_loss: 5.8769, val_acc: 0.6280\n",
      "Epoch [2884], train_loss: 0.0000, val_loss: 5.8236, val_acc: 0.6333\n",
      "Epoch [2885], train_loss: 0.0000, val_loss: 5.9001, val_acc: 0.6333\n",
      "Epoch [2886], train_loss: 0.0000, val_loss: 5.9270, val_acc: 0.6280\n",
      "Epoch [2887], train_loss: 0.0000, val_loss: 5.9533, val_acc: 0.6280\n",
      "Epoch [2888], train_loss: 0.0001, val_loss: 6.0248, val_acc: 0.6280\n",
      "Epoch [2889], train_loss: 0.0000, val_loss: 6.2089, val_acc: 0.6307\n",
      "Epoch [2890], train_loss: 0.0000, val_loss: 6.0927, val_acc: 0.6359\n",
      "Epoch [2891], train_loss: 0.0000, val_loss: 6.2074, val_acc: 0.6307\n",
      "Epoch [2892], train_loss: 0.0000, val_loss: 6.1690, val_acc: 0.6333\n",
      "Epoch [2893], train_loss: 0.0000, val_loss: 6.4241, val_acc: 0.6280\n",
      "Epoch [2894], train_loss: 0.0000, val_loss: 6.1999, val_acc: 0.6359\n",
      "Epoch [2895], train_loss: 0.0000, val_loss: 6.2843, val_acc: 0.6333\n",
      "Epoch [2896], train_loss: 0.0000, val_loss: 6.3700, val_acc: 0.6280\n",
      "Epoch [2897], train_loss: 0.0000, val_loss: 6.3551, val_acc: 0.6359\n",
      "Epoch [2898], train_loss: 0.0000, val_loss: 6.2591, val_acc: 0.6333\n",
      "Epoch [2899], train_loss: 0.0000, val_loss: 6.2972, val_acc: 0.6280\n",
      "Epoch [2900], train_loss: 0.0000, val_loss: 6.5953, val_acc: 0.6280\n",
      "Epoch [2901], train_loss: 0.0000, val_loss: 6.4848, val_acc: 0.6307\n",
      "Epoch [2902], train_loss: 0.0000, val_loss: 6.5760, val_acc: 0.6280\n",
      "Epoch [2903], train_loss: 0.0000, val_loss: 6.6390, val_acc: 0.6254\n",
      "Epoch [2904], train_loss: 0.0000, val_loss: 6.6326, val_acc: 0.6254\n",
      "Epoch [2905], train_loss: 0.0000, val_loss: 6.5051, val_acc: 0.6280\n",
      "Epoch [2906], train_loss: 0.0000, val_loss: 6.7370, val_acc: 0.6280\n",
      "Epoch [2907], train_loss: 0.0000, val_loss: 6.6940, val_acc: 0.6333\n",
      "Epoch [2908], train_loss: 0.0000, val_loss: 6.7672, val_acc: 0.6280\n",
      "Epoch [2909], train_loss: 0.0000, val_loss: 6.6313, val_acc: 0.6280\n",
      "Epoch [2910], train_loss: 0.0000, val_loss: 6.7147, val_acc: 0.6307\n",
      "Epoch [2911], train_loss: 0.0000, val_loss: 6.8511, val_acc: 0.6307\n",
      "Epoch [2912], train_loss: 0.0051, val_loss: 6.8953, val_acc: 0.6443\n",
      "Epoch [2913], train_loss: 0.0277, val_loss: 4.6711, val_acc: 0.6092\n",
      "Epoch [2914], train_loss: 0.0160, val_loss: 3.6318, val_acc: 0.6440\n",
      "Epoch [2915], train_loss: 0.0240, val_loss: 2.5374, val_acc: 0.5939\n",
      "Epoch [2916], train_loss: 0.0134, val_loss: 3.4169, val_acc: 0.5828\n",
      "Epoch [2917], train_loss: 0.0008, val_loss: 4.6229, val_acc: 0.5779\n",
      "Epoch [2918], train_loss: 0.0003, val_loss: 4.9352, val_acc: 0.5939\n",
      "Epoch [2919], train_loss: 0.0002, val_loss: 5.1744, val_acc: 0.6043\n",
      "Epoch [2920], train_loss: 0.0001, val_loss: 5.2308, val_acc: 0.6043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2921], train_loss: 0.0001, val_loss: 5.4966, val_acc: 0.6095\n",
      "Epoch [2922], train_loss: 0.0001, val_loss: 5.6666, val_acc: 0.6095\n",
      "Epoch [2923], train_loss: 0.0000, val_loss: 5.6954, val_acc: 0.6043\n",
      "Epoch [2924], train_loss: 0.0000, val_loss: 5.8926, val_acc: 0.6043\n",
      "Epoch [2925], train_loss: 0.0000, val_loss: 5.9131, val_acc: 0.6069\n",
      "Epoch [2926], train_loss: 0.0000, val_loss: 6.1082, val_acc: 0.5991\n",
      "Epoch [2927], train_loss: 0.0000, val_loss: 6.0582, val_acc: 0.6043\n",
      "Epoch [2928], train_loss: 0.0001, val_loss: 6.2401, val_acc: 0.6043\n",
      "Epoch [2929], train_loss: 0.0000, val_loss: 6.3444, val_acc: 0.6040\n",
      "Epoch [2930], train_loss: 0.0000, val_loss: 6.4158, val_acc: 0.5936\n",
      "Epoch [2931], train_loss: 0.0000, val_loss: 6.3873, val_acc: 0.5910\n",
      "Epoch [2932], train_loss: 0.0000, val_loss: 6.4576, val_acc: 0.5991\n",
      "Epoch [2933], train_loss: 0.0001, val_loss: 6.6170, val_acc: 0.5936\n",
      "Epoch [2934], train_loss: 0.0000, val_loss: 6.6604, val_acc: 0.5991\n",
      "Epoch [2935], train_loss: 0.0000, val_loss: 6.7058, val_acc: 0.6017\n",
      "Epoch [2936], train_loss: 0.0000, val_loss: 6.7179, val_acc: 0.6017\n",
      "Epoch [2937], train_loss: 0.0000, val_loss: 6.8067, val_acc: 0.5910\n",
      "Epoch [2938], train_loss: 0.0001, val_loss: 6.6774, val_acc: 0.6017\n",
      "Epoch [2939], train_loss: 0.0000, val_loss: 7.1923, val_acc: 0.6121\n",
      "Epoch [2940], train_loss: 0.0001, val_loss: 7.2377, val_acc: 0.6043\n",
      "Epoch [2941], train_loss: 0.0000, val_loss: 7.1108, val_acc: 0.6017\n",
      "Epoch [2942], train_loss: 0.0000, val_loss: 7.2590, val_acc: 0.5965\n",
      "Epoch [2943], train_loss: 0.0000, val_loss: 7.0922, val_acc: 0.5939\n",
      "Epoch [2944], train_loss: 0.0000, val_loss: 7.3898, val_acc: 0.5965\n",
      "Epoch [2945], train_loss: 0.0000, val_loss: 7.2663, val_acc: 0.5939\n",
      "Epoch [2946], train_loss: 0.0000, val_loss: 7.1505, val_acc: 0.6017\n",
      "Epoch [2947], train_loss: 0.0000, val_loss: 7.4031, val_acc: 0.5991\n",
      "Epoch [2948], train_loss: 0.0000, val_loss: 7.5718, val_acc: 0.6043\n",
      "Epoch [2949], train_loss: 0.0000, val_loss: 7.3247, val_acc: 0.5991\n",
      "Epoch [2950], train_loss: 0.0000, val_loss: 7.1426, val_acc: 0.6095\n",
      "Epoch [2951], train_loss: 0.0031, val_loss: 7.5779, val_acc: 0.6173\n",
      "Epoch [2952], train_loss: 0.0480, val_loss: 5.9533, val_acc: 0.6040\n",
      "Epoch [2953], train_loss: 0.0381, val_loss: 3.7889, val_acc: 0.5516\n",
      "Epoch [2954], train_loss: 0.0111, val_loss: 3.9940, val_acc: 0.5727\n",
      "Epoch [2955], train_loss: 0.0012, val_loss: 5.0624, val_acc: 0.5591\n",
      "Epoch [2956], train_loss: 0.0003, val_loss: 5.6964, val_acc: 0.5565\n",
      "Epoch [2957], train_loss: 0.0020, val_loss: 5.8182, val_acc: 0.5643\n",
      "Epoch [2958], train_loss: 0.0016, val_loss: 6.8085, val_acc: 0.5695\n",
      "Epoch [2959], train_loss: 0.0002, val_loss: 6.8496, val_acc: 0.5695\n",
      "Epoch [2960], train_loss: 0.0001, val_loss: 6.8454, val_acc: 0.5669\n",
      "Epoch [2961], train_loss: 0.0001, val_loss: 6.9847, val_acc: 0.5666\n",
      "Epoch [2962], train_loss: 0.0000, val_loss: 7.0173, val_acc: 0.5666\n",
      "Epoch [2963], train_loss: 0.0000, val_loss: 7.0815, val_acc: 0.5747\n",
      "Epoch [2964], train_loss: 0.0000, val_loss: 7.0963, val_acc: 0.5721\n",
      "Epoch [2965], train_loss: 0.0000, val_loss: 7.0587, val_acc: 0.5695\n",
      "Epoch [2966], train_loss: 0.0027, val_loss: 7.2950, val_acc: 0.5666\n",
      "Epoch [2967], train_loss: 0.0070, val_loss: 6.0070, val_acc: 0.5695\n",
      "Epoch [2968], train_loss: 0.0012, val_loss: 6.2230, val_acc: 0.5773\n",
      "Epoch [2969], train_loss: 0.0002, val_loss: 6.6840, val_acc: 0.5877\n",
      "Epoch [2970], train_loss: 0.0003, val_loss: 7.1821, val_acc: 0.5825\n",
      "Epoch [2971], train_loss: 0.0001, val_loss: 7.1450, val_acc: 0.5851\n",
      "Epoch [2972], train_loss: 0.0001, val_loss: 7.3607, val_acc: 0.5773\n",
      "Epoch [2973], train_loss: 0.0052, val_loss: 6.7853, val_acc: 0.5854\n",
      "Epoch [2974], train_loss: 0.0003, val_loss: 7.0434, val_acc: 0.5828\n",
      "Epoch [2975], train_loss: 0.0005, val_loss: 7.1917, val_acc: 0.5910\n",
      "Epoch [2976], train_loss: 0.0001, val_loss: 7.8887, val_acc: 0.5805\n",
      "Epoch [2977], train_loss: 0.0001, val_loss: 7.6913, val_acc: 0.5962\n",
      "Epoch [2978], train_loss: 0.0000, val_loss: 8.2409, val_acc: 0.5910\n",
      "Epoch [2979], train_loss: 0.0000, val_loss: 8.1738, val_acc: 0.5962\n",
      "Epoch [2980], train_loss: 0.0000, val_loss: 8.1016, val_acc: 0.5962\n",
      "Epoch [2981], train_loss: 0.0000, val_loss: 8.2202, val_acc: 0.5988\n",
      "Epoch [2982], train_loss: 0.0004, val_loss: 7.8445, val_acc: 0.5939\n",
      "Epoch [2983], train_loss: 0.0005, val_loss: 8.0743, val_acc: 0.5936\n",
      "Epoch [2984], train_loss: 0.0064, val_loss: 8.3554, val_acc: 0.5880\n",
      "Epoch [2985], train_loss: 0.0168, val_loss: 7.0071, val_acc: 0.5588\n",
      "Epoch [2986], train_loss: 0.0072, val_loss: 3.4402, val_acc: 0.5773\n",
      "Epoch [2987], train_loss: 0.0011, val_loss: 4.3904, val_acc: 0.5770\n",
      "Epoch [2988], train_loss: 0.0002, val_loss: 4.8470, val_acc: 0.5741\n",
      "Epoch [2989], train_loss: 0.0001, val_loss: 5.0357, val_acc: 0.5874\n",
      "Epoch [2990], train_loss: 0.0001, val_loss: 5.1917, val_acc: 0.5741\n",
      "Epoch [2991], train_loss: 0.0001, val_loss: 5.2416, val_acc: 0.5715\n",
      "Epoch [2992], train_loss: 0.0000, val_loss: 5.4181, val_acc: 0.5822\n",
      "Epoch [2993], train_loss: 0.0000, val_loss: 5.5613, val_acc: 0.5718\n",
      "Epoch [2994], train_loss: 0.0001, val_loss: 5.5305, val_acc: 0.5692\n",
      "Epoch [2995], train_loss: 0.0000, val_loss: 5.5517, val_acc: 0.5718\n",
      "Epoch [2996], train_loss: 0.0002, val_loss: 5.8570, val_acc: 0.5718\n",
      "Epoch [2997], train_loss: 0.0000, val_loss: 5.9509, val_acc: 0.5877\n",
      "Epoch [2998], train_loss: 0.0000, val_loss: 5.8559, val_acc: 0.5744\n",
      "Epoch [2999], train_loss: 0.0000, val_loss: 6.0895, val_acc: 0.5692\n"
     ]
    }
   ],
   "source": [
    "# CTX = torch.device('cuda')\n",
    "# train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # train_dl.train_labels.to(CTX)\n",
    "# # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # val_dl.train_labels.to(CTX)\n",
    "num_epochs = 3000\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26874318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "    \n",
    "\n",
    "plot_accuracies(history)\n",
    "\n",
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60afc6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6602515578269958"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([x['val_acc'] for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3164b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
