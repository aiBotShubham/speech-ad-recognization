{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid\n",
    "\n",
    "data_path = 'adress.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = load_data(data_path)\n",
    "\n",
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(2379).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "# train_x.shape, train_y.shape\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "train_dataset = TensorDataset(train_x.to(CTX),train_y.to(CTX)) # create your datset\n",
    "\n",
    " # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09537d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(297).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "\n",
    "val_dataset = TensorDataset(val_x,val_y) # create your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4701d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 64\n",
    "val_size = 297\n",
    "# train_size = train_x.size(0) - val_size \n",
    "\n",
    "# train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "# print(f\"Length of Train Data : {len(train_data)}\")\n",
    "# print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 2379\n",
    "#Length of Validation Data : 297\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_dataset,batch_size, shuffle = True, num_workers = 0)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d492176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "#         print(x.shape())\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    \n",
    "class Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(256, 224, kernel_size=1, stride=1, padding=1),\n",
    "            BatchNorm2d(224),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(224, 224, kernel_size=1, stride=1, padding=1),\n",
    "            BatchNorm2d(224),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "            Conv2d(224, 200, kernel_size=1, stride=1, padding=1),\n",
    "            BatchNorm2d(200),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "            Conv2d(200, 128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(128 * 26 * 7, 128*16),\n",
    "            Linear(128 * 16, 128*4),\n",
    "            Linear(128 * 4, 128*2),\n",
    "            Linear(256, 64),\n",
    "            Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.attention = CBAM(gate_channels=128)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "#         x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f81fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(300, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 224, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(224, 200, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(200, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=23296, out_features=2048, bias=True)\n",
      "    (1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention): CBAM(\n",
      "    (ChannelGate): ChannelGate(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten()\n",
      "        (1): Linear(in_features=128, out_features=8, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=8, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (SpatialGate): SpatialGate(\n",
      "      (compress): ChannelPool()\n",
      "      (spatial): BasicConv(\n",
      "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477fd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 256, 40, 3]         691,456\n",
      "       BatchNorm2d-2           [-1, 256, 40, 3]             512\n",
      "              ReLU-3           [-1, 256, 40, 3]               0\n",
      "         MaxPool2d-4           [-1, 256, 20, 1]               0\n",
      "            Conv2d-5           [-1, 224, 22, 3]          57,568\n",
      "       BatchNorm2d-6           [-1, 224, 22, 3]             448\n",
      "              ReLU-7           [-1, 224, 22, 3]               0\n",
      "         MaxPool2d-8           [-1, 224, 22, 3]               0\n",
      "            Conv2d-9           [-1, 224, 24, 5]          50,400\n",
      "      BatchNorm2d-10           [-1, 224, 24, 5]             448\n",
      "             ReLU-11           [-1, 224, 24, 5]               0\n",
      "        MaxPool2d-12           [-1, 224, 24, 5]               0\n",
      "           Conv2d-13           [-1, 200, 26, 7]          45,000\n",
      "      BatchNorm2d-14           [-1, 200, 26, 7]             400\n",
      "             ReLU-15           [-1, 200, 26, 7]               0\n",
      "        MaxPool2d-16           [-1, 200, 26, 7]               0\n",
      "           Conv2d-17           [-1, 128, 26, 7]         230,528\n",
      "      BatchNorm2d-18           [-1, 128, 26, 7]             256\n",
      "             ReLU-19           [-1, 128, 26, 7]               0\n",
      "        MaxPool2d-20           [-1, 128, 26, 7]               0\n",
      "           Conv2d-21           [-1, 128, 26, 7]         147,584\n",
      "      BatchNorm2d-22           [-1, 128, 26, 7]             256\n",
      "             ReLU-23           [-1, 128, 26, 7]               0\n",
      "        MaxPool2d-24           [-1, 128, 26, 7]               0\n",
      "          Flatten-25                  [-1, 128]               0\n",
      "           Linear-26                    [-1, 8]           1,032\n",
      "             ReLU-27                    [-1, 8]               0\n",
      "           Linear-28                  [-1, 128]           1,152\n",
      "          Flatten-29                  [-1, 128]               0\n",
      "           Linear-30                    [-1, 8]           1,032\n",
      "             ReLU-31                    [-1, 8]               0\n",
      "           Linear-32                  [-1, 128]           1,152\n",
      "      ChannelGate-33           [-1, 128, 26, 7]               0\n",
      "      ChannelPool-34             [-1, 2, 26, 7]               0\n",
      "           Conv2d-35             [-1, 1, 26, 7]              98\n",
      "      BatchNorm2d-36             [-1, 1, 26, 7]               2\n",
      "        BasicConv-37             [-1, 1, 26, 7]               0\n",
      "      SpatialGate-38           [-1, 128, 26, 7]               0\n",
      "             CBAM-39           [-1, 128, 26, 7]               0\n",
      "           Linear-40                 [-1, 2048]      47,712,256\n",
      "           Linear-41                  [-1, 512]       1,049,088\n",
      "           Linear-42                  [-1, 256]         131,328\n",
      "           Linear-43                   [-1, 64]          16,448\n",
      "           Linear-44                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 50,138,574\n",
      "Trainable params: 50,138,574\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 5.11\n",
      "Params size (MB): 191.26\n",
      "Estimated Total Size (MB): 196.51\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (300, 40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a72cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1.0176, val_loss: 0.6911, val_acc: 0.5298\n",
      "Epoch [1], train_loss: 0.6905, val_loss: 0.6888, val_acc: 0.5347\n",
      "Epoch [2], train_loss: 0.6894, val_loss: 0.6840, val_acc: 0.5321\n",
      "Epoch [3], train_loss: 0.6860, val_loss: 0.6930, val_acc: 0.5213\n",
      "Epoch [4], train_loss: 0.6825, val_loss: 0.6839, val_acc: 0.5086\n",
      "Epoch [5], train_loss: 0.6840, val_loss: 0.6818, val_acc: 0.4953\n",
      "Epoch [6], train_loss: 0.6836, val_loss: 0.7043, val_acc: 0.5295\n",
      "Epoch [7], train_loss: 0.6859, val_loss: 0.6854, val_acc: 0.4979\n",
      "Epoch [8], train_loss: 0.6823, val_loss: 0.6821, val_acc: 0.5086\n",
      "Epoch [9], train_loss: 0.6818, val_loss: 0.6888, val_acc: 0.5402\n",
      "Epoch [10], train_loss: 0.6808, val_loss: 0.6877, val_acc: 0.5213\n",
      "Epoch [11], train_loss: 0.6795, val_loss: 0.7008, val_acc: 0.5112\n",
      "Epoch [12], train_loss: 0.6828, val_loss: 0.6891, val_acc: 0.5321\n",
      "Epoch [13], train_loss: 0.6813, val_loss: 0.6940, val_acc: 0.5138\n",
      "Epoch [14], train_loss: 0.6791, val_loss: 0.6850, val_acc: 0.5217\n",
      "Epoch [15], train_loss: 0.6803, val_loss: 0.6861, val_acc: 0.5402\n",
      "Epoch [16], train_loss: 0.6752, val_loss: 0.6815, val_acc: 0.5405\n",
      "Epoch [17], train_loss: 0.6780, val_loss: 0.6802, val_acc: 0.5243\n",
      "Epoch [18], train_loss: 0.6740, val_loss: 0.6821, val_acc: 0.5561\n",
      "Epoch [19], train_loss: 0.6689, val_loss: 0.6707, val_acc: 0.5425\n",
      "Epoch [20], train_loss: 0.6668, val_loss: 0.6876, val_acc: 0.5373\n",
      "Epoch [21], train_loss: 0.6631, val_loss: 0.6970, val_acc: 0.5168\n",
      "Epoch [22], train_loss: 0.6634, val_loss: 0.6730, val_acc: 0.5281\n",
      "Epoch [23], train_loss: 0.6635, val_loss: 0.6875, val_acc: 0.5373\n",
      "Epoch [24], train_loss: 0.6590, val_loss: 0.6712, val_acc: 0.5561\n",
      "Epoch [25], train_loss: 0.6466, val_loss: 0.7236, val_acc: 0.5031\n",
      "Epoch [26], train_loss: 0.6339, val_loss: 0.6767, val_acc: 0.5333\n",
      "Epoch [27], train_loss: 0.6324, val_loss: 0.6933, val_acc: 0.5295\n",
      "Epoch [28], train_loss: 0.6377, val_loss: 0.6573, val_acc: 0.5675\n",
      "Epoch [29], train_loss: 0.6413, val_loss: 0.6841, val_acc: 0.5161\n",
      "Epoch [30], train_loss: 0.6523, val_loss: 0.7374, val_acc: 0.5295\n",
      "Epoch [31], train_loss: 0.6245, val_loss: 0.7114, val_acc: 0.5412\n",
      "Epoch [32], train_loss: 0.6264, val_loss: 0.7865, val_acc: 0.5318\n",
      "Epoch [33], train_loss: 0.6010, val_loss: 0.7018, val_acc: 0.5054\n",
      "Epoch [34], train_loss: 0.5883, val_loss: 0.7565, val_acc: 0.5187\n",
      "Epoch [35], train_loss: 0.5944, val_loss: 0.6969, val_acc: 0.5106\n",
      "Epoch [36], train_loss: 0.5970, val_loss: 0.8770, val_acc: 0.5220\n",
      "Epoch [37], train_loss: 0.5889, val_loss: 0.7043, val_acc: 0.5405\n",
      "Epoch [38], train_loss: 0.5946, val_loss: 0.7273, val_acc: 0.5350\n",
      "Epoch [39], train_loss: 0.5646, val_loss: 0.8329, val_acc: 0.5347\n",
      "Epoch [40], train_loss: 0.5746, val_loss: 0.7857, val_acc: 0.5913\n",
      "Epoch [41], train_loss: 0.5570, val_loss: 0.6916, val_acc: 0.6023\n",
      "Epoch [42], train_loss: 0.5369, val_loss: 0.7090, val_acc: 0.5887\n",
      "Epoch [43], train_loss: 0.5332, val_loss: 0.7211, val_acc: 0.5457\n",
      "Epoch [44], train_loss: 0.5258, val_loss: 0.9200, val_acc: 0.5649\n",
      "Epoch [45], train_loss: 0.5027, val_loss: 1.1306, val_acc: 0.6258\n",
      "Epoch [46], train_loss: 0.5583, val_loss: 0.7450, val_acc: 0.5487\n",
      "Epoch [47], train_loss: 0.5224, val_loss: 0.8957, val_acc: 0.6176\n",
      "Epoch [48], train_loss: 0.4874, val_loss: 1.0407, val_acc: 0.6385\n",
      "Epoch [49], train_loss: 0.4666, val_loss: 1.0597, val_acc: 0.5828\n",
      "Epoch [50], train_loss: 0.4385, val_loss: 1.2807, val_acc: 0.6153\n",
      "Epoch [51], train_loss: 0.4177, val_loss: 1.6485, val_acc: 0.5727\n",
      "Epoch [52], train_loss: 0.4880, val_loss: 0.9734, val_acc: 0.6121\n",
      "Epoch [53], train_loss: 0.4361, val_loss: 1.0112, val_acc: 0.6072\n",
      "Epoch [54], train_loss: 0.4069, val_loss: 0.8247, val_acc: 0.6254\n",
      "Epoch [55], train_loss: 0.3939, val_loss: 0.8691, val_acc: 0.6573\n",
      "Epoch [56], train_loss: 0.3485, val_loss: 1.4945, val_acc: 0.5887\n",
      "Epoch [57], train_loss: 0.3981, val_loss: 1.2784, val_acc: 0.5802\n",
      "Epoch [58], train_loss: 0.3426, val_loss: 1.2711, val_acc: 0.6141\n",
      "Epoch [59], train_loss: 0.3186, val_loss: 1.6448, val_acc: 0.6228\n",
      "Epoch [60], train_loss: 0.3103, val_loss: 1.3437, val_acc: 0.5523\n",
      "Epoch [61], train_loss: 0.4514, val_loss: 0.7812, val_acc: 0.6115\n",
      "Epoch [62], train_loss: 0.3316, val_loss: 1.6950, val_acc: 0.5936\n",
      "Epoch [63], train_loss: 0.2672, val_loss: 1.5459, val_acc: 0.6141\n",
      "Epoch [64], train_loss: 0.2395, val_loss: 1.8365, val_acc: 0.5851\n",
      "Epoch [65], train_loss: 0.2052, val_loss: 1.2958, val_acc: 0.6417\n",
      "Epoch [66], train_loss: 0.2175, val_loss: 1.8176, val_acc: 0.5962\n",
      "Epoch [67], train_loss: 0.1691, val_loss: 2.6802, val_acc: 0.6098\n",
      "Epoch [68], train_loss: 0.1660, val_loss: 1.5286, val_acc: 0.5588\n",
      "Epoch [69], train_loss: 0.1693, val_loss: 1.5167, val_acc: 0.5779\n",
      "Epoch [70], train_loss: 0.1189, val_loss: 2.4361, val_acc: 0.5890\n",
      "Epoch [71], train_loss: 0.1268, val_loss: 2.8552, val_acc: 0.5942\n",
      "Epoch [72], train_loss: 0.1288, val_loss: 2.3838, val_acc: 0.5997\n",
      "Epoch [73], train_loss: 0.1006, val_loss: 2.1812, val_acc: 0.6108\n",
      "Epoch [74], train_loss: 0.0986, val_loss: 2.5729, val_acc: 0.5887\n",
      "Epoch [75], train_loss: 0.1195, val_loss: 3.3505, val_acc: 0.6241\n",
      "Epoch [76], train_loss: 0.1125, val_loss: 2.1465, val_acc: 0.5783\n",
      "Epoch [77], train_loss: 0.0639, val_loss: 2.7853, val_acc: 0.5597\n",
      "Epoch [78], train_loss: 0.0564, val_loss: 3.9439, val_acc: 0.5805\n",
      "Epoch [79], train_loss: 0.0366, val_loss: 2.5873, val_acc: 0.5835\n",
      "Epoch [80], train_loss: 0.0454, val_loss: 2.2097, val_acc: 0.6075\n",
      "Epoch [81], train_loss: 0.0695, val_loss: 2.9140, val_acc: 0.6105\n",
      "Epoch [82], train_loss: 0.0386, val_loss: 2.8328, val_acc: 0.6046\n",
      "Epoch [83], train_loss: 0.0389, val_loss: 2.8535, val_acc: 0.6023\n",
      "Epoch [84], train_loss: 0.0216, val_loss: 3.8016, val_acc: 0.5945\n",
      "Epoch [85], train_loss: 0.0423, val_loss: 2.4369, val_acc: 0.5701\n",
      "Epoch [86], train_loss: 0.0281, val_loss: 3.6962, val_acc: 0.6261\n",
      "Epoch [87], train_loss: 0.0107, val_loss: 4.2290, val_acc: 0.6124\n",
      "Epoch [88], train_loss: 0.0155, val_loss: 4.5739, val_acc: 0.6072\n",
      "Epoch [89], train_loss: 0.0449, val_loss: 2.9320, val_acc: 0.5988\n",
      "Epoch [90], train_loss: 0.0545, val_loss: 2.5952, val_acc: 0.5939\n",
      "Epoch [91], train_loss: 0.0355, val_loss: 2.9959, val_acc: 0.5932\n",
      "Epoch [92], train_loss: 0.0255, val_loss: 3.7479, val_acc: 0.5939\n",
      "Epoch [93], train_loss: 0.0126, val_loss: 4.2156, val_acc: 0.5942\n",
      "Epoch [94], train_loss: 0.0371, val_loss: 2.9512, val_acc: 0.6368\n",
      "Epoch [95], train_loss: 0.0175, val_loss: 5.1522, val_acc: 0.6023\n",
      "Epoch [96], train_loss: 0.0232, val_loss: 3.7796, val_acc: 0.6453\n",
      "Epoch [97], train_loss: 0.0472, val_loss: 3.7576, val_acc: 0.6017\n",
      "Epoch [98], train_loss: 0.0134, val_loss: 3.2779, val_acc: 0.6391\n",
      "Epoch [99], train_loss: 0.0137, val_loss: 4.4965, val_acc: 0.5750\n",
      "Epoch [100], train_loss: 0.0318, val_loss: 2.1125, val_acc: 0.6423\n",
      "Epoch [101], train_loss: 0.0201, val_loss: 4.8397, val_acc: 0.5646\n",
      "Epoch [102], train_loss: 0.0888, val_loss: 2.0143, val_acc: 0.6127\n",
      "Epoch [103], train_loss: 0.0273, val_loss: 2.9937, val_acc: 0.6056\n",
      "Epoch [104], train_loss: 0.0237, val_loss: 2.7918, val_acc: 0.6401\n",
      "Epoch [105], train_loss: 0.0071, val_loss: 5.3779, val_acc: 0.6339\n",
      "Epoch [106], train_loss: 0.0184, val_loss: 3.9864, val_acc: 0.5867\n",
      "Epoch [107], train_loss: 0.0253, val_loss: 3.0418, val_acc: 0.6342\n",
      "Epoch [108], train_loss: 0.0029, val_loss: 6.2835, val_acc: 0.5835\n",
      "Epoch [109], train_loss: 0.0175, val_loss: 3.1834, val_acc: 0.5591\n",
      "Epoch [110], train_loss: 0.0130, val_loss: 3.0466, val_acc: 0.6157\n",
      "Epoch [111], train_loss: 0.0019, val_loss: 4.9843, val_acc: 0.6258\n",
      "Epoch [112], train_loss: 0.0012, val_loss: 5.4616, val_acc: 0.6153\n",
      "Epoch [113], train_loss: 0.0004, val_loss: 5.8287, val_acc: 0.6049\n",
      "Epoch [114], train_loss: 0.0002, val_loss: 6.3428, val_acc: 0.5968\n",
      "Epoch [115], train_loss: 0.0000, val_loss: 6.6233, val_acc: 0.6023\n",
      "Epoch [116], train_loss: 0.0000, val_loss: 6.8928, val_acc: 0.6105\n",
      "Epoch [117], train_loss: 0.0000, val_loss: 7.0243, val_acc: 0.6079\n",
      "Epoch [118], train_loss: 0.0000, val_loss: 7.1148, val_acc: 0.5916\n",
      "Epoch [119], train_loss: 0.0000, val_loss: 7.1684, val_acc: 0.5916\n",
      "Epoch [120], train_loss: 0.0000, val_loss: 7.1717, val_acc: 0.5916\n",
      "Epoch [121], train_loss: 0.0000, val_loss: 7.1120, val_acc: 0.6079\n",
      "Epoch [122], train_loss: 0.0000, val_loss: 7.2127, val_acc: 0.6079\n",
      "Epoch [123], train_loss: 0.0000, val_loss: 7.3093, val_acc: 0.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 0.0000, val_loss: 7.2775, val_acc: 0.5997\n",
      "Epoch [125], train_loss: 0.0000, val_loss: 7.3251, val_acc: 0.5942\n",
      "Epoch [126], train_loss: 0.0000, val_loss: 7.3685, val_acc: 0.5916\n",
      "Epoch [127], train_loss: 0.0000, val_loss: 7.3124, val_acc: 0.6079\n",
      "Epoch [128], train_loss: 0.0000, val_loss: 7.4094, val_acc: 0.5916\n",
      "Epoch [129], train_loss: 0.0000, val_loss: 7.4194, val_acc: 0.5971\n",
      "Epoch [130], train_loss: 0.0000, val_loss: 7.4585, val_acc: 0.6049\n",
      "Epoch [131], train_loss: 0.0000, val_loss: 7.4987, val_acc: 0.5968\n",
      "Epoch [132], train_loss: 0.0000, val_loss: 7.5555, val_acc: 0.6049\n",
      "Epoch [133], train_loss: 0.0000, val_loss: 7.5311, val_acc: 0.5997\n",
      "Epoch [134], train_loss: 0.0000, val_loss: 7.4634, val_acc: 0.6075\n",
      "Epoch [135], train_loss: 0.0000, val_loss: 7.5769, val_acc: 0.6049\n",
      "Epoch [136], train_loss: 0.0000, val_loss: 7.5913, val_acc: 0.5994\n",
      "Epoch [137], train_loss: 0.0000, val_loss: 7.5780, val_acc: 0.6131\n",
      "Epoch [138], train_loss: 0.0000, val_loss: 7.5938, val_acc: 0.6049\n",
      "Epoch [139], train_loss: 0.0000, val_loss: 7.6339, val_acc: 0.6131\n",
      "Epoch [140], train_loss: 0.0000, val_loss: 7.5560, val_acc: 0.6183\n",
      "Epoch [141], train_loss: 0.0000, val_loss: 7.6374, val_acc: 0.5965\n",
      "Epoch [142], train_loss: 0.0000, val_loss: 7.6615, val_acc: 0.5968\n",
      "Epoch [143], train_loss: 0.0000, val_loss: 7.7188, val_acc: 0.5942\n",
      "Epoch [144], train_loss: 0.0000, val_loss: 7.7684, val_acc: 0.5968\n",
      "Epoch [145], train_loss: 0.0000, val_loss: 7.7123, val_acc: 0.6049\n",
      "Epoch [146], train_loss: 0.0000, val_loss: 7.7408, val_acc: 0.6049\n",
      "Epoch [147], train_loss: 0.0000, val_loss: 7.8044, val_acc: 0.5887\n",
      "Epoch [148], train_loss: 0.0000, val_loss: 7.7966, val_acc: 0.6075\n",
      "Epoch [149], train_loss: 0.0000, val_loss: 7.8502, val_acc: 0.6127\n",
      "Epoch [150], train_loss: 0.0000, val_loss: 7.7936, val_acc: 0.5994\n",
      "Epoch [151], train_loss: 0.0000, val_loss: 7.8439, val_acc: 0.6075\n",
      "Epoch [152], train_loss: 0.0000, val_loss: 7.8674, val_acc: 0.6101\n",
      "Epoch [153], train_loss: 0.0000, val_loss: 7.8761, val_acc: 0.6020\n",
      "Epoch [154], train_loss: 0.0000, val_loss: 7.9462, val_acc: 0.5861\n",
      "Epoch [155], train_loss: 0.0000, val_loss: 7.9026, val_acc: 0.5994\n",
      "Epoch [156], train_loss: 0.0000, val_loss: 7.8616, val_acc: 0.6075\n",
      "Epoch [157], train_loss: 0.0005, val_loss: 8.7514, val_acc: 0.5884\n",
      "Epoch [158], train_loss: 0.4838, val_loss: 3.5252, val_acc: 0.5477\n",
      "Epoch [159], train_loss: 0.5339, val_loss: 1.4592, val_acc: 0.5750\n",
      "Epoch [160], train_loss: 0.1190, val_loss: 2.1180, val_acc: 0.5649\n",
      "Epoch [161], train_loss: 0.0903, val_loss: 2.1600, val_acc: 0.5672\n",
      "Epoch [162], train_loss: 0.0551, val_loss: 3.2080, val_acc: 0.5994\n",
      "Epoch [163], train_loss: 0.0382, val_loss: 3.1008, val_acc: 0.5884\n",
      "Epoch [164], train_loss: 0.0336, val_loss: 3.8429, val_acc: 0.5786\n",
      "Epoch [165], train_loss: 0.0243, val_loss: 4.3487, val_acc: 0.5194\n",
      "Epoch [166], train_loss: 0.0317, val_loss: 3.2927, val_acc: 0.6075\n",
      "Epoch [167], train_loss: 0.0635, val_loss: 3.6575, val_acc: 0.6052\n",
      "Epoch [168], train_loss: 0.0140, val_loss: 4.5975, val_acc: 0.6079\n",
      "Epoch [169], train_loss: 0.0207, val_loss: 4.6246, val_acc: 0.5704\n",
      "Epoch [170], train_loss: 0.0294, val_loss: 3.7595, val_acc: 0.5568\n",
      "Epoch [171], train_loss: 0.0124, val_loss: 4.4196, val_acc: 0.5779\n",
      "Epoch [172], train_loss: 0.0303, val_loss: 3.3913, val_acc: 0.5675\n",
      "Epoch [173], train_loss: 0.0101, val_loss: 4.5774, val_acc: 0.5861\n",
      "Epoch [174], train_loss: 0.0517, val_loss: 2.8034, val_acc: 0.5405\n",
      "Epoch [175], train_loss: 0.0266, val_loss: 3.1550, val_acc: 0.5965\n",
      "Epoch [176], train_loss: 0.0058, val_loss: 4.1072, val_acc: 0.6043\n",
      "Epoch [177], train_loss: 0.0030, val_loss: 4.1371, val_acc: 0.6072\n",
      "Epoch [178], train_loss: 0.0003, val_loss: 4.7296, val_acc: 0.6101\n",
      "Epoch [179], train_loss: 0.0001, val_loss: 4.9529, val_acc: 0.6075\n",
      "Epoch [180], train_loss: 0.0000, val_loss: 5.1385, val_acc: 0.5916\n",
      "Epoch [181], train_loss: 0.0000, val_loss: 5.2006, val_acc: 0.5968\n",
      "Epoch [182], train_loss: 0.0000, val_loss: 5.2608, val_acc: 0.5942\n",
      "Epoch [183], train_loss: 0.0000, val_loss: 5.3357, val_acc: 0.5942\n",
      "Epoch [184], train_loss: 0.0000, val_loss: 5.3878, val_acc: 0.5942\n",
      "Epoch [185], train_loss: 0.0001, val_loss: 5.4750, val_acc: 0.5991\n",
      "Epoch [186], train_loss: 0.0170, val_loss: 3.9958, val_acc: 0.6127\n",
      "Epoch [187], train_loss: 0.0102, val_loss: 4.0070, val_acc: 0.5942\n",
      "Epoch [188], train_loss: 0.0078, val_loss: 4.2803, val_acc: 0.6000\n",
      "Epoch [189], train_loss: 0.0446, val_loss: 3.5484, val_acc: 0.6046\n",
      "Epoch [190], train_loss: 0.0379, val_loss: 3.3547, val_acc: 0.5756\n",
      "Epoch [191], train_loss: 0.1026, val_loss: 3.2254, val_acc: 0.5971\n",
      "Epoch [192], train_loss: 0.0090, val_loss: 5.5184, val_acc: 0.5756\n",
      "Epoch [193], train_loss: 0.0147, val_loss: 5.0392, val_acc: 0.5968\n",
      "Epoch [194], train_loss: 0.0167, val_loss: 5.6080, val_acc: 0.5669\n",
      "Epoch [195], train_loss: 0.0171, val_loss: 4.3190, val_acc: 0.5854\n",
      "Epoch [196], train_loss: 0.0158, val_loss: 4.2659, val_acc: 0.5695\n",
      "Epoch [197], train_loss: 0.0081, val_loss: 5.1180, val_acc: 0.5727\n",
      "Epoch [198], train_loss: 0.0045, val_loss: 5.9088, val_acc: 0.5857\n",
      "Epoch [199], train_loss: 0.0026, val_loss: 6.2463, val_acc: 0.5672\n",
      "Epoch [200], train_loss: 0.0005, val_loss: 7.1291, val_acc: 0.5724\n",
      "Epoch [201], train_loss: 0.0002, val_loss: 7.0464, val_acc: 0.5750\n",
      "Epoch [202], train_loss: 0.0000, val_loss: 7.1743, val_acc: 0.5802\n",
      "Epoch [203], train_loss: 0.0000, val_loss: 7.3945, val_acc: 0.5750\n",
      "Epoch [204], train_loss: 0.0000, val_loss: 7.3090, val_acc: 0.5802\n",
      "Epoch [205], train_loss: 0.0000, val_loss: 7.4234, val_acc: 0.5802\n",
      "Epoch [206], train_loss: 0.0000, val_loss: 7.4900, val_acc: 0.5776\n",
      "Epoch [207], train_loss: 0.0000, val_loss: 7.4918, val_acc: 0.5802\n",
      "Epoch [208], train_loss: 0.0000, val_loss: 7.5522, val_acc: 0.5776\n",
      "Epoch [209], train_loss: 0.0000, val_loss: 7.6621, val_acc: 0.5643\n",
      "Epoch [210], train_loss: 0.0000, val_loss: 7.5904, val_acc: 0.5802\n",
      "Epoch [211], train_loss: 0.0000, val_loss: 7.6989, val_acc: 0.5750\n",
      "Epoch [212], train_loss: 0.0000, val_loss: 7.6365, val_acc: 0.5802\n",
      "Epoch [213], train_loss: 0.0024, val_loss: 7.4195, val_acc: 0.5561\n",
      "Epoch [214], train_loss: 0.1157, val_loss: 2.4008, val_acc: 0.5831\n",
      "Epoch [215], train_loss: 0.0448, val_loss: 3.3357, val_acc: 0.5509\n",
      "Epoch [216], train_loss: 0.0243, val_loss: 3.6073, val_acc: 0.5750\n",
      "Epoch [217], train_loss: 0.0226, val_loss: 4.0689, val_acc: 0.5936\n",
      "Epoch [218], train_loss: 0.0797, val_loss: 3.2667, val_acc: 0.5649\n",
      "Epoch [219], train_loss: 0.0027, val_loss: 5.7080, val_acc: 0.5675\n",
      "Epoch [220], train_loss: 0.0007, val_loss: 6.2404, val_acc: 0.5701\n",
      "Epoch [221], train_loss: 0.0004, val_loss: 6.7915, val_acc: 0.5672\n",
      "Epoch [222], train_loss: 0.0001, val_loss: 7.1508, val_acc: 0.5701\n",
      "Epoch [223], train_loss: 0.0001, val_loss: 7.3013, val_acc: 0.5727\n",
      "Epoch [224], train_loss: 0.0000, val_loss: 7.1824, val_acc: 0.5835\n",
      "Epoch [225], train_loss: 0.0000, val_loss: 7.2946, val_acc: 0.5727\n",
      "Epoch [226], train_loss: 0.0000, val_loss: 7.2629, val_acc: 0.5727\n",
      "Epoch [227], train_loss: 0.0000, val_loss: 7.4165, val_acc: 0.5701\n",
      "Epoch [228], train_loss: 0.0000, val_loss: 7.5915, val_acc: 0.5701\n",
      "Epoch [229], train_loss: 0.0000, val_loss: 7.4967, val_acc: 0.5701\n",
      "Epoch [230], train_loss: 0.0000, val_loss: 7.6387, val_acc: 0.5675\n",
      "Epoch [231], train_loss: 0.0000, val_loss: 7.7463, val_acc: 0.5675\n",
      "Epoch [232], train_loss: 0.0000, val_loss: 7.7318, val_acc: 0.5675\n",
      "Epoch [233], train_loss: 0.0000, val_loss: 7.7473, val_acc: 0.5675\n",
      "Epoch [234], train_loss: 0.0000, val_loss: 7.8667, val_acc: 0.5727\n",
      "Epoch [235], train_loss: 0.0000, val_loss: 7.8560, val_acc: 0.5727\n",
      "Epoch [236], train_loss: 0.0000, val_loss: 7.9439, val_acc: 0.5727\n",
      "Epoch [237], train_loss: 0.0000, val_loss: 7.9314, val_acc: 0.5756\n",
      "Epoch [238], train_loss: 0.0053, val_loss: 6.1407, val_acc: 0.5756\n",
      "Epoch [239], train_loss: 0.0360, val_loss: 3.5652, val_acc: 0.5861\n",
      "Epoch [240], train_loss: 0.0171, val_loss: 4.4578, val_acc: 0.5828\n",
      "Epoch [241], train_loss: 0.0129, val_loss: 4.6652, val_acc: 0.5643\n",
      "Epoch [242], train_loss: 0.0218, val_loss: 3.3858, val_acc: 0.5405\n",
      "Epoch [243], train_loss: 0.0135, val_loss: 4.1884, val_acc: 0.5805\n",
      "Epoch [244], train_loss: 0.0028, val_loss: 4.6472, val_acc: 0.5968\n",
      "Epoch [245], train_loss: 0.0132, val_loss: 4.9597, val_acc: 0.5756\n",
      "Epoch [246], train_loss: 0.0727, val_loss: 2.9785, val_acc: 0.5626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247], train_loss: 0.0382, val_loss: 2.6138, val_acc: 0.5704\n",
      "Epoch [248], train_loss: 0.0207, val_loss: 3.5619, val_acc: 0.5861\n",
      "Epoch [249], train_loss: 0.0155, val_loss: 4.2620, val_acc: 0.5753\n",
      "Epoch [250], train_loss: 0.0100, val_loss: 4.3079, val_acc: 0.5805\n",
      "Epoch [251], train_loss: 0.0044, val_loss: 5.4982, val_acc: 0.5887\n",
      "Epoch [252], train_loss: 0.0164, val_loss: 5.0975, val_acc: 0.6043\n",
      "Epoch [253], train_loss: 0.0284, val_loss: 3.8379, val_acc: 0.5701\n",
      "Epoch [254], train_loss: 0.0021, val_loss: 5.9253, val_acc: 0.5779\n",
      "Epoch [255], train_loss: 0.0014, val_loss: 6.8023, val_acc: 0.5672\n",
      "Epoch [256], train_loss: 0.0013, val_loss: 6.8669, val_acc: 0.5535\n",
      "Epoch [257], train_loss: 0.0094, val_loss: 4.9455, val_acc: 0.5490\n",
      "Epoch [258], train_loss: 0.0366, val_loss: 3.3485, val_acc: 0.6020\n",
      "Epoch [259], train_loss: 0.0079, val_loss: 4.3745, val_acc: 0.5805\n",
      "Epoch [260], train_loss: 0.0084, val_loss: 4.2365, val_acc: 0.5880\n",
      "Epoch [261], train_loss: 0.0032, val_loss: 5.1688, val_acc: 0.6202\n",
      "Epoch [262], train_loss: 0.0001, val_loss: 6.1035, val_acc: 0.6043\n",
      "Epoch [263], train_loss: 0.0000, val_loss: 6.3885, val_acc: 0.6069\n",
      "Epoch [264], train_loss: 0.0000, val_loss: 6.5161, val_acc: 0.6043\n",
      "Epoch [265], train_loss: 0.0000, val_loss: 6.6142, val_acc: 0.5991\n",
      "Epoch [266], train_loss: 0.0000, val_loss: 6.7071, val_acc: 0.6069\n",
      "Epoch [267], train_loss: 0.0000, val_loss: 6.8324, val_acc: 0.6069\n",
      "Epoch [268], train_loss: 0.0000, val_loss: 6.9127, val_acc: 0.6017\n",
      "Epoch [269], train_loss: 0.0000, val_loss: 6.9504, val_acc: 0.6043\n",
      "Epoch [270], train_loss: 0.0000, val_loss: 6.9801, val_acc: 0.6043\n",
      "Epoch [271], train_loss: 0.0000, val_loss: 7.0058, val_acc: 0.6043\n",
      "Epoch [272], train_loss: 0.0000, val_loss: 7.0754, val_acc: 0.6043\n",
      "Epoch [273], train_loss: 0.0000, val_loss: 7.0789, val_acc: 0.6069\n",
      "Epoch [274], train_loss: 0.0000, val_loss: 7.1326, val_acc: 0.6098\n",
      "Epoch [275], train_loss: 0.0000, val_loss: 7.1324, val_acc: 0.6095\n",
      "Epoch [276], train_loss: 0.0000, val_loss: 7.1450, val_acc: 0.6069\n",
      "Epoch [277], train_loss: 0.0000, val_loss: 7.2363, val_acc: 0.6017\n",
      "Epoch [278], train_loss: 0.0000, val_loss: 7.2516, val_acc: 0.6043\n",
      "Epoch [279], train_loss: 0.0000, val_loss: 7.2665, val_acc: 0.6017\n",
      "Epoch [280], train_loss: 0.0000, val_loss: 7.2709, val_acc: 0.6043\n",
      "Epoch [281], train_loss: 0.0000, val_loss: 7.4431, val_acc: 0.6014\n",
      "Epoch [282], train_loss: 0.0000, val_loss: 7.5251, val_acc: 0.5906\n",
      "Epoch [283], train_loss: 0.0000, val_loss: 7.5030, val_acc: 0.5988\n",
      "Epoch [284], train_loss: 0.0000, val_loss: 7.5323, val_acc: 0.5880\n",
      "Epoch [285], train_loss: 0.0000, val_loss: 7.6290, val_acc: 0.5936\n",
      "Epoch [286], train_loss: 0.0000, val_loss: 7.6378, val_acc: 0.5880\n",
      "Epoch [287], train_loss: 0.0000, val_loss: 7.6394, val_acc: 0.5962\n",
      "Epoch [288], train_loss: 0.0000, val_loss: 7.7141, val_acc: 0.5962\n",
      "Epoch [289], train_loss: 0.0000, val_loss: 7.7411, val_acc: 0.5880\n",
      "Epoch [290], train_loss: 0.0153, val_loss: 6.8337, val_acc: 0.5448\n",
      "Epoch [291], train_loss: 0.2577, val_loss: 6.9567, val_acc: 0.5623\n",
      "Epoch [292], train_loss: 0.7846, val_loss: 1.5385, val_acc: 0.5434\n",
      "Epoch [293], train_loss: 0.1695, val_loss: 2.5749, val_acc: 0.5301\n",
      "Epoch [294], train_loss: 0.0406, val_loss: 4.2840, val_acc: 0.5350\n",
      "Epoch [295], train_loss: 0.0205, val_loss: 5.2914, val_acc: 0.5431\n",
      "Epoch [296], train_loss: 0.0100, val_loss: 5.6464, val_acc: 0.5220\n",
      "Epoch [297], train_loss: 0.0015, val_loss: 6.6532, val_acc: 0.5272\n",
      "Epoch [298], train_loss: 0.0006, val_loss: 7.0847, val_acc: 0.5353\n",
      "Epoch [299], train_loss: 0.0003, val_loss: 7.2048, val_acc: 0.5298\n",
      "Epoch [300], train_loss: 0.0005, val_loss: 7.8438, val_acc: 0.5269\n",
      "Epoch [301], train_loss: 0.0001, val_loss: 8.1009, val_acc: 0.5246\n",
      "Epoch [302], train_loss: 0.0002, val_loss: 8.2066, val_acc: 0.5376\n",
      "Epoch [303], train_loss: 0.0019, val_loss: 8.0464, val_acc: 0.5269\n",
      "Epoch [304], train_loss: 0.1297, val_loss: 2.9756, val_acc: 0.5480\n",
      "Epoch [305], train_loss: 0.0316, val_loss: 4.0014, val_acc: 0.5509\n",
      "Epoch [306], train_loss: 0.0300, val_loss: 4.5943, val_acc: 0.5344\n",
      "Epoch [307], train_loss: 0.0029, val_loss: 5.2649, val_acc: 0.5477\n",
      "Epoch [308], train_loss: 0.0010, val_loss: 6.0072, val_acc: 0.5535\n",
      "Epoch [309], train_loss: 0.0004, val_loss: 6.6052, val_acc: 0.5428\n",
      "Epoch [310], train_loss: 0.0001, val_loss: 6.9738, val_acc: 0.5532\n",
      "Epoch [311], train_loss: 0.0001, val_loss: 7.3668, val_acc: 0.5425\n",
      "Epoch [312], train_loss: 0.0000, val_loss: 7.5361, val_acc: 0.5396\n",
      "Epoch [313], train_loss: 0.0000, val_loss: 7.6474, val_acc: 0.5347\n",
      "Epoch [314], train_loss: 0.0002, val_loss: 7.6784, val_acc: 0.5399\n",
      "Epoch [315], train_loss: 0.0000, val_loss: 7.7535, val_acc: 0.5373\n",
      "Epoch [316], train_loss: 0.0000, val_loss: 7.8657, val_acc: 0.5265\n",
      "Epoch [317], train_loss: 0.0000, val_loss: 7.9850, val_acc: 0.5373\n",
      "Epoch [318], train_loss: 0.0000, val_loss: 7.9804, val_acc: 0.5483\n",
      "Epoch [319], train_loss: 0.0000, val_loss: 8.0409, val_acc: 0.5269\n",
      "Epoch [320], train_loss: 0.0000, val_loss: 8.0595, val_acc: 0.5506\n",
      "Epoch [321], train_loss: 0.0000, val_loss: 8.0948, val_acc: 0.5477\n",
      "Epoch [322], train_loss: 0.0000, val_loss: 8.1332, val_acc: 0.5451\n",
      "Epoch [323], train_loss: 0.0000, val_loss: 8.2238, val_acc: 0.5399\n",
      "Epoch [324], train_loss: 0.0000, val_loss: 8.3018, val_acc: 0.5321\n",
      "Epoch [325], train_loss: 0.0000, val_loss: 8.3507, val_acc: 0.5321\n",
      "Epoch [326], train_loss: 0.0000, val_loss: 8.3296, val_acc: 0.5347\n",
      "Epoch [327], train_loss: 0.0000, val_loss: 8.3747, val_acc: 0.5428\n",
      "Epoch [328], train_loss: 0.0000, val_loss: 8.4244, val_acc: 0.5428\n",
      "Epoch [329], train_loss: 0.0000, val_loss: 8.4457, val_acc: 0.5480\n",
      "Epoch [330], train_loss: 0.0000, val_loss: 8.5253, val_acc: 0.5454\n",
      "Epoch [331], train_loss: 0.0003, val_loss: 8.5344, val_acc: 0.5373\n",
      "Epoch [332], train_loss: 0.0016, val_loss: 7.9479, val_acc: 0.5376\n",
      "Epoch [333], train_loss: 0.0042, val_loss: 6.4206, val_acc: 0.5581\n",
      "Epoch [334], train_loss: 0.0239, val_loss: 4.7441, val_acc: 0.5672\n",
      "Epoch [335], train_loss: 0.0318, val_loss: 4.1114, val_acc: 0.5490\n",
      "Epoch [336], train_loss: 0.0088, val_loss: 4.7494, val_acc: 0.5431\n",
      "Epoch [337], train_loss: 0.0045, val_loss: 5.3657, val_acc: 0.5594\n",
      "Epoch [338], train_loss: 0.0002, val_loss: 6.2402, val_acc: 0.5457\n",
      "Epoch [339], train_loss: 0.0001, val_loss: 6.5211, val_acc: 0.5402\n",
      "Epoch [340], train_loss: 0.0000, val_loss: 6.9316, val_acc: 0.5324\n",
      "Epoch [341], train_loss: 0.0000, val_loss: 7.0579, val_acc: 0.5298\n",
      "Epoch [342], train_loss: 0.0000, val_loss: 7.1463, val_acc: 0.5298\n",
      "Epoch [343], train_loss: 0.0000, val_loss: 7.3129, val_acc: 0.5350\n",
      "Epoch [344], train_loss: 0.0000, val_loss: 7.2923, val_acc: 0.5350\n",
      "Epoch [345], train_loss: 0.0000, val_loss: 7.3491, val_acc: 0.5324\n",
      "Epoch [346], train_loss: 0.0000, val_loss: 7.4896, val_acc: 0.5350\n",
      "Epoch [347], train_loss: 0.0000, val_loss: 7.4665, val_acc: 0.5324\n",
      "Epoch [348], train_loss: 0.0000, val_loss: 7.6418, val_acc: 0.5350\n",
      "Epoch [349], train_loss: 0.0000, val_loss: 7.6561, val_acc: 0.5350\n",
      "Epoch [350], train_loss: 0.0000, val_loss: 7.6326, val_acc: 0.5405\n",
      "Epoch [351], train_loss: 0.0000, val_loss: 7.6995, val_acc: 0.5350\n",
      "Epoch [352], train_loss: 0.0000, val_loss: 7.7427, val_acc: 0.5324\n",
      "Epoch [353], train_loss: 0.0000, val_loss: 7.7905, val_acc: 0.5324\n",
      "Epoch [354], train_loss: 0.0000, val_loss: 7.8010, val_acc: 0.5350\n",
      "Epoch [355], train_loss: 0.0000, val_loss: 7.8947, val_acc: 0.5324\n",
      "Epoch [356], train_loss: 0.0000, val_loss: 7.8820, val_acc: 0.5324\n",
      "Epoch [357], train_loss: 0.0000, val_loss: 7.9866, val_acc: 0.5324\n",
      "Epoch [358], train_loss: 0.0000, val_loss: 8.0044, val_acc: 0.5350\n",
      "Epoch [359], train_loss: 0.0000, val_loss: 7.9166, val_acc: 0.5405\n",
      "Epoch [360], train_loss: 0.0000, val_loss: 7.9842, val_acc: 0.5298\n",
      "Epoch [361], train_loss: 0.0000, val_loss: 8.2060, val_acc: 0.5324\n",
      "Epoch [362], train_loss: 0.0000, val_loss: 8.3307, val_acc: 0.5408\n",
      "Epoch [363], train_loss: 0.0000, val_loss: 8.2969, val_acc: 0.5434\n",
      "Epoch [364], train_loss: 0.0000, val_loss: 8.4256, val_acc: 0.5460\n",
      "Epoch [365], train_loss: 0.0000, val_loss: 8.4394, val_acc: 0.5460\n",
      "Epoch [366], train_loss: 0.0000, val_loss: 8.5415, val_acc: 0.5487\n",
      "Epoch [367], train_loss: 0.0000, val_loss: 8.3716, val_acc: 0.5434\n",
      "Epoch [368], train_loss: 0.0000, val_loss: 8.3586, val_acc: 0.5460\n",
      "Epoch [369], train_loss: 0.0000, val_loss: 8.4869, val_acc: 0.5434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370], train_loss: 0.0000, val_loss: 8.4841, val_acc: 0.5405\n",
      "Epoch [371], train_loss: 0.0000, val_loss: 8.5271, val_acc: 0.5434\n",
      "Epoch [372], train_loss: 0.0000, val_loss: 8.5678, val_acc: 0.5460\n",
      "Epoch [373], train_loss: 0.0000, val_loss: 8.6224, val_acc: 0.5460\n",
      "Epoch [374], train_loss: 0.0000, val_loss: 8.6827, val_acc: 0.5434\n",
      "Epoch [375], train_loss: 0.0000, val_loss: 8.5969, val_acc: 0.5434\n",
      "Epoch [376], train_loss: 0.0000, val_loss: 8.6679, val_acc: 0.5460\n",
      "Epoch [377], train_loss: 0.0000, val_loss: 8.6490, val_acc: 0.5434\n",
      "Epoch [378], train_loss: 0.0000, val_loss: 8.7261, val_acc: 0.5460\n",
      "Epoch [379], train_loss: 0.0000, val_loss: 8.7458, val_acc: 0.5460\n",
      "Epoch [380], train_loss: 0.0000, val_loss: 8.6858, val_acc: 0.5379\n",
      "Epoch [381], train_loss: 0.0000, val_loss: 8.7018, val_acc: 0.5379\n",
      "Epoch [382], train_loss: 0.0000, val_loss: 8.7398, val_acc: 0.5408\n",
      "Epoch [383], train_loss: 0.0000, val_loss: 8.8745, val_acc: 0.5434\n",
      "Epoch [384], train_loss: 0.0000, val_loss: 8.8479, val_acc: 0.5487\n",
      "Epoch [385], train_loss: 0.0000, val_loss: 8.7666, val_acc: 0.5487\n",
      "Epoch [386], train_loss: 0.0000, val_loss: 8.7488, val_acc: 0.5460\n",
      "Epoch [387], train_loss: 0.0000, val_loss: 8.8451, val_acc: 0.5460\n",
      "Epoch [388], train_loss: 0.0000, val_loss: 8.9702, val_acc: 0.5460\n",
      "Epoch [389], train_loss: 0.0000, val_loss: 8.9726, val_acc: 0.5434\n",
      "Epoch [390], train_loss: 0.0000, val_loss: 8.8736, val_acc: 0.5487\n",
      "Epoch [391], train_loss: 0.0000, val_loss: 8.9207, val_acc: 0.5434\n",
      "Epoch [392], train_loss: 0.0000, val_loss: 9.2219, val_acc: 0.5298\n",
      "Epoch [393], train_loss: 0.0000, val_loss: 9.2610, val_acc: 0.5324\n",
      "Epoch [394], train_loss: 0.0000, val_loss: 9.3189, val_acc: 0.5350\n",
      "Epoch [395], train_loss: 0.0000, val_loss: 9.3834, val_acc: 0.5324\n",
      "Epoch [396], train_loss: 0.0000, val_loss: 9.3916, val_acc: 0.5350\n",
      "Epoch [397], train_loss: 0.0000, val_loss: 9.4007, val_acc: 0.5379\n",
      "Epoch [398], train_loss: 0.0000, val_loss: 9.3025, val_acc: 0.5350\n",
      "Epoch [399], train_loss: 0.0000, val_loss: 9.5533, val_acc: 0.5324\n",
      "Epoch [400], train_loss: 0.0000, val_loss: 9.5849, val_acc: 0.5376\n",
      "Epoch [401], train_loss: 0.0000, val_loss: 9.6680, val_acc: 0.5324\n",
      "Epoch [402], train_loss: 0.0000, val_loss: 9.7901, val_acc: 0.5379\n",
      "Epoch [403], train_loss: 0.0000, val_loss: 9.7438, val_acc: 0.5405\n",
      "Epoch [404], train_loss: 0.0000, val_loss: 9.5518, val_acc: 0.5431\n",
      "Epoch [405], train_loss: 0.0000, val_loss: 9.6081, val_acc: 0.5324\n",
      "Epoch [406], train_loss: 0.0000, val_loss: 9.7505, val_acc: 0.5379\n",
      "Epoch [407], train_loss: 0.0000, val_loss: 9.5840, val_acc: 0.5457\n",
      "Epoch [408], train_loss: 0.0000, val_loss: 9.7038, val_acc: 0.5379\n",
      "Epoch [409], train_loss: 0.0000, val_loss: 9.7777, val_acc: 0.5379\n",
      "Epoch [410], train_loss: 0.0000, val_loss: 9.7770, val_acc: 0.5379\n",
      "Epoch [411], train_loss: 0.0000, val_loss: 9.8044, val_acc: 0.5379\n",
      "Epoch [412], train_loss: 0.0000, val_loss: 9.8459, val_acc: 0.5353\n",
      "Epoch [413], train_loss: 0.0000, val_loss: 9.8447, val_acc: 0.5298\n",
      "Epoch [414], train_loss: 0.0000, val_loss: 9.8696, val_acc: 0.5379\n",
      "Epoch [415], train_loss: 0.0000, val_loss: 9.8011, val_acc: 0.5405\n",
      "Epoch [416], train_loss: 0.0000, val_loss: 9.7358, val_acc: 0.5431\n",
      "Epoch [417], train_loss: 0.0000, val_loss: 9.9495, val_acc: 0.5353\n",
      "Epoch [418], train_loss: 0.0000, val_loss: 9.8792, val_acc: 0.5379\n",
      "Epoch [419], train_loss: 0.0000, val_loss: 10.0791, val_acc: 0.5379\n",
      "Epoch [420], train_loss: 0.0000, val_loss: 9.9796, val_acc: 0.5379\n",
      "Epoch [421], train_loss: 0.0000, val_loss: 10.0738, val_acc: 0.5379\n",
      "Epoch [422], train_loss: 0.0000, val_loss: 9.8729, val_acc: 0.5379\n",
      "Epoch [423], train_loss: 0.0000, val_loss: 10.1681, val_acc: 0.5405\n",
      "Epoch [424], train_loss: 0.0000, val_loss: 10.2597, val_acc: 0.5353\n",
      "Epoch [425], train_loss: 0.0000, val_loss: 10.2823, val_acc: 0.5379\n",
      "Epoch [426], train_loss: 0.0000, val_loss: 10.3633, val_acc: 0.5405\n",
      "Epoch [427], train_loss: 0.0000, val_loss: 10.1725, val_acc: 0.5379\n",
      "Epoch [428], train_loss: 0.0000, val_loss: 10.1771, val_acc: 0.5379\n",
      "Epoch [429], train_loss: 0.0000, val_loss: 10.1171, val_acc: 0.5405\n",
      "Epoch [430], train_loss: 0.0000, val_loss: 10.2495, val_acc: 0.5353\n",
      "Epoch [431], train_loss: 0.0000, val_loss: 10.1877, val_acc: 0.5405\n",
      "Epoch [432], train_loss: 0.0000, val_loss: 10.3980, val_acc: 0.5353\n",
      "Epoch [433], train_loss: 0.0000, val_loss: 10.3402, val_acc: 0.5379\n",
      "Epoch [434], train_loss: 0.0000, val_loss: 10.4400, val_acc: 0.5353\n",
      "Epoch [435], train_loss: 0.0000, val_loss: 10.5237, val_acc: 0.5405\n",
      "Epoch [436], train_loss: 0.0000, val_loss: 10.3999, val_acc: 0.5379\n",
      "Epoch [437], train_loss: 0.0000, val_loss: 10.2935, val_acc: 0.5379\n",
      "Epoch [438], train_loss: 0.0000, val_loss: 10.4397, val_acc: 0.5431\n",
      "Epoch [439], train_loss: 0.0000, val_loss: 10.4127, val_acc: 0.5379\n",
      "Epoch [440], train_loss: 0.0000, val_loss: 10.5287, val_acc: 0.5353\n",
      "Epoch [441], train_loss: 0.0000, val_loss: 10.2786, val_acc: 0.5376\n",
      "Epoch [442], train_loss: 0.0000, val_loss: 10.5537, val_acc: 0.5379\n",
      "Epoch [443], train_loss: 0.0000, val_loss: 10.3739, val_acc: 0.5353\n",
      "Epoch [444], train_loss: 0.0000, val_loss: 10.5186, val_acc: 0.5353\n",
      "Epoch [445], train_loss: 0.0000, val_loss: 10.5353, val_acc: 0.5327\n",
      "Epoch [446], train_loss: 0.0000, val_loss: 10.5352, val_acc: 0.5353\n",
      "Epoch [447], train_loss: 0.0000, val_loss: 10.5520, val_acc: 0.5379\n",
      "Epoch [448], train_loss: 0.0000, val_loss: 10.5403, val_acc: 0.5405\n",
      "Epoch [449], train_loss: 0.0000, val_loss: 10.5637, val_acc: 0.5379\n",
      "Epoch [450], train_loss: 0.0000, val_loss: 10.4433, val_acc: 0.5457\n",
      "Epoch [451], train_loss: 0.0000, val_loss: 10.7188, val_acc: 0.5353\n",
      "Epoch [452], train_loss: 0.0000, val_loss: 10.6766, val_acc: 0.5353\n",
      "Epoch [453], train_loss: 0.0000, val_loss: 10.6036, val_acc: 0.5379\n",
      "Epoch [454], train_loss: 0.0000, val_loss: 10.7214, val_acc: 0.5353\n",
      "Epoch [455], train_loss: 0.0000, val_loss: 10.5154, val_acc: 0.5353\n",
      "Epoch [456], train_loss: 0.0000, val_loss: 10.7793, val_acc: 0.5353\n",
      "Epoch [457], train_loss: 0.0000, val_loss: 10.7860, val_acc: 0.5353\n",
      "Epoch [458], train_loss: 0.0000, val_loss: 10.5976, val_acc: 0.5457\n",
      "Epoch [459], train_loss: 0.0000, val_loss: 10.6625, val_acc: 0.5457\n",
      "Epoch [460], train_loss: 0.0000, val_loss: 10.7046, val_acc: 0.5431\n",
      "Epoch [461], train_loss: 0.0000, val_loss: 10.8897, val_acc: 0.5353\n",
      "Epoch [462], train_loss: 0.0000, val_loss: 10.7914, val_acc: 0.5405\n",
      "Epoch [463], train_loss: 0.0000, val_loss: 10.8619, val_acc: 0.5405\n",
      "Epoch [464], train_loss: 0.0000, val_loss: 10.5932, val_acc: 0.5457\n",
      "Epoch [465], train_loss: 0.0000, val_loss: 10.8189, val_acc: 0.5353\n",
      "Epoch [466], train_loss: 0.0000, val_loss: 10.7887, val_acc: 0.5405\n",
      "Epoch [467], train_loss: 0.0000, val_loss: 10.6263, val_acc: 0.5483\n",
      "Epoch [468], train_loss: 0.0000, val_loss: 10.7195, val_acc: 0.5431\n",
      "Epoch [469], train_loss: 0.0000, val_loss: 10.7176, val_acc: 0.5431\n",
      "Epoch [470], train_loss: 0.0000, val_loss: 10.9588, val_acc: 0.5405\n",
      "Epoch [471], train_loss: 0.0000, val_loss: 11.0120, val_acc: 0.5405\n",
      "Epoch [472], train_loss: 0.0000, val_loss: 10.7354, val_acc: 0.5457\n",
      "Epoch [473], train_loss: 0.0000, val_loss: 10.7975, val_acc: 0.5379\n",
      "Epoch [474], train_loss: 0.0000, val_loss: 10.9706, val_acc: 0.5405\n",
      "Epoch [475], train_loss: 0.0000, val_loss: 10.8630, val_acc: 0.5379\n",
      "Epoch [476], train_loss: 0.0000, val_loss: 10.7301, val_acc: 0.5402\n",
      "Epoch [477], train_loss: 0.0000, val_loss: 11.0907, val_acc: 0.5379\n",
      "Epoch [478], train_loss: 0.0000, val_loss: 10.9569, val_acc: 0.5379\n",
      "Epoch [479], train_loss: 0.0000, val_loss: 11.0498, val_acc: 0.5353\n",
      "Epoch [480], train_loss: 0.0000, val_loss: 10.9578, val_acc: 0.5379\n",
      "Epoch [481], train_loss: 0.0000, val_loss: 10.7748, val_acc: 0.5457\n",
      "Epoch [482], train_loss: 0.0000, val_loss: 11.0645, val_acc: 0.5353\n",
      "Epoch [483], train_loss: 0.0000, val_loss: 10.7338, val_acc: 0.5402\n",
      "Epoch [484], train_loss: 0.0000, val_loss: 10.9241, val_acc: 0.5379\n",
      "Epoch [485], train_loss: 0.0000, val_loss: 10.9215, val_acc: 0.5405\n",
      "Epoch [486], train_loss: 0.0000, val_loss: 10.9452, val_acc: 0.5431\n",
      "Epoch [487], train_loss: 0.0000, val_loss: 11.0777, val_acc: 0.5379\n",
      "Epoch [488], train_loss: 0.0000, val_loss: 10.9974, val_acc: 0.5405\n",
      "Epoch [489], train_loss: 0.0000, val_loss: 11.0915, val_acc: 0.5405\n",
      "Epoch [490], train_loss: 0.0000, val_loss: 11.1397, val_acc: 0.5353\n",
      "Epoch [491], train_loss: 0.0000, val_loss: 10.9614, val_acc: 0.5431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [492], train_loss: 0.0000, val_loss: 11.0377, val_acc: 0.5405\n",
      "Epoch [493], train_loss: 0.0000, val_loss: 11.0565, val_acc: 0.5379\n",
      "Epoch [494], train_loss: 0.0000, val_loss: 11.1914, val_acc: 0.5405\n",
      "Epoch [495], train_loss: 0.0000, val_loss: 11.1693, val_acc: 0.5379\n",
      "Epoch [496], train_loss: 0.0000, val_loss: 11.1649, val_acc: 0.5379\n",
      "Epoch [497], train_loss: 0.0000, val_loss: 11.1341, val_acc: 0.5405\n",
      "Epoch [498], train_loss: 0.0000, val_loss: 11.1509, val_acc: 0.5379\n",
      "Epoch [499], train_loss: 0.0000, val_loss: 11.1881, val_acc: 0.5353\n",
      "Epoch [500], train_loss: 0.0000, val_loss: 11.1833, val_acc: 0.5431\n",
      "Epoch [501], train_loss: 0.0000, val_loss: 11.1419, val_acc: 0.5405\n",
      "Epoch [502], train_loss: 0.0000, val_loss: 11.1817, val_acc: 0.5379\n",
      "Epoch [503], train_loss: 0.0000, val_loss: 11.2564, val_acc: 0.5379\n",
      "Epoch [504], train_loss: 0.0000, val_loss: 11.2027, val_acc: 0.5405\n",
      "Epoch [505], train_loss: 0.0000, val_loss: 11.2618, val_acc: 0.5379\n",
      "Epoch [506], train_loss: 0.0000, val_loss: 11.0930, val_acc: 0.5379\n",
      "Epoch [507], train_loss: 0.0000, val_loss: 11.2231, val_acc: 0.5405\n",
      "Epoch [508], train_loss: 0.0000, val_loss: 11.3504, val_acc: 0.5405\n",
      "Epoch [509], train_loss: 0.0000, val_loss: 11.3478, val_acc: 0.5379\n",
      "Epoch [510], train_loss: 0.0000, val_loss: 11.2566, val_acc: 0.5405\n",
      "Epoch [511], train_loss: 0.0000, val_loss: 11.2636, val_acc: 0.5379\n",
      "Epoch [512], train_loss: 0.0004, val_loss: 12.8182, val_acc: 0.5324\n",
      "Epoch [513], train_loss: 0.3511, val_loss: 1.1645, val_acc: 0.5985\n",
      "Epoch [514], train_loss: 0.1700, val_loss: 1.6859, val_acc: 0.5783\n",
      "Epoch [515], train_loss: 0.0779, val_loss: 2.5284, val_acc: 0.5890\n",
      "Epoch [516], train_loss: 0.0590, val_loss: 2.1274, val_acc: 0.5269\n",
      "Epoch [517], train_loss: 0.0415, val_loss: 3.1441, val_acc: 0.5086\n",
      "Epoch [518], train_loss: 0.0128, val_loss: 4.7503, val_acc: 0.5672\n",
      "Epoch [519], train_loss: 0.0230, val_loss: 3.5987, val_acc: 0.5678\n",
      "Epoch [520], train_loss: 0.0104, val_loss: 4.5040, val_acc: 0.5408\n",
      "Epoch [521], train_loss: 0.0283, val_loss: 2.3370, val_acc: 0.5698\n",
      "Epoch [522], train_loss: 0.0144, val_loss: 3.2387, val_acc: 0.5513\n",
      "Epoch [523], train_loss: 0.0025, val_loss: 4.8480, val_acc: 0.5594\n",
      "Epoch [524], train_loss: 0.0002, val_loss: 5.5066, val_acc: 0.5487\n",
      "Epoch [525], train_loss: 0.0001, val_loss: 5.8401, val_acc: 0.5487\n",
      "Epoch [526], train_loss: 0.0000, val_loss: 6.0129, val_acc: 0.5434\n",
      "Epoch [527], train_loss: 0.0000, val_loss: 6.0663, val_acc: 0.5487\n",
      "Epoch [528], train_loss: 0.0000, val_loss: 6.2412, val_acc: 0.5434\n",
      "Epoch [529], train_loss: 0.0000, val_loss: 6.3370, val_acc: 0.5434\n",
      "Epoch [530], train_loss: 0.0000, val_loss: 6.4032, val_acc: 0.5434\n",
      "Epoch [531], train_loss: 0.0000, val_loss: 6.4742, val_acc: 0.5513\n",
      "Epoch [532], train_loss: 0.0000, val_loss: 6.5515, val_acc: 0.5434\n",
      "Epoch [533], train_loss: 0.0000, val_loss: 6.6155, val_acc: 0.5487\n",
      "Epoch [534], train_loss: 0.0000, val_loss: 6.6781, val_acc: 0.5434\n",
      "Epoch [535], train_loss: 0.0000, val_loss: 6.7526, val_acc: 0.5460\n",
      "Epoch [536], train_loss: 0.0000, val_loss: 6.7788, val_acc: 0.5434\n",
      "Epoch [537], train_loss: 0.0000, val_loss: 6.8223, val_acc: 0.5434\n",
      "Epoch [538], train_loss: 0.0000, val_loss: 6.8926, val_acc: 0.5434\n",
      "Epoch [539], train_loss: 0.0000, val_loss: 6.8781, val_acc: 0.5487\n",
      "Epoch [540], train_loss: 0.0000, val_loss: 6.9964, val_acc: 0.5434\n",
      "Epoch [541], train_loss: 0.0000, val_loss: 6.9556, val_acc: 0.5460\n",
      "Epoch [542], train_loss: 0.0000, val_loss: 7.1381, val_acc: 0.5434\n",
      "Epoch [543], train_loss: 0.0000, val_loss: 7.1351, val_acc: 0.5434\n",
      "Epoch [544], train_loss: 0.0000, val_loss: 7.2022, val_acc: 0.5434\n",
      "Epoch [545], train_loss: 0.0000, val_loss: 7.2270, val_acc: 0.5434\n",
      "Epoch [546], train_loss: 0.0000, val_loss: 7.2832, val_acc: 0.5434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9924/375910010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#fitting the model on training data and record the result after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9924/1190722326.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    215\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CTX = torch.device('cuda')\n",
    "# train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # train_dl.train_labels.to(CTX)\n",
    "# # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # val_dl.train_labels.to(CTX)\n",
    "num_epochs = 5000\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26874318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "    \n",
    "\n",
    "plot_accuracies(history)\n",
    "\n",
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([x['val_acc'] for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3164b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
