{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a9294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import python_speech_features as ps\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ac5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defs\n",
    "\n",
    "epsilon = 1e-5\n",
    "\n",
    "def wgn(x, snr):\n",
    "    snr = 10**(snr/10.0)\n",
    "    xpower = np.sum(x**2)/len(x)\n",
    "    npower = xpower / snr\n",
    "    return np.random.randn(len(x)) * np.sqrt(npower)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = wave.open(filename,'r')    \n",
    "    params = file.getparams()\n",
    "    nchannels, sampwidth, framerate, wav_length = params[:4]\n",
    "    str_data = file.readframes(wav_length)\n",
    "    wavedata = np.fromstring(str_data, dtype = np.short)\n",
    "    # librosa.load(wav_file_path + orig_wav_file, sr=sr)\n",
    "    time = np.arange(0,wav_length) * (1.0/framerate)\n",
    "    file.close()\n",
    "    return wavedata, time, framerate\n",
    "\n",
    "def generate_label(control):\n",
    "    label = 0\n",
    "    if(control == 'cc'):\n",
    "        label = 0\n",
    "    elif(control == 'cd'):\n",
    "        label = 1\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3e7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd: 1358\n",
    "# cc: 1476\n",
    "\n",
    "epsilon = 1e-5\n",
    "\n",
    "filter_num = 40\n",
    "\n",
    "rootdir = 'train/Normalised_audio-chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81055c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1,std1,mean2,std2,mean3,std3 = 0,1,0,1,0,1\n",
    "ccnum = 1358 #0\n",
    "cdnum = 1476 #1\n",
    "# pernum = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8b413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = np.empty((train_num,1), dtype = np.int8)\n",
    "# test_label = np.empty((test_num,1), dtype = np.int8)\n",
    "# valid_label = np.empty((valid_num,1), dtype = np.int8)\n",
    "\n",
    "# train_data = np.empty((train_num,300,filter_num,3),dtype = np.float32)\n",
    "# test_data = np.empty((test_num,300,filter_num,3),dtype = np.float32)\n",
    "# valid_data = np.empty((valid_num,300,filter_num,3),dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb2796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example shape  (8000,) Sample rate  22050 Data type <class 'numpy.ndarray'>\n",
      "[-3.4325361e-01 -5.6174737e-01 -5.7129568e-01 ...  1.3424507e-04\n",
      "  6.5730209e-04  1.2089183e-03]\n",
      "(128, 15)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rootdir = 'train/Normalised_audio-chunks'\n",
    "control = 'cc'\n",
    "sub_dir = rootdir + '/' + control\n",
    "\n",
    "for sample in os.listdir(sub_dir):\n",
    "    data, rate = librosa.load(sub_dir+'/'+sample)\n",
    "#     time = data.shape[0]\n",
    "   \n",
    "    sgram = librosa.stft(data)\n",
    "    data = librosa.util.fix_length(data, size=8000, mode='symmetric')\n",
    "    print ('Example shape ', data.shape, 'Sample rate ', rate, 'Data type', type(data))\n",
    "    print (data)\n",
    "    # use the mel-scale instead of raw frequency\n",
    "    sgram_mag, _ = librosa.magphase(sgram)\n",
    "    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=rate)\n",
    "    \n",
    "    mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
    "    print(mel_sgram.shape)\n",
    "#     librosa.display.specshow(mel_sgram, sr=rate, x_axis='time', y_axis='mel')\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be8b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_duration = 0\n",
    "\n",
    "# rootdir = 'train/Normalised_audio-chunks'\n",
    "# for control in os.listdir(rootdir):\n",
    "\n",
    "#     sub_dir = rootdir + '/' + control\n",
    "\n",
    "#     for sample in os.listdir(sub_dir):\n",
    "#         data, rate = librosa.load(sub_dir+'/'+sample)\n",
    "#         length = data.shape[0]\n",
    "#         if max_duration <= length:\n",
    "#             max_duration = length\n",
    "#         print(max_duration//rate)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2919d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Prepare training data from Metadata file\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "# # from pathlib import Path\n",
    "\n",
    "# download_path = Path.cwd()/'UrbanSound8K'\n",
    "\n",
    "# Read metadata file\n",
    "# metadata_file = download_path/'metadata'/'UrbanSound8K.csv'\n",
    "# df = pd.read_csv(metadata_file)\n",
    "# df.head()\n",
    "\n",
    "# # Construct file path by concatenating fold and file name\n",
    "# df['relative_path'] = '/fold' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "\n",
    "# # Take relevant columns\n",
    "# df = df[['relative_path', 'classID']]\n",
    "# df.head()\n",
    "rootdir = 'train/Normalised_audio-chunks'\n",
    "\n",
    "data = []\n",
    "for control in os.listdir(rootdir):\n",
    "    Class = 0\n",
    "    if control == 'cc':\n",
    "        Class = 0\n",
    "    else:\n",
    "        Class = 1\n",
    "        \n",
    "    sub_dir = rootdir + '/' + control\n",
    "\n",
    "    for sample in os.listdir(sub_dir):\n",
    "        data.append([control + '/' + sample,Class])\n",
    "#         print(sub_dir + '/' + sample,Class)\n",
    "#         print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae332e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc/S001-7-13310-20608-1-0-340.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc/S001-7-13310-20608-2-2680-3630.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cc/S001-7-13310-20608-3-4160-4880.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cc/S001-7-13310-20608-4-6660-7300.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cc/S001-7-20608-27071-1-320-980.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           relative_path  classID\n",
       "0      cc/S001-7-13310-20608-1-0-340.wav        0\n",
       "1  cc/S001-7-13310-20608-2-2680-3630.wav        0\n",
       "2  cc/S001-7-13310-20608-3-4160-4880.wav        0\n",
       "3  cc/S001-7-13310-20608-4-6660-7300.wav        0\n",
       "4    cc/S001-7-20608-27071-1-320-980.wav        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = ['relative_path', 'classID'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1a9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "\n",
    "CTX = torch.device('cuda')\n",
    "\n",
    "class AudioUtil():\n",
    "  # ----------------------------\n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "# ----------------------------\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "# ----------------------------\n",
    "  # Since Resample applies to a single channel, we resample one channel at a time\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "\n",
    " # ----------------------------\n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "#     print(sig_len)\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "\n",
    "# ----------------------------\n",
    "  # Shifts the signal to the left or right by some percent. Values at the end\n",
    "  # are 'wrapped around' to the start of the transformed signal.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "# ----------------------------\n",
    "  # Generate a Spectrogram\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    " # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec.to(torch.device('cuda'))\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "    # Get the Class ID\n",
    "    class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same \n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "data_path = 'train/Normalised_audio-chunks/'\n",
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fe2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0daa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda()) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.to(CTX))                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels.to(torch.int64).cuda())   # Calculate loss\n",
    "        acc = accuracy(out, labels.to(torch.int64).cuda())           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "        \n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "#         print(x.shape())\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.SpatialGate = SpatialGate()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        M_c = self.ChannelGate(x)\n",
    "        M_s = self.SpatialGate(x)\n",
    "        \n",
    "        M_f = self.sigmoid(M_c + M_s)\n",
    "        \n",
    "        \n",
    "        return x + x*M_f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a39f5a",
   "metadata": {},
   "source": [
    "* The audio from the file gets loaded into a Numpy array of shape (num_channels, num_samples). Most of the audio is sampled at 44.1kHz and is about 4 seconds in duration, resulting in 44,100 * 4 = 176,400 samples. If the audio has 1 channel, the shape of the array will be (1, 176,400). Similarly, audio of 4 seconds duration with 2 channels and sampled at 48kHz will have 192,000 samples and a shape of (2, 192,000).\n",
    "* Since the channels and sampling rates of each audio are different, the next two transforms resample the audio to a standard 44.1kHz and to a standard 2 channels.\n",
    "* Since some audio clips might be more or less than 4 seconds, we also standardize the audio duration to a fixed length of 4 seconds. Now arrays for all items have the same shape of (2, 176,400)\n",
    "* The Time Shift data augmentation now randomly shifts each audio sample forward or backward. The shapes are unchanged.\n",
    "* The augmented audio is now converted into a Mel Spectrogram, resulting in a shape of (num_channels, Mel freq_bands, time_steps) = (2, 64, 344)\n",
    "* The SpecAugment data augmentation now randomly applies Time and Frequency Masks to the Mel Spectrograms. The shapes are unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e75de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_Net(ImageClassificationBase):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "        #The LW_CNN module utilizes three convolutions (C), two max-pooling\n",
    "        # (MP), one average-pooling (AP), and one batch normalization\n",
    "        # (BN) layer.\n",
    "        self.LW_CNN = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "#             Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             BatchNorm2d(256),\n",
    "#             ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "#             # Defining another 2D convolution layer\n",
    "#             Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             BatchNorm2d(128),\n",
    "#             ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=1, stride=1),\n",
    "            \n",
    "              #C1, we used 120 number of kernels with size (11\n",
    "              # × 11) using (4 × 4) stride setting without padding to extract\n",
    "              # initially hidden patterns from input data. \n",
    "              Conv2d(2, 120, kernel_size=(11,11), stride=(4,4), padding=0),\n",
    "              MaxPool2d(kernel_size=(3,3), stride=None),\n",
    "              Conv2d(120, 256, kernel_size=(5,5), stride=(1,1), padding='same'),\n",
    "              MaxPool2d(kernel_size=(3,3)),\n",
    "              Conv2d(256, 384, kernel_size=(3,3), padding='same'),\n",
    "#               MaxPool2d(kernel_size=1, stride=0),\n",
    "#               Conv2d(128, 1, kernel_size=(3,3), stride=(1,1), padding='same'),\n",
    "              torch.nn.AvgPool2d(kernel_size=(1,1)),\n",
    "              BatchNorm2d(384),\n",
    "              ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(384 * 9 * 1, 256),\n",
    "            Linear(256, 64),\n",
    "            Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.attention = SAM(gate_channels=384)\n",
    "        \n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "#         x = x.view(-1, x.size(3),x.size(2),x.size(1))\n",
    "#         print(x.size)\n",
    "        x = self.LW_CNN(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab343ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (LW_CNN): Sequential(\n",
      "    (0): Conv2d(2, 120, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(120, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "    (3): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (5): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
      "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=3456, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention): SAM(\n",
      "    (ChannelGate): ChannelGate(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten()\n",
      "        (1): Linear(in_features=384, out_features=24, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=24, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (SpatialGate): SpatialGate(\n",
      "      (compress): ChannelPool()\n",
      "      (spatial): BasicConv(\n",
      "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac29ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 120, 14, 84]          29,160\n",
      "         MaxPool2d-2           [-1, 120, 4, 28]               0\n",
      "            Conv2d-3           [-1, 256, 4, 28]         768,256\n",
      "         MaxPool2d-4            [-1, 256, 1, 9]               0\n",
      "            Conv2d-5            [-1, 384, 1, 9]         885,120\n",
      "         AvgPool2d-6            [-1, 384, 1, 9]               0\n",
      "       BatchNorm2d-7            [-1, 384, 1, 9]             768\n",
      "              ReLU-8            [-1, 384, 1, 9]               0\n",
      "           Flatten-9                  [-1, 384]               0\n",
      "           Linear-10                   [-1, 24]           9,240\n",
      "             ReLU-11                   [-1, 24]               0\n",
      "           Linear-12                  [-1, 384]           9,600\n",
      "          Flatten-13                  [-1, 384]               0\n",
      "           Linear-14                   [-1, 24]           9,240\n",
      "             ReLU-15                   [-1, 24]               0\n",
      "           Linear-16                  [-1, 384]           9,600\n",
      "      ChannelGate-17            [-1, 384, 1, 9]               0\n",
      "      ChannelPool-18              [-1, 2, 1, 9]               0\n",
      "           Conv2d-19              [-1, 1, 1, 9]              98\n",
      "      BatchNorm2d-20              [-1, 1, 1, 9]               2\n",
      "        BasicConv-21              [-1, 1, 1, 9]               0\n",
      "      SpatialGate-22            [-1, 384, 1, 9]               0\n",
      "          Sigmoid-23            [-1, 384, 1, 9]               0\n",
      "              SAM-24            [-1, 384, 1, 9]               0\n",
      "           Linear-25                  [-1, 256]         884,992\n",
      "           Linear-26                   [-1, 64]          16,448\n",
      "           Linear-27                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 2,622,654\n",
      "Trainable params: 2,622,654\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.17\n",
      "Forward/backward pass size (MB): 1.64\n",
      "Params size (MB): 10.00\n",
      "Estimated Total Size (MB): 11.81\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (2, 64, 344))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e3c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ff1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.5050, val_loss: 0.5789, val_acc: 0.6962\n",
      "Epoch [1], train_loss: 0.5005, val_loss: 0.6176, val_acc: 0.6962\n",
      "Epoch [2], train_loss: 0.4870, val_loss: 0.6128, val_acc: 0.6587\n",
      "Epoch [3], train_loss: 0.5025, val_loss: 0.6164, val_acc: 0.6731\n",
      "Epoch [4], train_loss: 0.5005, val_loss: 0.7653, val_acc: 0.6396\n",
      "Epoch [5], train_loss: 0.4849, val_loss: 0.6332, val_acc: 0.6783\n",
      "Epoch [6], train_loss: 0.4867, val_loss: 0.6581, val_acc: 0.6466\n",
      "Epoch [7], train_loss: 0.4892, val_loss: 0.8400, val_acc: 0.6158\n",
      "Epoch [8], train_loss: 0.4850, val_loss: 0.8196, val_acc: 0.6049\n",
      "Epoch [9], train_loss: 0.4763, val_loss: 0.6602, val_acc: 0.6570\n",
      "Epoch [10], train_loss: 0.4784, val_loss: 0.6563, val_acc: 0.6466\n",
      "Epoch [11], train_loss: 0.4766, val_loss: 0.6624, val_acc: 0.6200\n",
      "Epoch [12], train_loss: 0.4653, val_loss: 0.6065, val_acc: 0.6887\n",
      "Epoch [13], train_loss: 0.4735, val_loss: 0.6315, val_acc: 0.6592\n",
      "Epoch [14], train_loss: 0.4720, val_loss: 0.6533, val_acc: 0.6887\n",
      "Epoch [15], train_loss: 0.4711, val_loss: 0.7987, val_acc: 0.6141\n",
      "Epoch [16], train_loss: 0.4655, val_loss: 0.6255, val_acc: 0.6848\n",
      "Epoch [17], train_loss: 0.4746, val_loss: 0.8365, val_acc: 0.5836\n",
      "Epoch [18], train_loss: 0.4702, val_loss: 0.6486, val_acc: 0.6518\n",
      "Epoch [19], train_loss: 0.4464, val_loss: 0.6479, val_acc: 0.6731\n",
      "Epoch [20], train_loss: 0.4755, val_loss: 0.6656, val_acc: 0.6587\n",
      "Epoch [21], train_loss: 0.4515, val_loss: 0.8317, val_acc: 0.6384\n",
      "Epoch [22], train_loss: 0.4608, val_loss: 0.6228, val_acc: 0.6731\n",
      "Epoch [23], train_loss: 0.4430, val_loss: 0.6090, val_acc: 0.7026\n",
      "Epoch [24], train_loss: 0.4576, val_loss: 0.6202, val_acc: 0.6627\n",
      "Epoch [25], train_loss: 0.4391, val_loss: 0.6252, val_acc: 0.6644\n",
      "Epoch [26], train_loss: 0.4550, val_loss: 0.8662, val_acc: 0.5732\n",
      "Epoch [27], train_loss: 0.4524, val_loss: 0.6406, val_acc: 0.6875\n",
      "Epoch [28], train_loss: 0.4429, val_loss: 0.6396, val_acc: 0.6610\n",
      "Epoch [29], train_loss: 0.4557, val_loss: 0.6521, val_acc: 0.6778\n",
      "Epoch [30], train_loss: 0.4420, val_loss: 0.6757, val_acc: 0.6426\n",
      "Epoch [31], train_loss: 0.4483, val_loss: 0.6358, val_acc: 0.6806\n",
      "Epoch [32], train_loss: 0.4397, val_loss: 0.6084, val_acc: 0.6900\n",
      "Epoch [33], train_loss: 0.4407, val_loss: 0.7529, val_acc: 0.6349\n",
      "Epoch [34], train_loss: 0.4203, val_loss: 0.7147, val_acc: 0.6448\n",
      "Epoch [35], train_loss: 0.4437, val_loss: 1.1053, val_acc: 0.5841\n",
      "Epoch [36], train_loss: 0.4258, val_loss: 0.6317, val_acc: 0.6610\n",
      "Epoch [37], train_loss: 0.4371, val_loss: 0.7218, val_acc: 0.6592\n",
      "Epoch [38], train_loss: 0.4298, val_loss: 0.6709, val_acc: 0.6892\n",
      "Epoch [39], train_loss: 0.4163, val_loss: 0.6392, val_acc: 0.6644\n",
      "Epoch [40], train_loss: 0.4165, val_loss: 0.6500, val_acc: 0.6667\n",
      "Epoch [41], train_loss: 0.4253, val_loss: 0.7794, val_acc: 0.6270\n",
      "Epoch [42], train_loss: 0.3982, val_loss: 0.7603, val_acc: 0.6362\n",
      "Epoch [43], train_loss: 0.4329, val_loss: 0.6906, val_acc: 0.6684\n",
      "Epoch [44], train_loss: 0.4252, val_loss: 0.6901, val_acc: 0.6731\n",
      "Epoch [45], train_loss: 0.4201, val_loss: 0.6596, val_acc: 0.6853\n",
      "Epoch [46], train_loss: 0.4149, val_loss: 0.8000, val_acc: 0.6476\n",
      "Epoch [47], train_loss: 0.4049, val_loss: 0.7292, val_acc: 0.7026\n",
      "Epoch [48], train_loss: 0.3950, val_loss: 0.6391, val_acc: 0.6825\n",
      "Epoch [49], train_loss: 0.4162, val_loss: 0.9552, val_acc: 0.6257\n",
      "Epoch [50], train_loss: 0.4245, val_loss: 0.6797, val_acc: 0.6632\n",
      "Epoch [51], train_loss: 0.3943, val_loss: 0.8129, val_acc: 0.6488\n",
      "Epoch [52], train_loss: 0.4073, val_loss: 0.7473, val_acc: 0.6731\n",
      "Epoch [53], train_loss: 0.3941, val_loss: 0.6420, val_acc: 0.6892\n",
      "Epoch [54], train_loss: 0.4142, val_loss: 0.7411, val_acc: 0.6505\n",
      "Epoch [55], train_loss: 0.4021, val_loss: 0.6786, val_acc: 0.6796\n",
      "Epoch [56], train_loss: 0.4087, val_loss: 0.7052, val_acc: 0.6858\n",
      "Epoch [57], train_loss: 0.4036, val_loss: 0.7457, val_acc: 0.6488\n",
      "Epoch [58], train_loss: 0.3853, val_loss: 0.7422, val_acc: 0.6726\n",
      "Epoch [59], train_loss: 0.3883, val_loss: 1.1180, val_acc: 0.6106\n",
      "Epoch [60], train_loss: 0.3764, val_loss: 0.6778, val_acc: 0.6887\n",
      "Epoch [61], train_loss: 0.3862, val_loss: 1.0384, val_acc: 0.5940\n",
      "Epoch [62], train_loss: 0.4026, val_loss: 0.7197, val_acc: 0.6605\n",
      "Epoch [63], train_loss: 0.4024, val_loss: 0.6743, val_acc: 0.6922\n",
      "Epoch [64], train_loss: 0.3761, val_loss: 0.7153, val_acc: 0.6448\n",
      "Epoch [65], train_loss: 0.4052, val_loss: 0.6124, val_acc: 0.6835\n",
      "Epoch [66], train_loss: 0.3825, val_loss: 0.7253, val_acc: 0.6535\n",
      "Epoch [67], train_loss: 0.3931, val_loss: 0.7096, val_acc: 0.6731\n",
      "Epoch [68], train_loss: 0.3794, val_loss: 0.7279, val_acc: 0.6778\n",
      "Epoch [69], train_loss: 0.3948, val_loss: 0.7616, val_acc: 0.6910\n",
      "Epoch [70], train_loss: 0.3976, val_loss: 0.7249, val_acc: 0.6761\n",
      "Epoch [71], train_loss: 0.3852, val_loss: 0.7069, val_acc: 0.6912\n",
      "Epoch [72], train_loss: 0.3750, val_loss: 0.7080, val_acc: 0.7049\n",
      "Epoch [73], train_loss: 0.3648, val_loss: 0.6412, val_acc: 0.6992\n",
      "Epoch [74], train_loss: 0.3742, val_loss: 0.7650, val_acc: 0.6823\n",
      "Epoch [75], train_loss: 0.3636, val_loss: 0.8728, val_acc: 0.6848\n",
      "Epoch [76], train_loss: 0.3707, val_loss: 0.8859, val_acc: 0.6639\n",
      "Epoch [77], train_loss: 0.3610, val_loss: 0.7135, val_acc: 0.7014\n",
      "Epoch [78], train_loss: 0.3682, val_loss: 1.1079, val_acc: 0.6084\n",
      "Epoch [79], train_loss: 0.3768, val_loss: 0.7597, val_acc: 0.6939\n",
      "Epoch [80], train_loss: 0.3630, val_loss: 1.1060, val_acc: 0.6032\n",
      "Epoch [81], train_loss: 0.3665, val_loss: 0.7525, val_acc: 0.6701\n",
      "Epoch [82], train_loss: 0.3754, val_loss: 1.0139, val_acc: 0.5880\n",
      "Epoch [83], train_loss: 0.3867, val_loss: 0.6932, val_acc: 0.6801\n",
      "Epoch [84], train_loss: 0.3468, val_loss: 1.9075, val_acc: 0.5367\n",
      "Epoch [85], train_loss: 0.3579, val_loss: 0.7744, val_acc: 0.6731\n",
      "Epoch [86], train_loss: 0.3935, val_loss: 0.9176, val_acc: 0.6436\n",
      "Epoch [87], train_loss: 0.3688, val_loss: 0.7814, val_acc: 0.6979\n",
      "Epoch [88], train_loss: 0.3657, val_loss: 0.9253, val_acc: 0.6131\n",
      "Epoch [89], train_loss: 0.3485, val_loss: 0.7885, val_acc: 0.6466\n",
      "Epoch [90], train_loss: 0.3723, val_loss: 0.6514, val_acc: 0.7153\n",
      "Epoch [91], train_loss: 0.3444, val_loss: 0.6930, val_acc: 0.7039\n",
      "Epoch [92], train_loss: 0.3588, val_loss: 0.7926, val_acc: 0.6674\n",
      "Epoch [93], train_loss: 0.3384, val_loss: 0.8261, val_acc: 0.6649\n",
      "Epoch [94], train_loss: 0.3526, val_loss: 0.8135, val_acc: 0.6969\n",
      "Epoch [95], train_loss: 0.3478, val_loss: 1.2159, val_acc: 0.5903\n",
      "Epoch [96], train_loss: 0.3616, val_loss: 0.6721, val_acc: 0.6714\n",
      "Epoch [97], train_loss: 0.3436, val_loss: 0.8157, val_acc: 0.6535\n",
      "Epoch [98], train_loss: 0.3416, val_loss: 0.8643, val_acc: 0.6275\n",
      "Epoch [99], train_loss: 0.3343, val_loss: 0.7923, val_acc: 0.7061\n",
      "Epoch [100], train_loss: 0.3383, val_loss: 1.0939, val_acc: 0.6205\n",
      "Epoch [101], train_loss: 0.3547, val_loss: 0.8294, val_acc: 0.6731\n",
      "Epoch [102], train_loss: 0.3357, val_loss: 0.8227, val_acc: 0.6448\n",
      "Epoch [103], train_loss: 0.3530, val_loss: 0.7087, val_acc: 0.6887\n",
      "Epoch [104], train_loss: 0.3532, val_loss: 0.8172, val_acc: 0.6771\n",
      "Epoch [105], train_loss: 0.3377, val_loss: 0.7280, val_acc: 0.6895\n",
      "Epoch [106], train_loss: 0.3361, val_loss: 0.9004, val_acc: 0.6518\n",
      "Epoch [107], train_loss: 0.3491, val_loss: 0.7414, val_acc: 0.6870\n",
      "Epoch [108], train_loss: 0.3397, val_loss: 0.7498, val_acc: 0.6801\n",
      "Epoch [109], train_loss: 0.3490, val_loss: 0.7112, val_acc: 0.6605\n",
      "Epoch [110], train_loss: 0.3419, val_loss: 0.7726, val_acc: 0.6944\n",
      "Epoch [111], train_loss: 0.3164, val_loss: 0.8356, val_acc: 0.6987\n",
      "Epoch [112], train_loss: 0.3488, val_loss: 0.8191, val_acc: 0.6696\n",
      "Epoch [113], train_loss: 0.3263, val_loss: 0.8946, val_acc: 0.6384\n",
      "Epoch [114], train_loss: 0.3308, val_loss: 0.7395, val_acc: 0.6823\n",
      "Epoch [115], train_loss: 0.3236, val_loss: 0.7590, val_acc: 0.6788\n",
      "Epoch [116], train_loss: 0.3423, val_loss: 0.7068, val_acc: 0.6858\n",
      "Epoch [117], train_loss: 0.3111, val_loss: 0.9739, val_acc: 0.6696\n",
      "Epoch [118], train_loss: 0.3348, val_loss: 1.0736, val_acc: 0.6401\n",
      "Epoch [119], train_loss: 0.3377, val_loss: 0.8433, val_acc: 0.6461\n",
      "Epoch [120], train_loss: 0.3428, val_loss: 0.7510, val_acc: 0.6905\n",
      "Epoch [121], train_loss: 0.3356, val_loss: 0.9244, val_acc: 0.6870\n",
      "Epoch [122], train_loss: 0.3281, val_loss: 0.7354, val_acc: 0.6644\n",
      "Epoch [123], train_loss: 0.3326, val_loss: 0.7517, val_acc: 0.6813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 0.3119, val_loss: 0.7803, val_acc: 0.6895\n",
      "Epoch [125], train_loss: 0.3187, val_loss: 0.8047, val_acc: 0.6813\n",
      "Epoch [126], train_loss: 0.3384, val_loss: 0.6805, val_acc: 0.6987\n",
      "Epoch [127], train_loss: 0.3160, val_loss: 0.7379, val_acc: 0.6662\n",
      "Epoch [128], train_loss: 0.3168, val_loss: 0.6783, val_acc: 0.6818\n",
      "Epoch [129], train_loss: 0.3204, val_loss: 0.7886, val_acc: 0.6709\n",
      "Epoch [130], train_loss: 0.3128, val_loss: 0.7890, val_acc: 0.6992\n",
      "Epoch [131], train_loss: 0.3312, val_loss: 0.7560, val_acc: 0.6843\n",
      "Epoch [132], train_loss: 0.3017, val_loss: 0.8385, val_acc: 0.6997\n",
      "Epoch [133], train_loss: 0.3321, val_loss: 0.7457, val_acc: 0.6818\n",
      "Epoch [134], train_loss: 0.3110, val_loss: 0.8847, val_acc: 0.6905\n",
      "Epoch [135], train_loss: 0.3163, val_loss: 0.7130, val_acc: 0.6813\n",
      "Epoch [136], train_loss: 0.3428, val_loss: 0.6615, val_acc: 0.7096\n",
      "Epoch [137], train_loss: 0.2981, val_loss: 0.9663, val_acc: 0.6488\n",
      "Epoch [138], train_loss: 0.3118, val_loss: 0.8527, val_acc: 0.6518\n",
      "Epoch [139], train_loss: 0.3051, val_loss: 0.8042, val_acc: 0.6749\n",
      "Epoch [140], train_loss: 0.3050, val_loss: 0.7733, val_acc: 0.7188\n",
      "Epoch [141], train_loss: 0.3007, val_loss: 0.8183, val_acc: 0.6835\n",
      "Epoch [142], train_loss: 0.3048, val_loss: 0.9537, val_acc: 0.6540\n",
      "Epoch [143], train_loss: 0.3306, val_loss: 0.7299, val_acc: 0.6622\n",
      "Epoch [144], train_loss: 0.2967, val_loss: 1.0294, val_acc: 0.6287\n",
      "Epoch [145], train_loss: 0.3167, val_loss: 0.7450, val_acc: 0.6927\n",
      "Epoch [146], train_loss: 0.3004, val_loss: 0.7773, val_acc: 0.6771\n",
      "Epoch [147], train_loss: 0.3005, val_loss: 1.0341, val_acc: 0.6553\n",
      "Epoch [148], train_loss: 0.3349, val_loss: 0.7904, val_acc: 0.6939\n",
      "Epoch [149], train_loss: 0.2967, val_loss: 1.0661, val_acc: 0.6600\n",
      "Epoch [150], train_loss: 0.3246, val_loss: 0.8341, val_acc: 0.6997\n",
      "Epoch [151], train_loss: 0.2901, val_loss: 0.8971, val_acc: 0.6691\n",
      "Epoch [152], train_loss: 0.2972, val_loss: 0.7188, val_acc: 0.6927\n",
      "Epoch [153], train_loss: 0.3151, val_loss: 0.7498, val_acc: 0.6649\n",
      "Epoch [154], train_loss: 0.3077, val_loss: 0.7559, val_acc: 0.6766\n",
      "Epoch [155], train_loss: 0.3027, val_loss: 0.7624, val_acc: 0.6912\n",
      "Epoch [156], train_loss: 0.2783, val_loss: 1.0392, val_acc: 0.6558\n",
      "Epoch [157], train_loss: 0.2958, val_loss: 1.1194, val_acc: 0.6032\n",
      "Epoch [158], train_loss: 0.2880, val_loss: 0.8830, val_acc: 0.6766\n",
      "Epoch [159], train_loss: 0.2959, val_loss: 0.8659, val_acc: 0.6714\n",
      "Epoch [160], train_loss: 0.2916, val_loss: 0.9042, val_acc: 0.6813\n",
      "Epoch [161], train_loss: 0.2925, val_loss: 0.8526, val_acc: 0.6570\n",
      "Epoch [162], train_loss: 0.2890, val_loss: 0.9875, val_acc: 0.6667\n",
      "Epoch [163], train_loss: 0.2988, val_loss: 0.7616, val_acc: 0.6778\n",
      "Epoch [164], train_loss: 0.3013, val_loss: 0.7266, val_acc: 0.6900\n",
      "Epoch [165], train_loss: 0.2920, val_loss: 0.8827, val_acc: 0.6910\n",
      "Epoch [166], train_loss: 0.3086, val_loss: 1.0119, val_acc: 0.6453\n",
      "Epoch [167], train_loss: 0.2753, val_loss: 0.9465, val_acc: 0.6431\n",
      "Epoch [168], train_loss: 0.2887, val_loss: 0.6999, val_acc: 0.7061\n",
      "Epoch [169], train_loss: 0.2873, val_loss: 0.7154, val_acc: 0.6905\n",
      "Epoch [170], train_loss: 0.2991, val_loss: 0.6977, val_acc: 0.7061\n",
      "Epoch [171], train_loss: 0.2924, val_loss: 0.8169, val_acc: 0.6652\n",
      "Epoch [172], train_loss: 0.2701, val_loss: 1.1282, val_acc: 0.6357\n",
      "Epoch [173], train_loss: 0.2713, val_loss: 1.1633, val_acc: 0.6424\n",
      "Epoch [174], train_loss: 0.2858, val_loss: 0.7922, val_acc: 0.6778\n",
      "Epoch [175], train_loss: 0.2908, val_loss: 0.9902, val_acc: 0.6287\n",
      "Epoch [176], train_loss: 0.2962, val_loss: 0.7324, val_acc: 0.6939\n",
      "Epoch [177], train_loss: 0.2718, val_loss: 0.7911, val_acc: 0.7051\n",
      "Epoch [178], train_loss: 0.2998, val_loss: 0.6619, val_acc: 0.6997\n",
      "Epoch [179], train_loss: 0.2881, val_loss: 0.8264, val_acc: 0.6910\n",
      "Epoch [180], train_loss: 0.2916, val_loss: 0.9906, val_acc: 0.6587\n",
      "Epoch [181], train_loss: 0.2627, val_loss: 0.7774, val_acc: 0.6528\n",
      "Epoch [182], train_loss: 0.2806, val_loss: 0.8457, val_acc: 0.6744\n",
      "Epoch [183], train_loss: 0.2815, val_loss: 1.0341, val_acc: 0.6171\n",
      "Epoch [184], train_loss: 0.2649, val_loss: 0.9145, val_acc: 0.6558\n",
      "Epoch [185], train_loss: 0.2708, val_loss: 0.7940, val_acc: 0.6974\n",
      "Epoch [186], train_loss: 0.2774, val_loss: 0.8206, val_acc: 0.6786\n",
      "Epoch [187], train_loss: 0.2746, val_loss: 0.7878, val_acc: 0.6830\n",
      "Epoch [188], train_loss: 0.2923, val_loss: 0.8283, val_acc: 0.6939\n",
      "Epoch [189], train_loss: 0.2902, val_loss: 0.7222, val_acc: 0.6838\n",
      "Epoch [190], train_loss: 0.2640, val_loss: 0.7876, val_acc: 0.6753\n",
      "Epoch [191], train_loss: 0.2887, val_loss: 1.2274, val_acc: 0.6193\n",
      "Epoch [192], train_loss: 0.2773, val_loss: 0.7213, val_acc: 0.6939\n",
      "Epoch [193], train_loss: 0.2537, val_loss: 0.7650, val_acc: 0.6962\n",
      "Epoch [194], train_loss: 0.2813, val_loss: 0.8590, val_acc: 0.6922\n",
      "Epoch [195], train_loss: 0.2686, val_loss: 0.7390, val_acc: 0.7195\n",
      "Epoch [196], train_loss: 0.2560, val_loss: 0.8072, val_acc: 0.6771\n",
      "Epoch [197], train_loss: 0.2919, val_loss: 0.8445, val_acc: 0.6825\n",
      "Epoch [198], train_loss: 0.2795, val_loss: 0.8400, val_acc: 0.6910\n",
      "Epoch [199], train_loss: 0.2595, val_loss: 0.8959, val_acc: 0.6761\n",
      "Epoch [200], train_loss: 0.2776, val_loss: 0.9906, val_acc: 0.6714\n",
      "Epoch [201], train_loss: 0.2747, val_loss: 0.8308, val_acc: 0.6709\n",
      "Epoch [202], train_loss: 0.2615, val_loss: 0.9371, val_acc: 0.6726\n",
      "Epoch [203], train_loss: 0.2794, val_loss: 0.7385, val_acc: 0.6778\n",
      "Epoch [204], train_loss: 0.2596, val_loss: 0.9882, val_acc: 0.6592\n",
      "Epoch [205], train_loss: 0.2776, val_loss: 0.7632, val_acc: 0.6905\n",
      "Epoch [206], train_loss: 0.2624, val_loss: 0.8421, val_acc: 0.6731\n",
      "Epoch [207], train_loss: 0.2645, val_loss: 0.7592, val_acc: 0.6910\n",
      "Epoch [208], train_loss: 0.2648, val_loss: 0.8282, val_acc: 0.6979\n",
      "Epoch [209], train_loss: 0.2811, val_loss: 0.7401, val_acc: 0.7026\n",
      "Epoch [210], train_loss: 0.2571, val_loss: 0.8157, val_acc: 0.7135\n",
      "Epoch [211], train_loss: 0.2481, val_loss: 0.8172, val_acc: 0.7240\n",
      "Epoch [212], train_loss: 0.2715, val_loss: 0.8293, val_acc: 0.6860\n",
      "Epoch [213], train_loss: 0.2590, val_loss: 1.4142, val_acc: 0.6530\n",
      "Epoch [214], train_loss: 0.2837, val_loss: 0.8709, val_acc: 0.6749\n",
      "Epoch [215], train_loss: 0.2703, val_loss: 0.8496, val_acc: 0.6992\n",
      "Epoch [216], train_loss: 0.2642, val_loss: 0.8839, val_acc: 0.6796\n",
      "Epoch [217], train_loss: 0.2666, val_loss: 0.7402, val_acc: 0.6939\n",
      "Epoch [218], train_loss: 0.2606, val_loss: 0.6995, val_acc: 0.6939\n",
      "Epoch [219], train_loss: 0.2545, val_loss: 0.7111, val_acc: 0.7026\n",
      "Epoch [220], train_loss: 0.2297, val_loss: 1.1299, val_acc: 0.6453\n",
      "Epoch [221], train_loss: 0.2626, val_loss: 0.8543, val_acc: 0.6987\n",
      "Epoch [222], train_loss: 0.2662, val_loss: 0.9342, val_acc: 0.6957\n",
      "Epoch [223], train_loss: 0.2599, val_loss: 0.8126, val_acc: 0.7083\n",
      "Epoch [224], train_loss: 0.2471, val_loss: 0.8244, val_acc: 0.7274\n",
      "Epoch [225], train_loss: 0.2503, val_loss: 0.7910, val_acc: 0.7004\n",
      "Epoch [226], train_loss: 0.2544, val_loss: 0.9362, val_acc: 0.6905\n",
      "Epoch [227], train_loss: 0.2537, val_loss: 0.8906, val_acc: 0.6870\n",
      "Epoch [228], train_loss: 0.2616, val_loss: 0.9472, val_acc: 0.6731\n",
      "Epoch [229], train_loss: 0.2446, val_loss: 0.9027, val_acc: 0.6974\n",
      "Epoch [230], train_loss: 0.2679, val_loss: 0.8818, val_acc: 0.6823\n",
      "Epoch [231], train_loss: 0.2460, val_loss: 0.8917, val_acc: 0.6709\n",
      "Epoch [232], train_loss: 0.2669, val_loss: 0.7556, val_acc: 0.7130\n"
     ]
    }
   ],
   "source": [
    "# CTX = torch.device('cuda')\n",
    "# train_dl.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # train_dl.train_labels.to(CTX)\n",
    "# # val_dl.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)\n",
    "# # val_dl.train_labels.to(CTX)\n",
    "num_epochs = 400\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
