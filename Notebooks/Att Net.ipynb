{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e18534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d832d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid\n",
    "\n",
    "data_path = 'adress.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,test_data,test_label,valid_data,valid_label,pernums_valid = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5013d37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2379, 300, 40, 3]), torch.Size([2379]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting training images into torch format\n",
    "train_x = train_data\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = train_label\n",
    "train_y = train_y.reshape(2379).astype(float);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "# shape of training data\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1142ffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 1., 0., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939264f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([297, 300, 40, 3]), torch.Size([297]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting training images into torch format\n",
    "val_x = valid_data\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = valid_label\n",
    "val_y = val_y.reshape(297).astype(float);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "# shape of training data\n",
    "val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdadcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e9fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(128 * 20 * 1, 20)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "#         print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce65f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(300, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=2560, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be36717",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46372397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 256, 40, 3]         691,456\n",
      "       BatchNorm2d-2           [-1, 256, 40, 3]             512\n",
      "              ReLU-3           [-1, 256, 40, 3]               0\n",
      "         MaxPool2d-4           [-1, 256, 20, 1]               0\n",
      "            Conv2d-5           [-1, 128, 20, 1]         295,040\n",
      "       BatchNorm2d-6           [-1, 128, 20, 1]             256\n",
      "              ReLU-7           [-1, 128, 20, 1]               0\n",
      "         MaxPool2d-8           [-1, 128, 20, 1]               0\n",
      "            Linear-9                   [-1, 20]          51,220\n",
      "================================================================\n",
      "Total params: 1,038,484\n",
      "Trainable params: 1,038,484\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.82\n",
      "Params size (MB): 3.96\n",
      "Estimated Total Size (MB): 4.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (300, 40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(val_x), Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.to(torch.int64).cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.to(torch.int64).cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "    \n",
    "#     print(output_train)\n",
    "    # computing the training and validation loss\n",
    "    with torch.autocast('cuda'):\n",
    "        loss_train = criterion(output_train, y_train)\n",
    "        loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87f21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining the number of epochs\n",
    "# n_epochs = 25\n",
    "# # empty list to store training losses\n",
    "# train_losses = []\n",
    "# # empty list to store validation losses\n",
    "# val_losses = []\n",
    "# # training the model\n",
    "# for epoch in range(n_epochs):\n",
    "#     train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c708f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b9988ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # plotting the training and validation loss\n",
    "# plt.plot(train_losses, label='Training loss')\n",
    "# plt.plot(val_losses, label='Validation loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2ab9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction for training set\n",
    "# with torch.no_grad():\n",
    "#     output = model(train_x.cuda())\n",
    "    \n",
    "# softmax = torch.exp(output).cpu()\n",
    "# prob = list(softmax.numpy())\n",
    "# predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# # accuracy on training set\n",
    "# accuracy_score(train_y, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cfbb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction for validation set\n",
    "# with torch.no_grad():\n",
    "#     output = model(val_x.cuda())\n",
    "\n",
    "# softmax = torch.exp(output).cpu()\n",
    "# prob = list(softmax.numpy())\n",
    "# predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# # accuracy on validation set\n",
    "# accuracy_score(val_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9478e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "#         print(x.shape())\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7596a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Att_Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(300, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(128 * 20 * 1, 256),\n",
    "            Linear(256, 64),\n",
    "            Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.attention = CBAM(gate_channels=128)\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d5168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att_Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(300, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=2560, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (attention): CBAM(\n",
      "    (ChannelGate): ChannelGate(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten()\n",
      "        (1): Linear(in_features=128, out_features=8, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=8, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (SpatialGate): SpatialGate(\n",
      "      (compress): ChannelPool()\n",
      "      (spatial): BasicConv(\n",
      "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Att_Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8deb9698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 256, 40, 3]         691,456\n",
      "       BatchNorm2d-2           [-1, 256, 40, 3]             512\n",
      "              ReLU-3           [-1, 256, 40, 3]               0\n",
      "         MaxPool2d-4           [-1, 256, 20, 1]               0\n",
      "            Conv2d-5           [-1, 128, 20, 1]         295,040\n",
      "       BatchNorm2d-6           [-1, 128, 20, 1]             256\n",
      "              ReLU-7           [-1, 128, 20, 1]               0\n",
      "         MaxPool2d-8           [-1, 128, 20, 1]               0\n",
      "           Flatten-9                  [-1, 128]               0\n",
      "           Linear-10                    [-1, 8]           1,032\n",
      "             ReLU-11                    [-1, 8]               0\n",
      "           Linear-12                  [-1, 128]           1,152\n",
      "          Flatten-13                  [-1, 128]               0\n",
      "           Linear-14                    [-1, 8]           1,032\n",
      "             ReLU-15                    [-1, 8]               0\n",
      "           Linear-16                  [-1, 128]           1,152\n",
      "      ChannelGate-17           [-1, 128, 20, 1]               0\n",
      "      ChannelPool-18             [-1, 2, 20, 1]               0\n",
      "           Conv2d-19             [-1, 1, 20, 1]              98\n",
      "      BatchNorm2d-20             [-1, 1, 20, 1]               2\n",
      "        BasicConv-21             [-1, 1, 20, 1]               0\n",
      "      SpatialGate-22           [-1, 128, 20, 1]               0\n",
      "             CBAM-23           [-1, 128, 20, 1]               0\n",
      "           Linear-24                  [-1, 256]         655,616\n",
      "           Linear-25                   [-1, 64]          16,448\n",
      "           Linear-26                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 1,663,926\n",
      "Trainable params: 1,663,926\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.89\n",
      "Params size (MB): 6.35\n",
      "Estimated Total Size (MB): 7.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\tf-btp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (300, 40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a21670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining the number of epochs\n",
    "# n_epochs = 1000\n",
    "# # empty list to store training losses\n",
    "# train_losses = []\n",
    "# # empty list to store validation losses\n",
    "# val_losses = []\n",
    "# # training the model\n",
    "# for epoch in range(n_epochs):\n",
    "#     train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91eef8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction for training set\n",
    "# with torch.no_grad():\n",
    "#     output = model(train_x.cuda())\n",
    "    \n",
    "# softmax = torch.exp(output).cpu()\n",
    "# prob = list(softmax.numpy())\n",
    "# predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# # accuracy on training set\n",
    "# accuracy_score(train_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e22fa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# scaler = GradScaler()\n",
    "# batch_size = 4\n",
    "# gradient_accumulations = 16\n",
    "# # this means training will be done for affective batch size of 4 * 16 = 64\n",
    "# epochs = range(100)\n",
    "# model.zero_grad()\n",
    "# for epoch in epochs:\n",
    "#         # Create a Batch\n",
    "#         for batch_idx, batch in enumerate(zip(train_x,train_y)):\n",
    "#             x, y = batch\n",
    "#             with autocast():\n",
    "#                 y_hat = model(x)\n",
    "#                 loss = objective_function(y_hat, y)\n",
    "\n",
    "#             scaler.scale(loss / gradient_accumulations).backward()\n",
    "\n",
    "#             if (batch_idx + 1) % gradient_accumulations == 0:\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b58a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(6.2333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  51 \t loss : tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  101 \t loss : tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  151 \t loss : tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  201 \t loss : tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  251 \t loss : tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  301 \t loss : tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  351 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  401 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  451 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  501 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  551 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  601 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  651 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  701 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  751 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  801 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  851 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  901 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  951 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1001 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1051 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1101 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1151 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1201 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1251 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1301 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1351 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1401 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1451 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1501 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1551 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1601 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1651 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1701 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1751 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1801 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1851 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1901 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  1951 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2001 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2051 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2101 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2151 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2201 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2251 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2301 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2351 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2401 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2451 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2501 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2551 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2601 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2651 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2701 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2751 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2801 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2851 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2901 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch :  2951 \t loss : tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "n_epochs = 3000 # or whatever\n",
    "batch_size = 100 # or whatever\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # X is a torch Variable\n",
    "    permutation = torch.randperm(train_x.size()[0])\n",
    "\n",
    "    for i in range(0,train_x.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = train_x[indices], train_y[indices]\n",
    "\n",
    "#         # in case you wanted a semi-full example\n",
    "#         outputs = model.forward(batch_x)\n",
    "#         loss = lossfunction(outputs,batch_y)\n",
    "        with autocast():\n",
    "                outputs = model(batch_x.cuda())\n",
    "                loss = criterion(outputs,batch_y.to(torch.int64).cuda())\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%50 == 0:\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(batch_x.cuda())\n",
    "#         softmax = torch.exp(outputs).cpu()\n",
    "#         prob = list(softmax.numpy())\n",
    "#         predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "#         # accuracy on batch\n",
    "\n",
    "\n",
    "#         print('Accuracy : ',epoch+1, '\\t', 'accuracy :', accuracy_score(batch_y, predictions))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e571bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd9bbb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4859184531315679"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction for training set\n",
    "with torch.no_grad():\n",
    "    output = model(train_x.cuda())\n",
    "    \n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(train_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b250618b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 1., 0., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4652767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80871afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
