{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213a141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "    #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "    #v = tf.tanh(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "    v = tf.sigmoid(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu)              # (B,T) shape also\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204947ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.training import moving_averages\n",
    "# Importer and Exporting\n",
    "# ========\n",
    "\n",
    "# tf.app.flags.DEFINE_string  ('data_path',  'IEMOCAP1.pkl',   'total dataset includes training set, valid set and test set')\n",
    "# tf.app.flags.DEFINE_string  ('checkpoint', 'checkpoint/',   'the checkpoint dir')\n",
    "# tf.app.flags.DEFINE_string  ('model_name', 'model.ckpt',      'model name')\n",
    "# tf.app.flags.DEFINE_string  ('pred_name',  'pred0.pkl',        'the test output dir')\n",
    "# tf.app.flags.DEFINE_integer ('checkpoint_secs',  60,         'checkpoint saving interval in seconds')\n",
    "# # Global Constants\n",
    "# # ================\n",
    "\n",
    "# tf.app.flags.DEFINE_float   ('dropout_conv',     1,        'dropout rate for covvolutional layers')\n",
    "# tf.app.flags.DEFINE_float   ('dropout_linear',   1,        'dropout rate for linear layer')\n",
    "# tf.app.flags.DEFINE_float   ('dropout_lstm',     1,        'dropout rate for lstm')\n",
    "# tf.app.flags.DEFINE_float   ('dropout_fully1',   1,        'dropout rate for fully connected layer1')\n",
    "# tf.app.flags.DEFINE_float   ('dropout_fully2',   1,        'dropout rate for fully connected layer1')\n",
    "\n",
    "# tf.app.flags.DEFINE_float('decay_rate', 0.99, 'the lr decay rate')\n",
    "# tf.app.flags.DEFINE_float('beta1', 0.9, 'parameter of adam optimizer beta1')\n",
    "# tf.app.flags.DEFINE_float('beta2', 0.999, 'adam parameter beta2')\n",
    "\n",
    "\n",
    "# tf.app.flags.DEFINE_integer('decay_steps', 570, 'the lr decay_step for optimizer')\n",
    "# tf.app.flags.DEFINE_float('momentum', 0.99, 'the momentum')\n",
    "# tf.app.flags.DEFINE_integer('num_epochs', 30000, 'maximum epochs')\n",
    "# tf.app.flags.DEFINE_float   ('relu_clip',        20.0,        'ReLU clipping value for non-recurrant layers')\n",
    "\n",
    "# tf.app.flags.DEFINE_float   ('adam_beta1',            0.9,         'beta 1 parameter of Adam optimizer')\n",
    "# tf.app.flags.DEFINE_float   ('adam_beta2',            0.999,       'beta 2 parameter of Adam optimizer')\n",
    "# tf.app.flags.DEFINE_float   ('epsilon',          1e-8,        'epsilon parameter of Adam optimizer')\n",
    "# tf.app.flags.DEFINE_float   ('learning_rate',    0.0001,       'learning rate of Adam optimizer')\n",
    "\n",
    "\n",
    "# tf.app.flags.DEFINE_integer ('train_batch_size', 40,           'number of elements in a training batch')\n",
    "# tf.app.flags.DEFINE_integer ('valid_batch_size',   40,           'number of elements in a validation batch')\n",
    "# tf.app.flags.DEFINE_integer ('test_batch_size',  40,           'number of elements in a test batch')\n",
    "\n",
    "# tf.app.flags.DEFINE_integer('save_steps', 10, 'the step to save checkpoint')\n",
    "\n",
    "# tf.app.flags.DEFINE_integer('image_height', 300, 'image height')\n",
    "# tf.app.flags.DEFINE_integer('image_width', 40, 'image width')\n",
    "# tf.app.flags.DEFINE_integer('image_channel', 3, 'image channels as input')\n",
    "# tf.app.flags.DEFINE_integer('linear_num', 786, 'hidden number of linear layer')\n",
    "# tf.app.flags.DEFINE_integer('seq_len', 150, 'sequence length of lstm')\n",
    "# tf.app.flags.DEFINE_integer('cell_num', 128, 'cell units of the lstm')\n",
    "# tf.app.flags.DEFINE_integer('hidden1', 64, 'number of hidden units of fully connected layer')\n",
    "# tf.app.flags.DEFINE_integer('hidden2', 4, 'number of softmax layer')\n",
    "# tf.app.flags.DEFINE_integer('attention_size', 1, 'attention_size')\n",
    "# tf.app.flags.DEFINE_boolean('attention', False, 'whether to use attention, False mean use max-pooling')\n",
    "\n",
    "data_path = '/IEMOCAP1.pkl'\n",
    "checkpoint = '/checkpoint/'\n",
    "model_name = '/model.ckpt'\n",
    "pred_name = '/pred0.pkl'\n",
    "checkpoint_secs = 60\n",
    "\n",
    "dropout_conv = 1\n",
    "dropout_linear = 1\n",
    "dropout_lstm = 1\n",
    "dropout_fully1 = 1\n",
    "dropout_fully2 = 1\n",
    "\n",
    "#decayed_learning rate\n",
    "decay_rate = 0.99\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "#Moving Average\n",
    "decay_steps = 570\n",
    "momentum = 0.99\n",
    "num_epochs = 30000\n",
    "relu_clip =  20.0\n",
    "\n",
    "# Adam optimizer (http://arxiv.org/abs/1412.6980) parameters\n",
    "\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "epsilon =  1e-8\n",
    "learning_rate =   0.0001\n",
    "\n",
    "# Batch sizes\n",
    "\n",
    "\n",
    "train_batch_size = 40\n",
    "valid_batch_size = 40\n",
    "test_batch_size =  40\n",
    "\n",
    "save_steps =   10\n",
    "\n",
    "image_height =   300\n",
    "image_width = 40\n",
    "image_channel = 3\n",
    "\n",
    "linear_num =  786\n",
    "seq_len =   150\n",
    "cell_num = 128\n",
    "hidden1 = 64\n",
    "hidden2 =  4\n",
    "attention_size =   1\n",
    "attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713afac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(object):\n",
    "    \n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        # log Mel-spectrogram\n",
    "        self.attention = attention\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, image_height, image_width, image_channel])\n",
    "        # emotion label\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, 4])\n",
    "        # lstm time step\n",
    "        #self.seq_len = tf.placeholder(tf.int32, [None])\n",
    "        # l2\n",
    "        self._extra_train_ops = []\n",
    "\n",
    "    def _conv2d(self, x, name, filter_size, in_channels, out_channels, strides):\n",
    "        with tf.variable_scope(name):\n",
    "            kernel = tf.get_variable(name='DW',\n",
    "                                     shape=[filter_size[0], filter_size[1], in_channels, out_channels],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            b = tf.get_variable(name='bais',\n",
    "                                shape=[out_channels],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer())\n",
    "\n",
    "            con2d_op = tf.nn.conv2d(x, kernel, [1, strides[0], strides[1], 1], padding='SAME')\n",
    "\n",
    "        return tf.nn.bias_add(con2d_op, b) \n",
    "    \n",
    "    def _max_pool(self, x, ksize, strides):\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize=[1, ksize[0], ksize[1], 1],\n",
    "                              strides=[1, strides[0], strides[1], 1],\n",
    "                              padding='VALID',\n",
    "                              name='max_pool')\n",
    "    \n",
    "\n",
    "    def _linear(self,x,names,shapes):\n",
    "        with tf.variable_scope(names):\n",
    "            weights = tf.get_variable(name='weights',\n",
    "                                      shape=shapes,\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            bias = tf.get_variable(name='bias',\n",
    "                                   shape=shapes[1],\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x,weights) + bias\n",
    "    \n",
    "\n",
    "    def _leaky_relu(self, x, leakiness=0.0):\n",
    "        return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "    \n",
    "    def _batch_norm(self, name, x):\n",
    "        \"\"\"Batch normalization.\"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            params_shape = [x.get_shape()[-1]]\n",
    "\n",
    "            beta = tf.get_variable(\n",
    "                'beta', params_shape, tf.float32,\n",
    "                initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "            gamma = tf.get_variable(\n",
    "                'gamma', params_shape, tf.float32,\n",
    "                initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "\n",
    "                moving_mean = tf.get_variable(\n",
    "                    'moving_mean', params_shape, tf.float32,\n",
    "                    initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "                    trainable=False)\n",
    "                moving_variance = tf.get_variable(\n",
    "                    'moving_variance', params_shape, tf.float32,\n",
    "                    initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "                    trainable=False)\n",
    "\n",
    "                self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "                    moving_mean, mean, 0.9))\n",
    "                self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "                    moving_variance, variance, 0.9))\n",
    "            else:\n",
    "                mean = tf.get_variable(\n",
    "                    'moving_mean', params_shape, tf.float32,\n",
    "                    initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "                    trainable=False)\n",
    "                variance = tf.get_variable(\n",
    "                    'moving_variance', params_shape, tf.float32,\n",
    "                    initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "                    trainable=False)\n",
    "\n",
    "#                tf.summary.histogram(mean.op.name, mean)\n",
    "#                tf.summary.histogram(variance.op.name, variance)\n",
    "            # elipson used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
    "            x_bn = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.001)\n",
    "            x_bn.set_shape(x.get_shape())\n",
    "\n",
    "            return x_bn\n",
    "        \n",
    "        \n",
    "    def _batch_norm_wrapper(self, name, inputs, decay = 0.999):\n",
    "        #batch normalization for fully connected layer\n",
    "        with tf.variable_scope(name):\n",
    "            scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "            beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "            pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "            pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "                train_mean = tf.assign(pop_mean,\n",
    "                                       pop_mean * decay + batch_mean * (1 - decay))\n",
    "                train_var = tf.assign(pop_var,\n",
    "                                      pop_var * decay + batch_var * (1 - decay))\n",
    "                with tf.control_dependencies([train_mean, train_var]):\n",
    "                    return tf.nn.batch_normalization(inputs,\n",
    "                                                     batch_mean, batch_var, beta, scale, epsilon)\n",
    "            else:\n",
    "                return tf.nn.batch_normalization(inputs,\n",
    "                                                 pop_mean, pop_var, beta, scale, epsilon)\n",
    "            \n",
    "            \n",
    "    def _attention(self,inputs, attention_size, time_major=False, return_alphas=False):\n",
    "        \n",
    "        if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "            inputs = tf.concat(inputs, 2)\n",
    "\n",
    "        if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "            inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "        hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "        # Trainable parameters\n",
    "        W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "        b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "        u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        #v = tf.tanh(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "        v = tf.sigmoid(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
    "        # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "        vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
    "        alphas = tf.nn.softmax(vu)              # (B,T) shape also\n",
    "        \n",
    "        # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "        output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "        if not return_alphas:\n",
    "            return output\n",
    "        else:\n",
    "            return output, alphas\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        filters = [128, 512]\n",
    "        filter_size = [5, 3]\n",
    "        filter_strides = [1, 1]\n",
    "        pool1_size = [2, 4]\n",
    "        pool2_size = [1, 2]\n",
    "        p = 5\n",
    "        with tf.variable_scope('cnn'):\n",
    "            with tf.variable_scope('unit-1'):\n",
    "                x = self._conv2d(self.inputs, 'cnn-1', filter_size, image_channel, filters[0], filter_strides)\n",
    "                x = self._batch_norm('bn1', x)\n",
    "                x = self._leaky_relu(x, 0.01)\n",
    "                x = self._max_pool(x, pool1_size, pool1_size)\n",
    "#                print x.get_shape()\n",
    "            with tf.variable_scope('unit-2'):\n",
    "                x = self._conv2d(x, 'cnn-2',  filter_size, filters[0], filters[1], filter_strides)\n",
    "                x = self._batch_norm('bn2', x)\n",
    "                x = self._leaky_relu(x, 0.01)\n",
    "                x = self._max_pool(x, pool2_size, pool2_size)\n",
    "#                print x.get_shape()\n",
    "        with tf.variable_scope('linear'):\n",
    "            # linear layer for dim reduction\n",
    "            x = tf.reshape(x,[-1,p*filters[1]])\n",
    "            x = self._linear(x,'linear1',[p*filters[1],linear_num])\n",
    "#            print x.get_shape()\n",
    "        with tf.variable_scope('lstm'):\n",
    "            x = tf.reshape(x,[-1,seq_len,linear_num])\n",
    "            \n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(cell_num, forget_bias=1.0)\n",
    "            if self.mode == 'train':\n",
    "                cell_fw = tf.contrib.rnn.DropoutWrapper(cell=cell_fw, output_keep_prob=dropout_lstm)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(cell_num, forget_bias=1.0)\n",
    "            if self.mode == 'train':\n",
    "                cell_bw = tf.contrib.rnn.DropoutWrapper(cell=cell_bw, output_keep_prob=dropout_lstm)\n",
    "            \n",
    "            # Now we feed `linear` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n",
    "                                                                       cell_bw=cell_bw,\n",
    "                                                                       inputs= x,\n",
    "                                                                       dtype=tf.float32,\n",
    "                                                                       time_major=False,\n",
    "                                                                       scope='LSTM1')\n",
    "        with tf.variable_scope('time_pooling'):\n",
    "            if self.attention is not None:\n",
    "                outputs, alphas = self._attention(outputs, attention_size, return_alphas=True)\n",
    "            else:\n",
    "                outputs = tf.concat(outputs,2)\n",
    "                outputs = tf.reshape(outputs, [-1, seq_len,2*cell_num, 1])\n",
    "                outputs = self._max_pool(outputs,[seq_len,1],[seq_len,1])\n",
    "                outputs = tf.reshape(outputs, [-1,2*cell_num])\n",
    "#            print outputs.get_shape()\n",
    "        \n",
    "        with tf.variable_scope('dense'):\n",
    "            y = self._linear(outputs,'dense-matmul',[2*cell_num,hidden1])\n",
    "            y = self._batch_norm_wrapper('dense-bn', y)\n",
    "            y = self._leaky_relu(y, 0.01)\n",
    "        \n",
    "        self.logits = self._linear(y,'softmax',[hidden1,hidden2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d48c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "\n",
    "def leaky_relu(x, leakiness=0.0):\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "\n",
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training is not None:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "def acrnn(inputs, num_classes=4,\n",
    "                  is_training=True,\n",
    "                  L1=128,\n",
    "                  L2=256,\n",
    "                  cell_units=128,\n",
    "                  num_linear=768,\n",
    "                  p=10,\n",
    "                  time_step=150,\n",
    "                  F1=64,\n",
    "                  dropout_keep_prob=1):\n",
    "    \n",
    "    global ndims\n",
    "    layer1_filter = tf.compat.v1.get_variable('layer1_filter', shape=[5, 3, 3, L1], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer1_bias = tf.compat.v1.get_variable('layer1_bias', shape=[L1], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer1_stride = [1, 1, 1, 1]\n",
    "    layer2_filter = tf.compat.v1.get_variable('layer2_filter', shape=[5, 3, L1, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer2_bias = tf.compat.v1.get_variable('layer2_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer2_stride = [1, 1, 1, 1]\n",
    "    layer3_filter = tf.compat.v1.get_variable('layer3_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer3_bias = tf.compat.v1.get_variable('layer3_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer3_stride = [1, 1, 1, 1]\n",
    "    layer4_filter = tf.compat.v1.get_variable('layer4_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer4_bias = tf.compat.v1.get_variable('layer4_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer4_stride = [1, 1, 1, 1]\n",
    "    layer5_filter = tf.compat.v1.get_variable('layer5_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer5_bias = tf.compat.v1.get_variable('layer5_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer5_stride = [1, 1, 1, 1]\n",
    "    layer6_filter = tf.compat.v1.get_variable('layer6_filter', shape=[5, 3, L2, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    layer6_bias = tf.compat.v1.get_variable('layer6_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    layer6_stride = [1, 1, 1, 1]\n",
    "    \n",
    "    linear1_weight = tf.compat.v1.get_variable('linear1_weight', shape=[p*L2,num_linear], dtype=tf.float32,\n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    linear1_bias = tf.compat.v1.get_variable('linear1_bias', shape=[num_linear], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    " \n",
    "    fully1_weight = tf.compat.v1.get_variable('fully1_weight', shape=[2*cell_units,F1], dtype=tf.float32,\n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    fully1_bias = tf.compat.v1.get_variable('fully1_bias', shape=[F1], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    fully2_weight = tf.compat.v1.get_variable('fully2_weight', shape=[F1,num_classes], dtype=tf.float32,\n",
    "                                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1))\n",
    "    fully2_bias = tf.compat.v1.get_variable('fully2_bias', shape=[num_classes], dtype=tf.float32,\n",
    "                                  initializer=tf.compat.v1.constant_initializer(0.1))\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(inputs, layer1_filter, layer1_stride, padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1,layer1_bias)\n",
    "    layer1 = leaky_relu(layer1, 0.01)\n",
    "    layer1 = tf.nn.max_pool(layer1,ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID', name='max_pool')\n",
    "    layer1 = tf.keras.layers.Dropout(layer1)\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2,layer2_bias)\n",
    "    layer2 = leaky_relu(layer2, 0.01)\n",
    "    layer2 = tf.keras.layers.Dropout(layer2)\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer2, layer3_filter, layer3_stride, padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3,layer3_bias)\n",
    "    layer3 = leaky_relu(layer3, 0.01)\n",
    "    layer3 = tf.keras.layers.Dropout(layer3)\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer3, layer4_filter, layer4_stride, padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4,layer4_bias)\n",
    "    layer4 = leaky_relu(layer4, 0.01)\n",
    "    layer4 = tf.keras.layers.Dropout(layer4)\n",
    "    \n",
    "    layer5 = tf.nn.conv2d(layer4, layer5_filter, layer5_stride, padding='SAME')\n",
    "    layer5 = tf.nn.bias_add(layer5,layer5_bias)\n",
    "    layer5 = leaky_relu(layer5, 0.01)    \n",
    "    layer5 = tf.keras.layers.Dropout(layer5)\n",
    "\n",
    "    layer6 = tf.nn.conv2d(layer5, layer6_filter, layer6_stride, padding='SAME')\n",
    "    layer6 = tf.nn.bias_add(layer6,layer6_bias)\n",
    "    layer6 = leaky_relu(layer6, 0.01)    \n",
    "    layer6 = tf.keras.layers.Dropout(layer6)\n",
    "    \n",
    "    layer6 = tf.reshape(layer6,[-1,time_step,L2*p])\n",
    "    layer6 = tf.reshape(layer6, [-1,p*L2])\n",
    "    \n",
    "    linear1 = tf.matmul(layer6,linear1_weight) + linear1_bias\n",
    "    linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = leaky_relu(linear1, 0.01)\n",
    "    #linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = tf.reshape(linear1, [-1, time_step, num_linear])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    \n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs1, output_states1 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell1,\n",
    "                                                             cell_bw=gru_bw_cell1,\n",
    "                                                             inputs= linear1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM1')\n",
    "\n",
    "    # Attention layer\n",
    "    gru, alphas = attention(outputs1, 1, return_alphas=True)\n",
    "    \n",
    "    \n",
    "    fully1 = tf.matmul(gru,fully1_weight) + fully1_bias\n",
    "    fully1 = leaky_relu(fully1, 0.01)\n",
    "    fully1 = tf.nn.dropout(fully1, dropout_keep_prob)\n",
    "    \n",
    "    \n",
    "    Ylogits = tf.matmul(fully1, fully2_weight) + fully2_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    return Ylogits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aefdfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import confusion_matrix as confusion\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a3e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.app.flags.DEFINE_integer('num_epoch', 5000, 'The number of epoches for training.')\n",
    "# tf.app.flags.DEFINE_integer('num_classes', 4, 'The number of emotion classes.')\n",
    "# tf.app.flags.DEFINE_integer('batch_size', 60, 'The number of samples in each batch.')\n",
    "# tf.app.flags.DEFINE_boolean('is_adam', True,'whether to use adam optimizer.')\n",
    "# tf.app.flags.DEFINE_float('learning_rate', 0.00001, 'learning rate of Adam optimizer')\n",
    "# tf.app.flags.DEFINE_float   ('dropout_keep_prob',     1,        'the prob of every unit keep in dropout layer')\n",
    "# tf.app.flags.DEFINE_integer('image_height', 300, 'image height')\n",
    "# tf.app.flags.DEFINE_integer('image_width', 40, 'image width')\n",
    "# tf.app.flags.DEFINE_integer('image_channel', 3, 'image channels as input')\n",
    "\n",
    "# tf.app.flags.DEFINE_string  ('traindata_path', './IEMOCAP.pkl', 'total dataset includes training set')\n",
    "# tf.app.flags.DEFINE_string  ('validdata_path', 'inputs/valid.pkl', 'total dataset includes valid set')\n",
    "# tf.app.flags.DEFINE_string  ('checkpoint', './checkpoint/', 'the checkpoint dir')\n",
    "# tf.app.flags.DEFINE_string  ('model_name', 'model4.ckpt', 'model name')\n",
    "\n",
    "def load_data(in_dir):\n",
    "    f = open(in_dir,'rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,Valid_label,Test_label,pernums_test,pernums_valid = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label,Valid_label,Test_label,pernums_test,pernums_valid\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb2fffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ndims' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10436/90585678.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mYlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mYlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10436/3429188144.py\u001b[0m in \u001b[0;36macrnn\u001b[1;34m(inputs, num_classes, is_training, L1, L2, cell_units, num_linear, p, time_step, F1, dropout_keep_prob)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mlayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2_stride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer2_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2157\u001b[0m   \"\"\"\n\u001b[0;32m   2158\u001b[0m   \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m   return conv2d(input,  # pylint: disable=redefined-builtin\n\u001b[0m\u001b[0;32m   2160\u001b[0m                 \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m                 \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2260\u001b[0m     \u001b[0mndims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ndims\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mndims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2262\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mndims\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2263\u001b[0m     \u001b[1;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2264\u001b[0m     \u001b[1;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'ndims' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "tf.disable_eager_execution()\n",
    "\n",
    "num_classes = 4\n",
    "is_adam = True\n",
    "dropout_keep_prob = 1\n",
    "data_path = 'IEMOCAP.pkl'\n",
    "checkpoint = 'checkpoint/'\n",
    "\n",
    "train_data,train_label,test_data,test_label,valid_data,valid_label,Valid_label,Test_label,pernums_test,pernums_valid = load_data(data_path)\n",
    "\n",
    "\n",
    "\n",
    "train_label = dense_to_one_hot(train_label,num_classes)\n",
    "valid_label = dense_to_one_hot(valid_label,num_classes)\n",
    "Valid_label = dense_to_one_hot(Valid_label,num_classes)\n",
    "\n",
    "valid_size = valid_data.shape[0]\n",
    "dataset_size = train_data.shape[0]\n",
    "vnum = pernums_valid.shape[0]\n",
    "best_valid_uw = 0\n",
    "\n",
    "\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, image_height,image_width,image_channel])\n",
    "Y = tf.compat.v1.placeholder(tf.int32, shape=[None, num_classes])\n",
    "\n",
    "is_training = tf.compat.v1.placeholder(tf.bool)\n",
    "lr = tf.compat.v1.placeholder(tf.float32)\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "Ylogits = acrnn(X, is_training=is_training, dropout_keep_prob=keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "var_trainable_op = tf.trainable_variables()\n",
    "if is_adam:\n",
    "    # not apply gradient clipping\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(cost)            \n",
    "else:\n",
    "    # apply gradient clipping\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, var_trainable_op), 5)\n",
    "    opti = tf.train.AdamOptimizer(lr)\n",
    "    train_op = opti.apply_gradients(zip(grads, var_trainable_op))\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver=tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epoch):\n",
    "        #learning_rate = FLAGS.learning_rate            \n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "        [_,tcost,tracc] = sess.run([train_op,cost,accuracy], feed_dict={X:train_data[start:end,:,:,:], Y:train_label[start:end,:],\n",
    "                                        is_training:True, keep_prob:dropout_keep_prob, lr:learning_rate})\n",
    "        if i % 5 == 0:\n",
    "            #for valid data\n",
    "            valid_iter = divmod((valid_size),batch_size)[0]\n",
    "            y_pred_valid = np.empty((valid_size,num_classes),dtype=np.float32)\n",
    "            y_valid = np.empty((vnum,4),dtype=np.float32)\n",
    "            index = 0\n",
    "            cost_valid = 0\n",
    "            if(valid_size < batch_size):\n",
    "                loss, y_pred_valid = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data, Y:Valid_label,is_training:False, keep_prob:1})\n",
    "                cost_valid = cost_valid + np.sum(loss)\n",
    "            for v in range(valid_iter):\n",
    "                v_begin = v*batch_size\n",
    "                v_end = (v+1)*batch_size\n",
    "                if(v == valid_iter-1):\n",
    "                    if(v_end < valid_size):\n",
    "                        v_end = valid_size\n",
    "                loss, y_pred_valid[v_begin:v_end,:] = sess.run([cross_entropy,Ylogits],feed_dict = {X:valid_data[v_begin:v_end],Y:Valid_label[v_begin:v_end],is_training:False, keep_prob:1})\n",
    "                cost_valid = cost_valid + np.sum(loss)\n",
    "            cost_valid = cost_valid/valid_size\n",
    "            \n",
    "            for s in range(vnum):\n",
    "                y_valid[s,:] = np.max(y_pred_valid[index:index+pernums_valid[s],:],0)\n",
    "                index = index + pernums_valid[s]\n",
    "\n",
    "            valid_acc_uw = recall(np.argmax(valid_label,1),np.argmax(y_valid,1),average='macro')\n",
    "            valid_conf = confusion(np.argmax(valid_label, 1),np.argmax(y_valid,1))\n",
    "            \n",
    "            if valid_acc_uw > best_valid_uw:\n",
    "                best_valid_uw = valid_acc_uw\n",
    "                best_valid_conf = valid_conf\n",
    "                saver.save(sess, os.path.join(checkpoint, model_name), global_step = i+1)\n",
    "            \n",
    "            print (\"*****************************************************************\")\n",
    "            print (\"Epoch: %05d\" %(i+1))\n",
    "            print (\"Training cost: %2.3g\" %tcost)   \n",
    "            print (\"Training accuracy: %3.4g\" %tracc) \n",
    "            print (\"Valid cost: %2.3g\" %cost_valid)\n",
    "            print (\"Valid_UA: %3.4g\" %valid_acc_uw)    \n",
    "            print (\"Best valid_UA: %3.4g\" %best_valid_uw) \n",
    "            print ('Valid Confusion Matrix:[\"ang\",\"sad\",\"hap\",\"neu\"]')\n",
    "            print (valid_conf)\n",
    "            print ('Best Valid Confusion Matrix:[\"ang\",\"sad\",\"hap\",\"neu\"]')\n",
    "            print (best_valid_conf)\n",
    "            print (\"*****************************************************************\" )\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d68564",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def leaky_relu(x, leakiness=0.0):\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "def load_data():\n",
    "    f = open('./CASIA_40_delta.pkl','rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,Valid_label,Test_label,pernums_test,pernums_valid = cPickle.load(f)\n",
    "    #train_data,train_label,test_data,test_label,valid_data,valid_label = cPickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label\n",
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training is not None:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_averages\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "def build_model(inputX, is_training,keep_prob):\n",
    "    # 3 2-D convolution layers\n",
    "    L1 = 256\n",
    "    L2 = 512\n",
    "    L3 = 512\n",
    "    Li1 = 768\n",
    "    F1 = 64\n",
    "    F2 = 6\n",
    "    p = 5\n",
    "    cell_units1 = 128\n",
    "    timesteps = 200\n",
    "    ATTENTION_SIZE = 1\n",
    "    layer1_filter = tf.get_variable('layer1_filter', shape=[5, 3, 3, L1], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer1_bias = tf.get_variable('layer1_bias', shape=[L1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer1_stride = [1, 1, 1, 1]\n",
    "    layer2_filter = tf.get_variable('layer2_filter', shape=[5, 3, L1, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer2_bias = tf.get_variable('layer2_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer2_stride = [1, 1, 1, 1]\n",
    "    layer3_filter = tf.get_variable('layer3_filter', shape=[5, 3, L2, L3], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer3_bias = tf.get_variable('layer3_bias', shape=[L3], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer3_stride = [1, 1, 1, 1]\n",
    "    \n",
    "    linear1_weight = tf.get_variable('linear1_weight', shape=[p*L2,Li1], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    linear1_bias = tf.get_variable('linear1_bias', shape=[Li1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    " \n",
    "    fully1_weight = tf.get_variable('fully1_weight', shape=[2*cell_units1,F1], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully1_bias = tf.get_variable('fully1_bias', shape=[F1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    fully2_weight = tf.get_variable('fully2_weight', shape=[F1,F2], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully2_bias = tf.get_variable('fully2_bias', shape=[F2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer1 = tf.nn.conv2d(inputX, layer1_filter, layer1_stride, padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1,layer1_bias)\n",
    "    #layer1 = tf.layers.batch_normalization(layer1, training=is_training)\n",
    "    #layer1 = Batch_Normalization(layer1, training=is_training, scope='layer1_batch')\n",
    "    layer1 = leaky_relu(layer1, 0.01)\n",
    "    #layer1 = Batch_Normalization(layer1, training=is_training, scope='layer1_batch')\n",
    "    #print layer1.get_shape()\n",
    "    layer1 = tf.nn.max_pool(layer1,ksize=[1, 1, 4, 1], strides=[1, 1, 4, 1], padding='VALID', name='max_pool')\n",
    "    #print layer1.get_shape()\n",
    "    layer1 = tf.contrib.layers.dropout(layer1, keep_prob=keep_prob, is_training=is_training)\n",
    "    #layer1 = tf.reshape(layer1,[-1,timesteps,L1*p])\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2,layer2_bias)\n",
    "    #layer1 = tf.layers.batch_normalization(layer1, training=is_training)\n",
    "    \n",
    "    layer2 = leaky_relu(layer2, 0.01)\n",
    "    #print layer2.get_shape()\n",
    "    #layer2 = Batch_Normalization(layer2, training=is_training, scope='layer1_batch')\n",
    "    layer2 = tf.nn.max_pool(layer2,ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='VALID', name='max_pool')\n",
    "    #print layer2.get_shape()\n",
    "    layer2 = tf.contrib.layers.dropout(layer2, keep_prob=keep_prob, is_training=is_training)\n",
    "    layer2 = tf.reshape(layer2,[-1,timesteps,L2*p])\n",
    "    \n",
    "    \n",
    "    layer2 = tf.reshape(layer2, [-1,p*L2])\n",
    "    \n",
    "    #layer1 = tf.reshape(layer1,[-1,p*L1])\n",
    "    linear1 = tf.matmul(layer2,linear1_weight) + linear1_bias\n",
    "    linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = leaky_relu(linear1, 0.01)\n",
    "    #linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = tf.reshape(linear1, [-1, timesteps, Li1])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #adding gru cell\n",
    "    gru_bw_cell1 = tf.nn.rnn_cell.GRUCell(cell_units)\n",
    "    #if is_training is not None:\n",
    "    #    gru_bw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_bw_cell1, output_keep_prob=keep_prob)\n",
    "    # Forward direction cell: (if else required for TF 1.0 and 1.1 compat)\n",
    "    gru_fw_cell1 = tf.nn.rnn_cell.GRUCell(cell_units)\n",
    "    #if is_training is not None:\n",
    "    #    gru_fw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_fw_cell1, output_keep_prob=keep_prob)\n",
    "    \n",
    "    '''\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units1, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units1, forget_bias=1.0)\n",
    "    \n",
    "    '''\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    if is_training is not None:\n",
    "        gru_fw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_fw_cell1, output_keep_prob=keep_prob)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    if is_training is not None:\n",
    "        gru_bw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_bw_cell1, output_keep_prob=keep_prob)\n",
    "    '''\n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs1, output_states1 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell1,\n",
    "                                                             cell_bw=gru_bw_cell1,\n",
    "                                                             inputs= linear1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM1')\n",
    "    '''\n",
    "    outputs1 = tf.concat(outputs1,2)\n",
    "     # Forward direction cell\n",
    "    gru_fw_cell2 = tf.contrib.rnn.BasicLSTMCell(cell_units2, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell2 = tf.contrib.rnn.BasicLSTMCell(cell_units2, forget_bias=1.0)\n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs, output_states2 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell2,\n",
    "                                                             cell_bw=gru_bw_cell2,\n",
    "                                                             inputs= outputs1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM2')\n",
    "    '''\n",
    "    #time_major=false,tensor的shape为[batch_size, max_time, depth]。实验中使用tf.concat(outputs, 2)将其拼接\n",
    "    \n",
    "    outputs = tf.concat(outputs1,2)\n",
    "    outputs = tf.reshape(outputs, [-1, timesteps,2*cell_units1, 1])\n",
    "    gru = tf.nn.max_pool(outputs,ksize=[1,timesteps,1,1], strides=[1,timesteps,1,1], padding='VALID', name='max_pool')\n",
    "    gru = tf.reshape(gru, [-1,2*cell_units1])    \n",
    "    '''\n",
    "    # Attention layer\n",
    "    gru, alphas = attention(outputs1, ATTENTION_SIZE, return_alphas=True)\n",
    "    ''' \n",
    "    \n",
    "    fully1 = tf.matmul(gru,fully1_weight) + fully1_bias\n",
    "    #fully1 = batch_norm_wrapper(fully1,is_training)\n",
    "    fully1 = leaky_relu(fully1, 0.01)\n",
    "    #fully1 = batch_norm_wrapper(fully1,is_training) \n",
    "    fully1 = tf.nn.dropout(fully1, keep_prob)\n",
    "    \n",
    "    \n",
    "    Ylogits = tf.matmul(fully1, fully2_weight) + fully2_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    '''\n",
    "    fully2 = tf.matmul(fully1,fully2_weight) + fully2_bias  \n",
    "    fully2 = leaky_relu(fully2, 0.01)\n",
    "    #fully2 = batch_norm_wrapper(fully2,is_training) \n",
    "    Ylogits = tf.matmul(fully2, fully3_weight) + fully3_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    '''\n",
    "    return Ylogits\n",
    "    \n",
    "def train_op(norm):\n",
    "    STEPS = 50000\n",
    "    batch_size = 60\n",
    "    grad_clip = 5\n",
    "    MODEL_SAVE_PATH = \"./model2/\"\n",
    "    MODEL_NAME = \"model.ckpt\"\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 300,40,3])\n",
    "    Y = tf.placeholder(tf.int32, shape=[None, 4])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    Ylogits = build_model(X, is_training, keep_prob)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    #train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    var_trainable_op = tf.trainable_variables()\n",
    "    if norm == -1:\n",
    "        # not apply gradient clipping\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(cost)            \n",
    "    else:\n",
    "        # apply gradient clipping\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, var_trainable_op), grad_clip)\n",
    "        opti = tf.train.AdamOptimizer(lr)\n",
    "        train_op = opti.apply_gradients(zip(grads, var_trainable_op))\n",
    "    correct_pred = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))   \n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label = load_data()\n",
    "    train_label = dense_to_one_hot(train_label,len(np.unique(train_label)))\n",
    "    test_label = dense_to_one_hot(test_label,len(np.unique(test_label)))\n",
    "    valid_label = dense_to_one_hot(valid_label,len(np.unique(valid_label)))\n",
    "    max_learning_rate = 0.0001\n",
    "    min_learning_rate = 0.000001\n",
    "    decay_speed = 1600\n",
    "    dataset_size = train_data.shape[0]\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "    best_acc = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(STEPS):\n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "            start = (i * batch_size) % dataset_size\n",
    "            end = min(start+batch_size, dataset_size)\n",
    "            if i % 5 == 0:\n",
    "                loss, train_acc = sess.run([cost,accuracy],feed_dict = {X:valid_data, Y:valid_label,is_training:False, keep_prob:1})\n",
    "                test_acc = sess.run(accuracy, feed_dict = {X:test_data, Y:test_label, is_training:False, keep_prob:1})\n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                print \"After %5d trainging step(s), validation cross entropy is %2.2g, validation accuracy is %3.2g, test accuracy is %3.2g, the best accuracy is %3.2g\" %(i, loss, train_acc, test_acc, best_acc)\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME),global_step = i)\n",
    "            sess.run(train_op, feed_dict={X:train_data[start:end,:,:,:], Y:train_label[start:end,:],\n",
    "                                            is_training:True, keep_prob:1, lr:learning_rate})\n",
    "                                    \n",
    "if __name__=='__main__':\n",
    "    train_op(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
