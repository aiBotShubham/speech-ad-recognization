{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a18e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3234d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with tf.name_scope([tf.keras.layers.BatchNormalization],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : tf.keras.layers.BatchNormalization(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : tf.keras.layers.BatchNormalization(inputs=x, is_training=training, reuse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee3823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, leakiness=0.0):\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8117d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data():\n",
    "    f = open('IEMOCAP.pkl','rb')\n",
    "    train_data,train_label,test_data,test_label,valid_data,valid_label,Valid_label,Test_label,pernums_test,pernums_valid = pickle.load(f)\n",
    "    return train_data,train_label,test_data,test_label,valid_data,valid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234e5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training is not None:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547fa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_averages\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0177743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputX, is_training,keep_prob):\n",
    "    # 3 2-D convolution layers\n",
    "    L1 = 256\n",
    "    L2 = 512\n",
    "    L3 = 512\n",
    "    Li1 = 768\n",
    "    F1 = 64\n",
    "    F2 = 4\n",
    "    p = 5\n",
    "    cell_units1 = 128\n",
    "    timesteps = 200\n",
    "    ATTENTION_SIZE = 1\n",
    "    \n",
    "    import tensorflow.compat.v1 as tf\n",
    "    \n",
    "    layer1_filter = tf.get_variable('layer1_filter', shape=[5, 3, 3, L1], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer1_bias = tf.get_variable('layer1_bias', shape=[L1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer1_stride = [1, 1, 1, 1]\n",
    "    layer2_filter = tf.get_variable('layer2_filter', shape=[5, 3, L1, L2], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer2_bias = tf.get_variable('layer2_bias', shape=[L2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer2_stride = [1, 1, 1, 1]\n",
    "    layer3_filter = tf.get_variable('layer3_filter', shape=[5, 3, L2, L3], dtype=tf.float32, \n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    layer3_bias = tf.get_variable('layer3_bias', shape=[L3], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer3_stride = [1, 1, 1, 1]\n",
    "    \n",
    "    linear1_weight = tf.get_variable('linear1_weight', shape=[p*L2,Li1], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    linear1_bias = tf.get_variable('linear1_bias', shape=[Li1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    " \n",
    "    fully1_weight = tf.get_variable('fully1_weight', shape=[2*cell_units1,F1], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully1_bias = tf.get_variable('fully1_bias', shape=[F1], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    fully2_weight = tf.get_variable('fully2_weight', shape=[F1,F2], dtype=tf.float32,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fully2_bias = tf.get_variable('fully2_bias', shape=[F2], dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "    layer1 = tf.nn.conv2d(inputX, layer1_filter, layer1_stride, padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1,layer1_bias)\n",
    "    #layer1 = tf.layers.batch_normalization(layer1, training=is_training)\n",
    "    #layer1 = Batch_Normalization(layer1, training=is_training, scope='layer1_batch')\n",
    "    layer1 = leaky_relu(layer1, 0.01)\n",
    "    #layer1 = Batch_Normalization(layer1, training=is_training, scope='layer1_batch')\n",
    "    #print layer1.get_shape()\n",
    "    layer1 = tf.nn.max_pool(layer1,ksize=[1, 1, 4, 1], strides=[1, 1, 4, 1], padding='VALID', name='max_pool')\n",
    "    #print layer1.get_shape()\n",
    "    layer1 = tf.layers.dropout(layer1, rate = 1 - keep_prob)\n",
    "    #layer1 = tf.reshape(layer1,[-1,timesteps,L1*p])\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2,layer2_bias)\n",
    "    #layer1 = tf.layers.batch_normalization(layer1, training=is_training)\n",
    "    \n",
    "    layer2 = leaky_relu(layer2, 0.01)\n",
    "    #print layer2.get_shape()\n",
    "    #layer2 = Batch_Normalization(layer2, training=is_training, scope='layer1_batch')\n",
    "    layer2 = tf.nn.max_pool(layer2,ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='VALID', name='max_pool')\n",
    "    #print layer2.get_shape()\n",
    "    layer2 = tf.layers.dropout(layer2, rate = 1 - keep_prob)\n",
    "    layer2 = tf.reshape(layer2,[-1,timesteps,L2*p])\n",
    "    \n",
    "    \n",
    "    layer2 = tf.reshape(layer2, [-1,p*L2])\n",
    "    \n",
    "    #layer1 = tf.reshape(layer1,[-1,p*L1])\n",
    "    linear1 = tf.matmul(layer2,linear1_weight) + linear1_bias\n",
    "    linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = leaky_relu(linear1, 0.01)\n",
    "    #linear1 = batch_norm_wrapper(linear1,is_training)\n",
    "    linear1 = tf.reshape(linear1, [-1, timesteps, Li1])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #adding gru cell\n",
    "    gru_bw_cell1 = tf.nn.rnn_cell.GRUCell(cell_units)\n",
    "    #if is_training is not None:\n",
    "    #    gru_bw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_bw_cell1, output_keep_prob=keep_prob)\n",
    "    # Forward direction cell: (if else required for TF 1.0 and 1.1 compat)\n",
    "    gru_fw_cell1 = tf.nn.rnn_cell.GRUCell(cell_units)\n",
    "    #if is_training is not None:\n",
    "    #    gru_fw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_fw_cell1, output_keep_prob=keep_prob)\n",
    "    \n",
    "    '''\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.nn.rnn_cell.BasicLSTMCell(cell_units1, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.nn.rnn_cell.BasicLSTMCell(cell_units1, forget_bias=1.0)\n",
    "    \n",
    "    '''\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    gru_fw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    if is_training is not None:\n",
    "        gru_fw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_fw_cell1, output_keep_prob=keep_prob)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell1 = tf.contrib.rnn.BasicLSTMCell(cell_units, forget_bias=1.0)\n",
    "    if is_training is not None:\n",
    "        gru_bw_cell1 = tf.contrib.rnn.DropoutWrapper(cell=gru_bw_cell1, output_keep_prob=keep_prob)\n",
    "    '''\n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs1, output_states1 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell1,\n",
    "                                                             cell_bw=gru_bw_cell1,\n",
    "                                                             inputs= linear1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM1')\n",
    "    '''\n",
    "    outputs1 = tf.concat(outputs1,2)\n",
    "     # Forward direction cell\n",
    "    gru_fw_cell2 = tf.contrib.rnn.BasicLSTMCell(cell_units2, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    gru_bw_cell2 = tf.contrib.rnn.BasicLSTMCell(cell_units2, forget_bias=1.0)\n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs, output_states2 = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell2,\n",
    "                                                             cell_bw=gru_bw_cell2,\n",
    "                                                             inputs= outputs1,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=False,\n",
    "                                                             scope='LSTM2')\n",
    "    '''\n",
    "    #time_major=false,tensor的shape为[batch_size, max_time, depth]。实验中使用tf.concat(outputs, 2)将其拼接\n",
    "    \n",
    "    outputs = tf.concat(outputs1,2)\n",
    "    outputs = tf.reshape(outputs, [-1, timesteps,2*cell_units1, 1])\n",
    "    gru = tf.nn.max_pool(outputs,ksize=[1,timesteps,1,1], strides=[1,timesteps,1,1], padding='VALID', name='max_pool')\n",
    "    gru = tf.reshape(gru, [-1,2*cell_units1])    \n",
    "    '''\n",
    "    # Attention layer\n",
    "    gru, alphas = attention(outputs1, ATTENTION_SIZE, return_alphas=True)\n",
    "    ''' \n",
    "    \n",
    "    fully1 = tf.matmul(gru,fully1_weight) + fully1_bias\n",
    "    #fully1 = batch_norm_wrapper(fully1,is_training)\n",
    "    fully1 = leaky_relu(fully1, 0.01)\n",
    "    #fully1 = batch_norm_wrapper(fully1,is_training) \n",
    "    fully1 = tf.nn.dropout(fully1, keep_prob)\n",
    "    \n",
    "    \n",
    "    Ylogits = tf.matmul(fully1, fully2_weight) + fully2_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    '''\n",
    "    fully2 = tf.matmul(fully1,fully2_weight) + fully2_bias  \n",
    "    fully2 = leaky_relu(fully2, 0.01)\n",
    "    #fully2 = batch_norm_wrapper(fully2,is_training) \n",
    "    Ylogits = tf.matmul(fully2, fully3_weight) + fully3_bias\n",
    "    #Ylogits = tf.nn.softmax(Ylogits)\n",
    "    '''\n",
    "    return Ylogits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9108a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\AppData\\Local\\Temp/ipykernel_5440/1837530719.py:54: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\AppData\\Local\\Temp/ipykernel_5440/1837530719.py:93: BasicLSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\AppData\\Local\\Temp/ipykernel_5440/1837530719.py:109: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:438: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:738: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\SHUBHAM\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[654,4] labels_size=[298,4]\n\t [[node softmax_cross_entropy_with_logits_sg (defined at \\AppData\\Local\\Temp/ipykernel_5440/2092845188.py:18) ]]\n\nOriginal stack trace for 'softmax_cross_entropy_with_logits_sg':\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n    self.io_loop.start()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n    await self.process_one()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n    await dispatch(*args)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n    await result\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n    reply_content = await reply_content\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n    result = self._run_cell(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n    return runner(coro)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"\\AppData\\Local\\Temp/ipykernel_5440/2092845188.py\", line 18, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3996, in softmax_cross_entropy_with_logits\n    return softmax_cross_entropy_with_logits_v2(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3799, in softmax_cross_entropy_with_logits_v2\n    return softmax_cross_entropy_with_logits_v2_helper(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3901, in softmax_cross_entropy_with_logits_v2_helper\n    cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 10886, in softmax_cross_entropy_with_logits\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[654,4] labels_size=[298,4]\n\t [[{{node softmax_cross_entropy_with_logits_sg}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5440/2092845188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvalid_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[654,4] labels_size=[298,4]\n\t [[node softmax_cross_entropy_with_logits_sg (defined at \\AppData\\Local\\Temp/ipykernel_5440/2092845188.py:18) ]]\n\nOriginal stack trace for 'softmax_cross_entropy_with_logits_sg':\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n    self.io_loop.start()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n    await self.process_one()\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n    await dispatch(*args)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n    await result\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n    reply_content = await reply_content\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n    result = self._run_cell(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n    return runner(coro)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"\\AppData\\Local\\Temp/ipykernel_5440/2092845188.py\", line 18, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3996, in softmax_cross_entropy_with_logits\n    return softmax_cross_entropy_with_logits_v2(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3799, in softmax_cross_entropy_with_logits_v2\n    return softmax_cross_entropy_with_logits_v2_helper(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3901, in softmax_cross_entropy_with_logits_v2_helper\n    cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 10886, in softmax_cross_entropy_with_logits\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "STEPS = 50000\n",
    "batch_size = 60\n",
    "grad_clip = 5\n",
    "MODEL_SAVE_PATH = \"/model/\"\n",
    "MODEL_NAME = \"model.ckpt\"\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 300,40,3])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 4])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "Ylogits = build_model(X, is_training, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels =  Y, logits =  Ylogits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "#train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "var_trainable_op = tf.trainable_variables()\n",
    "norm = -1\n",
    "if norm == -1:\n",
    "    # not apply gradient clipping\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(cost)            \n",
    "else:\n",
    "    # apply gradient clipping\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, var_trainable_op), grad_clip)\n",
    "    opti = tf.train.AdamOptimizer(lr)\n",
    "    train_op = opti.apply_gradients(zip(grads, var_trainable_op))\n",
    "correct_pred = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))   \n",
    "saver=tf.train.Saver(tf.global_variables())\n",
    "\n",
    "train_data,train_label,test_data,test_label,valid_data,valid_label = load_data()\n",
    "train_label = dense_to_one_hot(train_label,len(np.unique(train_label)))\n",
    "test_label = dense_to_one_hot(test_label,len(np.unique(test_label)))\n",
    "valid_label = dense_to_one_hot(valid_label,len(np.unique(valid_label)))\n",
    "max_learning_rate = 0.0001\n",
    "min_learning_rate = 0.000001\n",
    "decay_speed = 1600\n",
    "dataset_size = train_data.shape[0]\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "best_acc = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(STEPS):\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "        if i % 5 == 0:\n",
    "            loss, train_acc = sess.run([cost,accuracy],feed_dict = {X:valid_data, Y:valid_label,is_training:False, keep_prob:1})\n",
    "            test_acc = sess.run(accuracy, feed_dict = {X:test_data, Y:test_label, is_training:False, keep_prob:1})\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "            print(\"After %5d trainging step(s), validation cross entropy is %2.2g, validation accuracy is %3.2g, test accuracy is %3.2g, the best accuracy is %3.2g\" %(i, loss, train_acc, test_acc, best_acc))\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME),global_step = i)\n",
    "        sess.run(train_op, feed_dict={X:train_data[start:end,:,:,:], Y:train_label[start:end,:],\n",
    "                                        is_training:True, keep_prob:1, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0800391",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.RNN.BasicLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.rnn_cell.BasicLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e9494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
